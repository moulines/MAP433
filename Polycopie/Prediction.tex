% \section{théorème de Projection}
%     \subsection{Espace de Hilbert}
%     \subsection{Bases orthonormales}
%     \subsection{théorème de projection}
% \section{Algorithmes de Levinson-Durbin}
%================================================
%================================================
\section{Prédiction linéaire de processus stationnaires}
%================================================
% %================================================
% \subsection{Estimation linéaire en moyenne quadratique}
% %================================================
% Soient $X$ et $\{Y_1,\cdots,Y_p\}$ des variables aléatoires
% réelles de $L^2(\Omega,\cF,\PP)$. On cherche à
% déterminer la meilleure approximation de $X$ par une combinaison
% linéaire des variables $Y_k$. Nous supposons ici que nous
% connaissons les quantités $\mu=\PE{X}$, $\nu_k=\PE{Y_k}$ ainsi
% que les coefficients de covariance $\cov(X,Y_k)$ et
% $\cov(Y_k,Y_{\ell})$, pour tout $1\leq k,\ell\leq p$. En pratique,
% nous verrons au chapitre~\ref{chap:estim-lineaire}
% comment il est possible, sous certaines hypothèses, de construire des
% estimateurs consistants et asymptotiquement normaux de ces quantités à partir d'une suite
% d'observations.

% On considère l'espace fermé de dimension
% finie ${\cal Y} = \lspan{1, Y_1,\cdots,Y_p}$ et on cherche
% l'élément $Y\in \cY$ qui minimise la norme de le risque quadratique
% $\| X - Y\|^2$. Il découle immédiatement du théorème de
% projection que le prédicteur linéaire optimal est la projection
% orthogonale $\proj{X}{\cY}$ de $X$ sur ${\cal Y}$ qui vérifie
% $(X-\proj{X}{\cY})\perp {\cal Y}$. On en déduit que\,:
% \begin{gather}
% \label{eq:reglin1}
% \begin{cases}
%  \pscal{X - \proj{X}{\cY}}{1} = 0  &\\
%  \pscal{X - \proj{X}{\cY}}{Y_k} = 0 &\text{pour $k \in \{1, \cdots, p \}$}
% \end{cases}  \eqsp.
% \end{gather}
% Ce sont ces $(p+1)$ équations qui vont nous donner la solution
% cherchée. En effet la condition $\proj{X}{\cY} \in \cY$ implique (comme $\cY$ est de
% dimension finie) que $\proj{X}{\cY}=a_0 + \sum_{k=1}^p a_k (Y_k - \nu_k)$.
% Il suffit donc de calculer $a_0,a_1,\dots,a_p$. Partant de la première expression de
% \eqref{eq:reglin1}, on obtient\,:
% \begin{equation}
%  \label{eq:ortho0}
%  \pscal{X - a_0 - \sum_{k=1}^p a_k (Y_k - \nu_k)}{1} = \pscal{X}{1} - a_0 = 0 \eqsp,
% \end{equation}
% qui donne $a_0=\mu$. En posant $a_0=\mu$ dans la seconde
% expression de \eqref{eq:reglin1}, on a obtient alors
% $k\in\{1,\dots,p\}$\,:
% \begin{multline}
%  \label{eq:ortho1p}
%  \pscal{X - \mu - \sum_{j=1}^p a_j (Y_j - \nu_j)}{Y_k - \nu_k}
%  \\= \pscal{X - \mu}{Y_k-\nu_k} - \sum_{j=1}^p a_j \pscal{Y_j - \nu_j}{Y_k - \nu_k} = 0 \eqsp,
% \end{multline}
% qui montrent que $\{a_1, \cdots, a_p\}$ sont solution d'un
% système de $p$ équations linéaires à $p$ inconnues.

% Ce système d'équations peut se mettre sous forme plus compacte en
% utilisant la matrice $\Gamma = [\cov(Y_k,Y_{\ell})]_{1 \leq k,\ell
% \leq p}$ des coefficients de covariance de $(Y_1, \cdots, Y_p)$ et
% le vecteur $\bfgamma= [\cov (X,Y_1), \cdots, \cov(X,Y_p)]^T$ des
% coefficients de covariance entre $X$ et les composantes $Y_k$.
% Avec ces notations, le vecteur $\bfalpha = [a_1, \cdots, a_p]^T$
% est solution de l'équation\,:
% \begin{equation}
% \label{eq:Eqsnormales}
% \Gamma \bfalpha = \bfgamma
% \end{equation}
% Ce système linéaire admet une unique solution si la matrice
% $\Gamma$ est inversible. Notons enfin qu'en vertu de l'identité de
% Pythagore, nous avons\,:
% \[
% \| X \|^2 = \| \proj{X}{\cY} \|^2 + \| X - \proj{X}{\cY} \|^2
% \]
% et donc la norme minimale de l'erreur de prédiction a pour
% expression\,:
% \[
% \| X - \proj{X}{\cY} \|^2 = \| X \|^2 - \| \proj{X}{\cY}\|^2 \eqsp.
% \]
% Nous allons à présent appliquer ce résultat à la prédiction
% d'un processus stationnaire au second-ordre à partir de son
% passé immédiat en prenant $X=X_t$ et $Y_k=X_{t-k}$ avec
% $k=\{1,\dots,p\}$.
%==============================================================
%==============================================================
%\subsection{Prédiction linéaire de processus stationnaires}
%==============================================================
Soit $(X_t)_{t\in\Zset}$ un processus stationnaire au
second ordre à valeurs réelles, \textbf{d'espérance nulle} et de fonction
d'autocovariance $\gamma(h)= \cov(X_h,X_0)$. On cherche à
\emph{prédire} la valeur du processus à la date $t$ à partir
d'une combinaison linéaire des $p$ derniers échantillons du passé
$X_{t-1}, \dots, X_{t-p}$. La meilleure combinaison linéaire
(\textit{i.e.} le prédicteur linéaire optimal)
est la projection orthogonale de $X_t$ sur
$\cH_{t-1,p}$  notée $\proj{X_t}{\cH_{t-1,p}}$, où $\cH_{t-1,p}$ est
défini par :
\begin{equation}
\label{eq:cht}
\cH_{t-1,p} = \lspan{X_{t-1}, X_{t-2}, \cdots, X_{t-p}}\;.
\end{equation}
Les indices dans la notation $\cH_{t-1,p}$ doivent être compris ainsi
: $\cH_{t-1,p}$ est le sous-espace vectoriel engendré par les
$p$ observations précédant $X_{t-1}$ à savoir
$X_{t-1}, \dots, X_{t-p}$.
D'après le théorème \ref{theo:projection},
\begin{equation}\label{eq:def_phi_kp}
\proj{X_t}{\cH_{t-1,p}}=\sum_{k=1}^p \phi_{k,p} X_{t-k}\;,
\end{equation}
où les coefficients $(\phi_{k,p})_{1\leq k\leq p}$ satisfont
\begin{equation}\label{eq:scal_1}
\left\langle X_t-\sum_{k=1}^p \phi_{k,p} X_{t-k},X_{t-j}\right\rangle=0\;,\;
j=1,\dots,p\;,
\end{equation}
la notation $\left\langle \cdot,\cdot\right\rangle$ correspondant au
produit scalaire dans $\ltwo\espaceproba$ défini pour $X$ et $Y$
dans $\ltwo\espaceproba$ par  $\left\langle X,Y\right\rangle=\PE{XY}$.
L'équation (\ref{eq:scal_1}) se réécrit encore sous la forme
\begin{equation}\label{eq:scal_2}
\left\langle X_t,X_{t-j}\right\rangle=\sum_{k=1}^p \phi_{k,p}
\left\langle  X_{t-k},X_{t-j}\right\rangle\;,\;
j=1,\dots,p\;,
\end{equation}
soit encore
\begin{equation}\label{eq:scal_3}
\sum_{k=1}^p \phi_{k,p}\gamma(k-j)=\gamma(j)\;,\;
j=1,\dots,p\;.
\end{equation}
En posant $\Gamma_p$ la matrice de covariance
du vecteur $(X_{t-1},\dots, X_{t-p})$ définie par
\begin{equation*}
\Gamma_p =
 \left[
 \begin{matrix}
  \gamma(0)  & \gamma(1)  &  \cdots    &            & \gamma(p-1) \\
  \gamma(1)  & \gamma(0)  &  \gamma(1) &            & \vdots \\
  \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
  \vdots     &            &            &            &\gamma(1)  \\
  \gamma(p-1)& \gamma(p-2)&  \cdots    & \gamma(1)  & \gamma(0)
 \end{matrix}
 \right]\;,
\end{equation*}
on peut réécrire (\ref{eq:scal_3}) comme suit :
\begin{equation}\label{eq:YW1}
 \Gamma_p \bfphi_p = \bfgamma_p\;,
\end{equation}
où $\bfphi_p=(\phi_{1,p},\dots,\phi_{p,p})^T$ et
$\bfgamma_p=(\gamma(1), \gamma(2), \cdots, \gamma(p))^T$.

\begin{definition}
\index{Innovation!partielle}
Nous appellerons dans la suite
\emph{erreur de prédiction directe} d'ordre $p$ ou
\emph{innovation partielle} d'ordre $p$ le processus\,:
\begin{equation}
\label{eq:deferreurforward}
 \epsilon_{t,p}^+
 = X_t - \proj{X_t}{\cH_{t-1,p}}
 = X_t - \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;.
\end{equation}
La variance de l'erreur de prédiction directe d'ordre $p$ est notée
$\sigma_p^2$ et définie par
\begin{equation}\label{eq:var_pred_dir}
\sigma_p^2=\|X_t - \proj{X_t}{\cH_{t-1,p}}\|^2=\PE{|X_t -
  \proj{X_t}{\cH_{t-1,p}}|^2}\;.
\end{equation}
\end{definition}
D'après (\ref{eq:def_phi_kp}) et la proposition \ref{prop:projecteur},
la variance de l'erreur de prédiction directe d'ordre $p$ a pour expression :
\begin{equation}
 \label{eq:YW2}
 \sigma_p^2 = \pscal{X_t}{X_t - \proj{X_t}{\cH_{t-1,p}}}
            =\gamma(0)- \sum_{k=1}^p \phi_{k,p}\gamma(k)
            =\gamma(0)-\bfphi_p^T\bfgamma_p\;.
\end{equation}
Les équations \eqref{eq:YW1} et \eqref{eq:YW2} sont appelées les
\emph{équations de Yule-Walker}.
% Notons la propriété importante
% suivante~: pour $p$ fixé, la suite des coefficients
% $\{\phi_{k,p}\}_{1\leq k \leq p}$ du prédicteur linéaire optimal
% et la variance de l'erreur minimale de prédiction {\em ne
% dépendent pas de $t$}.

Notons que (\ref{eq:YW1}) a une unique solution si et seulement si
la matrice $\Gamma_p$ est inversible auquel cas la solution vaut :
\begin{equation}\label{eq:sol_unique}
\bfphi_p=\Gamma_p^{-1} \bfgamma_p\; .
\end{equation}
La proposition \ref{prop:Gammanrangplein} fournit les conditions suffisantes assurant
que $\Gamma_p$ est inversible pour tout $p$. Une autre preuve de la proposition \ref{prop:Gammanrangplein}
résultat est proposée dans le problème \ref{prob1:prediction}. On a ainsi des conditions
sous lesquelles on peut calculer le prédicteur de $X_t$ à partir de
$X_{t-1},\dots,X_{t-p}$.



% Ce problème est bien entendu
% un cas particulier du problème précédent où nous avons $X=
% X_t$ et $Y_k = X_{t-k}$, pour $k \in \{1, \dots, p \}$ et
% où\,:
% \begin{equation}
% \label{eq:cht}
%   \cH_{t-1,p} = \lspan{1, X_{t-1}, X_{t-2}, \cdots, X_{t-p}}
% \end{equation}
% Formons la matrice de covariance $\Gamma_p$ du vecteur $[X_{t-1},
% \cdots, X_{t-p}]$:
% \begin{equation}
% \Gamma_p =
%  \left[
%  \begin{matrix}
%   \gamma(0)  & \gamma(1)  &  \cdots    &            & \gamma(p-1) \\
%   \gamma(1)  & \gamma(0)  &  \gamma(1) &            & \vdots \\
%   \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
%   \vdots     &            &            &            &\gamma(1)  \\
%   \gamma(p-1)& \gamma(p-2)&  \cdots    & \gamma(1)  & \gamma(0)
%  \end{matrix}
%  \right]
% \end{equation}
% Cette matrice est dite de Toeplitz, ses éléments étant égaux
% le long de ses diagonales. Notons $\bfgamma_p$ le vecteur
% $[\gamma(1), \gamma(2), \cdots, \gamma(p)]^T$ le vecteur des
% coefficients de corrélation. D'après l'équation
% \eqref{eq:Eqsnormales}, les coefficients $\{\phi_{k,p}\}_{1\leq
% k\leq p}$ du prédicteur linéaire optimal défini par\,:
% \begin{equation}
%  \label{eq:formegenedeXtsurH}
%  \proj{X_t}{\cH_{t-1,p}} - \mu=\sum_{k=1}^p \phi_{k,p} (X_{t-k}-\mu)
% \end{equation}
% sont solutions du système d'équations\,:
% \begin{equation}
%  \label{eq:YW1}
%  \Gamma_p \bfphi_p = \bfgamma_p \hspace{1cm}
% \end{equation}
% D'autre part l'erreur de prédiction minimale a pour expression\,:
% \begin{eqnarray}
%  \label{eq:YW2}
%  &\sigma_p^2 &= \| X_t - \proj{X_t}{\cH_{t-1,p}}\|^2
%             = \pscal{X_t-\mu}{X_t - \proj{X_t}{\cH_{t-1,p}}} \nonumber \\
%             &&=\gamma(0)- \sum_{k=1}^p \phi_{k,p}\gamma(k)
%             =\gamma(0)-\bfphi_p^T\bfgamma_p
% \end{eqnarray}
% Les équations \eqref{eq:YW1} et \eqref{eq:YW2} sont appelées
% \emph{équations de Yule-Walker}. Notons la propriété importante
% suivante~: pour $p$ fixé, la suite des coefficients
% $\{\phi_{k,p}\}_{1\leq k \leq p}$ du prédicteur linéaire optimal
% et la variance de l'erreur minimale de prédiction {\em ne
% dépendent pas de $t$}.

% Les équations \eqref{eq:YW1} et
% \eqref{eq:YW2} peuvent encore être réécrites à partir des
% coefficients de corrélation $\rho(h)=\gamma(h)/\gamma(0)$. Il
% vient\,:
% \begin{equation}
%  \label{eq:YWcorrelation}
%  \left[
%  \begin{matrix}
%   \rho(0)  & \rho(1)  &  \cdots    &            & \rho(p-1) \\
%   \rho(1)  & \rho(0)  &  \rho(1) &            & \vdots \\
%   \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
%   \vdots     &            &            &            &\rho(1)  \\
%   \rho(p-1)& \rho(p-2)&  \cdots    & \rho(1)  & \rho(0)
%  \end{matrix}
%  \right]
%  \left[
%  \begin{matrix}
%   \phi_{1,p}\\
%   \phi_{2,p}\\
%   \vdots\\
%   \vdots \\
%   \phi_{p,p}
%  \end{matrix}
%  \right]
%  =
%   \left[
%  \begin{matrix}
%   \rho(1)\\
%   \rho(2)\\
%   \vdots\\
%   \vdots \\
%   \rho(p)
%  \end{matrix}
%  \right]
% \end{equation}
%========================================================
\begin{example}[Cas d'un processus AR$(m)$ causal]
Soit $(X_t)$ le processus AR$(m)$ causal solution de
l'équation récurrente\,:
\begin{equation}\label{eq:def_ar}
  X_t=\phi_1X_{t-1}+\cdots+\phi_m X_{t-m}+Z_t\;,
\end{equation}
où $Z_t\sim \BB(0,\sigma^2)$ et où
$\phi(z)=1-\sum_{k=1}^m\phi_k z^{k}\neq 0$ lorsque $|z|\leq 1$.
Dans ce cas, pour tout $p\geq m$ :
$$
 \phi_{k,p}=
 \begin{cases}
    \phi_k,&\text{lorsque }  1\leq k\leq m\;,\\
    0,& \text{lorsque }  m< k\leq p \;.
 \end{cases}
$$
En effet, $(X_t)$ étant causal on a, pour tout $h\geq 1$,
$\PE{Z_tX_{t-h}}=0$ et donc, d'après (\ref{eq:def_ar}),
$\PE{(X_t-\sum_{k=1}^m\phi_kX_{t-k})X_{t-h}}=0$.
Ainsi, pour tout $p\geq m$, $\sum_{k=1}^m\phi_kX_{t-k} \in
\cH_{t-1,p}$ et $(X_t-\sum_{k=1}^m\phi_kX_{t-k})\perp \cH_{t-1,p}$
et donc, d'après le théorème \ref{theo:projection}, pour tout $p\geq m$,
$$
\sum_{k=1}^m\phi_kX_{t-k}=\proj{X_t}{\cH_{t-1,p}}\;.
$$
% La projection orthogonale d'un AR$(m)$ causal sur son passé de longueur $p\geq m$ coïncide avec la projection
% orthogonale sur les $m$ dernières valeurs et les coefficients
% de prédiction sont précisément les coefficients de l'équation
% récurrente.
\end{example}
% Dans le cas où la matrice de covariance $\Gamma_p$, supposée
% \emph{connue}, est inversible, le problème de la
% détermination des coefficients de prédiction $\bfphi_p$ et de la
% variance de l'erreur de prédiction $\sigma_p^2$ a une solution
% unique. Rappelons que, d'après la propriété
% \ref{prop:Gammanrangplein}, si $\gamma(0)>0$ et si $\limn
% \gamma(n)=0$, alors la matrice $\Gamma_p$ est inversible à
% tout ordre.

% Il est facile de démontrer que\,:
% \begin{multline}
%  \label{eq:onpeutcenter}
%  \proj{X_t}{\lspan{1,X_{t-1},\dots,X_{t-p}}}
% \\ =\mu+
%  \proj{X_t-\mu}{\lspan{X_{t-1}-\mu,\dots,X_{t-p}-\mu}} \eqsp.
% \end{multline}
% Par conséquent, dans le problème de la prédiction,
% il n'y a aucune perte de généralité à considérer que le
% processus est centré. S'il ne l'était pas, il suffirait,
% d'après l'équation \eqref{eq:onpeutcenter}, d'effectuer le
% calcul des prédicteurs sur le processus centré $X_t^c=X_t-\mu$
% puis d'ajouter $\mu$. \emph{Dans la suite, sauf indication
% contraire, les processus sont supposés centrés}.

Les
coefficients de prédiction d'un processus stationnaire au second
ordre fournissent une décomposition particulière de la matrice
de covariance $\Gamma_{p+1}$ sous la forme d'un produit de matrices
triangulaires explicitée dans le théorème \ref{theo:choleski}.
\begin{theorem}
 \label{theo:choleski} Soit $(X_t)$ un processus stationnaire au second
ordre, centré, de fonction d'autocovariance $\gamma(h)$. On
note\,:
\[
A_{p+1} = \left[
\begin{matrix}
   1            & 0             & \cdots & \cdots      & 0 \\
   - \phi_{1,1} & 1             & \ddots &             & \vdots \\
   \vdots       &               & \ddots & \ddots      & \vdots \\
   \vdots       &               &        & \ddots      & 0 \\
   - \phi_{p,p} & - \phi_{p-1,p}& \cdots &- \phi_{1,p} &1
\end{matrix}
\right] \text{ et } D_{p+1} = \left[
\begin{matrix}
\sigma^2_0 & 0          & \cdots & 0 \\
0          & \sigma_1^2 & \cdots & 0 \\
\vdots     &            &        & \vdots \\
0          &            & \cdots & \sigma_p^2
\end{matrix}
\right]\;,
\]
où les coefficients $(\phi_{k,p})_{1\leq k\leq p}$ et
$(\sigma_k^2)_{1\leq k\leq p}$ sont respectivement définis dans
(\ref{eq:def_phi_kp}) et (\ref{eq:var_pred_dir}).
On a alors\,:
\begin{equation}
 \label{eq:decompocholeski}
 \Gamma_{p+1} = A_{p+1}^{-1} D_{p+1} (A_{p+1}^T)^{-1}\;.
\end{equation}
\end{theorem}
\begin{proof}\smartqed
Pour simplifier les notations, posons $\cH_k =\cH_{k,k}= \lspan{X_k, \cdots, X_1}$ et montrons tout
d'abord que, pour $k \neq \ell$, nous avons\,:
\begin{equation}
 \label{eq:erreurblanche}
 \pscal{X_k - \proj{X_k}{\cH_{k-1}}}{X_{\ell} - \proj{X_{\ell}}{\cH_{{\ell}-1}}} = 0 \eqsp.
\end{equation}
En effet, pour $k < \ell$, on a $X_{k} - \proj{X_{k}}{\cH_{k-1}} \in
\cH_k\subseteq\cH_{\ell-1}$ et $X_{\ell} -
\proj{X_{\ell}}{\cH_{\ell-1}} \perp \cH_{\ell-1}$.
D'autre part, si on note ${\bf X}_{p+1}$ le vecteur :
$(X_1,\dots,X_{p+1})^T$, alors,
par définition des coefficients de prédiction (\ref{eq:def_phi_kp}), on peut écrire :
\[
A_{p+1} {\bf X}_{p+1} = \left[
\begin{array}{llll}
1            & 0             & \cdots & 0 \\
- \phi_{1,1} & 1             & \cdots & 0 \\
\vdots       &               &        & \vdots \\
- \phi_{p,p} & - \phi_{p-1,p}& \cdots & 1
\end{array}
\right] \left[
\begin{array}{l}
X_{1} \\
X_{2} \\
\vdots \\
X_{p+1}
\end{array}
\right] = \left [
\begin{array}{l}
X_1 \\
X_2 - \proj{X_2}{\cH_1} \\
\vdots \\
X_{p+1} - \proj{X_{p+1}}{\cH_{p}}
\end{array}
\right ]\;,
\]
qui donne\,:
$$
 \PE{A_{p+1} {\bf X}_{p+1} {\bf X}_{p+1}^T A_{p+1}^T}
 =D_{p+1}\;,
$$
d'après \eqref{eq:erreurblanche} et (\ref{eq:var_pred_dir}).
Par ailleurs,
$$
\PE{A_{p+1} {\bf X}_{p+1} {\bf X}_{p+1}^T A_{p+1}^T}
= A_{p+1} \Gamma_{p+1}A_{p+1}^T\;,
$$
ce qui démontre \eqref{eq:decompocholeski} puisque la
matrice $A_{p+1}$ est inversible, son déterminant étant égal à
$1$.
\qed
\end{proof}
% Dans la suite nous notons
% $\cH_{t-1,p}=\lspan{X_{t-1},\dots,X_{t-p}}$ et nous appelons
% \emph{erreur de prédiction directe} d'ordre $p$ ou
% \emph{innovation partielle} d'ordre $p$ le processus\,:
% \begin{equation}
% \label{eq:deferreurforward}
%  \epsilon_{t,p}^+
%  = X_t - \proj{X_t}{\cH_{t-1,p}}
%  = X_t - \sum_{k=1}^{p} \phi_{k,p} X_{t-k}
% \end{equation}
D'après l'équation \eqref{eq:decompocholeski} lorsque la
matrice $\Gamma_{p+1}$ est inversible, la variance
$\sigma_p^2=\|\epsilon_{t,p}^+\|^2$ est strictement positive.
D'autre part, la suite $\sigma_p^2$ est
décroissante. En effet, par définition de $\cH_{t-1,p}$,
$\cH_{t-1,p}$ est inclus dans $\cH_{t-1,p+1}$ donc
$\proj{X_{p+1}}{\cH_{t-1,p}}$ est dans $\cH_{t-1,p+1}$.
On déduit donc du théorème \ref{theo:projection} que $\sigma_{p+1}^2\leq\sigma_{p}^2$.
La suite $(\sigma_p^2)$ étant décroissante et minorée, elle possède
donc une limite quand $p$ tend vers l'infini. Cela conduit à la définition suivante,
dont nous verrons au paragraphe \ref{s:wold} qu'elle joue un rôle
fondamental dans la décomposition des processus stationnaires au
second ordre.

\begin{definition}[Processus régulier/déterministe]
 \label{def:paregulier}
 Soit $(X_t)_{t \in \Zset}$ un processus aléatoire stationnaire au second
ordre. On note $\sigma^2=\lim_{p\to\infty}\sigma_p^2$ où
$\sigma_p^2$ est la variance de l'innovation partielle
d'ordre $p$. On dit que le processus $(X_t)$ est \emph{régulier} si
$\sigma^2 > 0$ et \emph{déterministe} si $\sigma^2=0$.
\end{definition}

% \clnote{je me demande si ce qui est écrit à partir d'ici et jusqu'à la
% fin de la section ne serait pas mieux dans le chapitre sur Wold, à voir.}

% D'après \eqref{eq:YW1}, comme nous l'avons déjà remarqué,
% la suite $\{\phi_{k,p}\}$ ne dépend pas de $t$, pour $p$ fixé, et donc
% le processus $\epsilon_{t,p}^+$ (relativement à l'indice
% $t$) est stationnaire au second ordre et centré. On a de plus la
% relation suivante\,:
% \begin{equation}
%  \label{eq:pserreurforward}
% \pscal{\epsilon_{t,p}^+}{\epsilon_{t,q}^+}= \sigma_{\max(p,q)}^2 \eqsp.
% \end{equation}
% En effet soit $q > p$. Par construction, nous avons
% $\epsilon_{t,q}^+ \perp \cH_{t-1,q}$, et comme $\cH_{t-1,p}
% \subseteq \cH_{t-1,q}$, $\epsilon_{t,q}^+ \perp \cH_{t-1,p}$ et en
% particulier $\epsilon_{t,q}^+\perp \proj{X_t}{\cH_{t-1,p}}$ puisque
% $\proj{X_t}{\cH_{t-1,p}}\in \cH_{t-1,p}$. Par conséquent, pour $q
% > p$, on a\,:
% \begin{multline*}
%  (\epsilon_{t,p}^+, \epsilon_{t,q}^+)
%  = \pscal{X_t - \proj{X_t}{\cH_{t-1,p}}}{ \epsilon_{t,q}^+} \\
%  = \pscal{X_t}{X_t - \proj{X_t}{\cH_{t-1,q}}}
%  = \pscal{X_t}{X_t - \proj{X_t}{\cH_{t-1,q}}}= \sigma_q^2 \eqsp,
% \end{multline*}
% ce qui démontre \eqref{eq:pserreurforward}.

Par ailleurs, nous pouvons remarquer que le problème de la recherche des coefficients de prédiction pour
un processus stationnaire au second ordre se ramène à
celui de la minimisation de l'intégrale\,:
\[
%\inf_{\psi \in \cP_p}
   \frac{1}{2\pi}
             \int_{-\pi}^{\pi} |\psi(\rme^{-\rmi\lambda})|^2 \nu_X(\rmd\lambda)
%           = \frac{1}{2\pi}
%             \int_{-\pi}^{\pi} |\phi_p(e^{\rmi x})|^2 \mu_X(dx)
%           =\sigma_p^2
\]
sur l'ensemble $\cP_p$ des polynômes à coefficients réels
de degré $p$ de la forme $\psi(z) = 1 + \psi_1 z + \cdots + \psi_p
z^p$. En effet, en utilisant la relation \eqref{eq:dspfiltrage} de
filtrage des mesures spectrales, on peut écrire que la variance de
$ \| \epsilon_{t,p}^+ \|^2$, qui minimise l'erreur de
prédiction, a pour expression\,:
\begin{equation}
\label{eq:variance:innovation:partielle}
% \| \epsilon_{t,p}^+ \|^2
 \sigma_p^2
 = \frac{1}{2 \pi} \int_{-\pi}^{\pi} |
    \phi_p(\rme^{-\rmi\lambda})|^2 \nu_X(\rmd\lambda)
\end{equation}
où\,:
\[
  \phi_p(z) = 1 - \sum_{k=1}^p \phi_{k,p} z^k
\]
désigne le \emph{polynôme prédicteur d'ordre $p$}.
\begin{theorem}
 \label{theo:procregulpredicstable}
 Si $\{ X_t \}$ est un processus régulier, alors, pour
tout $p$, $\phi_p(z) \ne 0$ pour $|z|\leq 1$. Tous les zéros des
polynômes prédicteurs sont à l'extérieur du cercle unité.
\end{theorem}

\begin{proof}[Preuve du théorème~\ref{theo:procregulpredicstable}]
\smartqed
Nous allons tout d'abord montrer que le prédicteur optimal n'a pas
de racines sur le cercle unité. Raisonnons par contradiction.
Supposons que le polynôme $\phi_p(z)$ ait deux racines
complexes conjuguées, de la forme $\exp(\pm i\theta)$, sur le
cercle unité (on traite de façon similaire le cas de racines
réelles, $\theta= 0$ ou $\pi$). Nous pouvons écrire\,:
\[
 \phi_p(z) = \phi^*_p(z) (1 - 2 \cos(\theta) z + z^2)
\]
On note $\bar{\nu}_X(d \lambda) = \nu_X(d \lambda) |\phi^*_p(\rme^{-i
\lambda})|^2$. $\bar{\nu}_X$ est une mesure positive sur
$[-\pi,\pi]$ de masse finie. On note $\bar{\gamma}(\tau)$ la suite
des coefficients de Fourier associés à  $\bar{\nu}_X$\,:
\[
 \bar{\gamma}(\tau) = \frac{1}{2 \pi} \int_{- \pi}^{\pi}
 \rme^{\rmi \tau \lambda} \bar{\nu}_X(\rmd \lambda)\;.
\]
Nous avons donc\,:
\begin{multline*}
 \sigma_p^2 = \frac{1}{2 \pi} \int_{-\pi}^\pi
                  (1 - 2 \cos(\theta) \rme^{-i \lambda} + \rme^{-2 i \lambda})
                  \bar{\nu}_X(\rmd \lambda)\\
           = \inf_{\psi \in \cP_2} \frac{1}{2 \pi}
           \int_{-\pi}^\pi
           |1 + \psi_1 \rme^{-\rmi\lambda} + \psi_2 \rme^{-2 \rmi \lambda}|^2
             \bar{\nu}_X(\rmd \lambda) \eqsp.
\end{multline*}
La minimisation de $\sigma_p^2$ par rapport à  $\psi_1$ et $\psi_2$ est équivalente
à la résolution des équations de Yule-Walker à
l'ordre $p=2$ pour la suite des covariances $\bar{\gamma}(h)$.
Par conséquent la suite des coefficients $\{1,-2\cos(\theta),1\}$
doit vérifier l'équation\,:
\clnote{je ne comprends pas cette égalité}
\[
\left[
\begin{array}{ccc}
\bar{\gamma}(0) & \bar{\gamma}(1) & \bar{\gamma}(2) \\
\bar{\gamma}(1) & \bar{\gamma}(0) & \bar{\gamma}(1) \\
\bar{\gamma}(2) & \bar{\gamma}(1) & \bar{\gamma}(0) \\
\end{array}
\right] \left[
\begin{array}{c}
1 \\
-2 \cos(\theta) \\
1
\end{array}
\right] = \left[
\begin{array}{c}
\sigma_p^2 \\
0 \\
0
\end{array}
\right]
\]
De cette équation il s'en suit (les première et troisième
lignes sont égales) que $\sigma_p^2=0$, ce qui est contraire à
l'hypothèse que le processus est régulier.

Démontrons maintenant que les racines des polynômes
prédicteurs sont toutes \emph{strictement à  l'extérieur du
cercle unité}. Raisonnons encore par l'absurde. Supposons que le
polynôme prédicteur à l'ordre $p$ ait $m$ racines $\{a_k,
|a_k| < 1, 1 \le k \leq m \}$ à l'intérieur du cercle unité et
$(p-m)$ racines $\{ b_{\ell}, |b_{\ell}| > 1, 1 \leq \ell \leq p-m
\}$ à l'extérieur du cercle unité. Le polynôme prédicteur
à l'ordre $p$ s'écrit donc\,:
\[
 \phi_p(z)=
 \prod_{k=1}^m ( 1 - a_k^{-1} z) \prod_{\ell=1}^{p-m} (1 - b_{\ell}^{-1} z)\;.
\]
Considérons alors le polynôme\,:
\[
 \bar{\phi}_p(z) =
 \prod_{k=1}^m (1 - a_k^{*} z) \prod_{\ell=1}^{p-m} (1 - b_{\ell}^{-1} z)\;.
\]
Il a d'une part toutes ses racines strictement à l'extérieur
du cercle unité et d'autre part il vérifie
$|\bar{\phi}_p(\rme^{-i\lambda })|^2< |\phi_p(\rme^{-i\lambda })|^2$. On a
en effet $| 1 - a_k^* \rme^{-i\lambda } | = |1 - a_k \rme^{\rmi \lambda }|=
|a_k| |1 - a_k^{-1} \rme^{-i\lambda }|$ et donc
$|\bar{\phi}_p(\rme^{-i\lambda })|^2 = \left (\prod_{k=1}^m |a_k|^2
\right )|\phi_p(\rme^{-i\lambda })|^2$, ce qui démontre le résultat
annoncé puisque $|a_k|<1$. On en déduit alors
que\,:
\begin{gather*}
\frac{1}{2 \pi} \int_{-\pi}^{\pi} | \bar{\phi}_p(\rme^{-i\lambda
})|^2 \nu_X(\rmd\lambda )   < \sigma_p^2\;,
\end{gather*}
ce qui contredit que $\phi_p(z)=\inf_{\psi \in \cP_p}(2\pi)^{-1}
\int_{-\pi}^\pi |\psi(\rme^{-i\lambda })|^2 \nu_X(\rmd\lambda )$.
\qed
\end{proof}

\clnote{je ne comprends pas la remarque}
Une conséquence directe du théorème
\ref{theo:procregulpredicstable} est qu'à toute matrice de
covariance de type défini positif, de dimension $(p+1)\times
(p+1)$, on peut associer un processus AR$(p)$ causal dont les
$(p+1)$ premiers coefficients de covariance sont précisément la
première ligne de cette matrice. Ce résultat n'est pas général.
Ainsi il existe bien un processus AR$(2)$ causal ayant
$\gamma(0)=1$ et $\gamma(1)=\rho$, comme premiers coefficients de
covariance, à condition toutefois que la matrice de covariance
soit positive c'est-à-dire que $|\rho|<1$, tandis qu'il n'existe
pas, pour cette même matrice de processus MA$(1)$. Il faut en
effet, en plus du caractère positif, que $|\rho|\leq 1/2$
(voir exemple \ref{exe:testposivite1}).
%============================================================================
%============================================================================
%============================================================================
\section{Algorithme de Levinson-Durbin}
\label{sec:algorithme-levinson-durbin}
%============================================================================
La solution directe du système des équations de Yule-Walker
requiert de l'ordre de $p^3$ opérations~: la résolution classique
de ce système implique en effet la décomposition de la matrice
$\Gamma_p$ sous la forme du produit d'une matrice triangulaire
inférieure et de sa transposée, $\Gamma_p = L_p L_p^T$
(décomposition de Choleski) et la résolution par substitution de
deux systèmes triangulaires. Cette procédure peut s'avérer
coûteuse lorsque l'ordre de prédiction est grand (on utilise
généralement des ordres de prédiction de l'ordre de quelques
dizaines à quelques centaines), ou lorsque, à des fins de
modélisation, on est amené à évaluer la qualité de prédiction
pour différents horizons de prédiction. L'algorithme de
Levinson-Durbin exploite la structure géométrique particulière
des processus stationnaires au second ordre pour établir une
formule de récurrence donnant les coefficients de prédiction à
l'ordre $(p+1)$ à partir des coefficients de prédiction
obtenus à l'ordre $p$. Il fournit également une relation de récurrence
entre l'erreur de prédiction directe à l'ordre $p+1$
et l'erreur de prédiction directe à l'ordre $p$.

On supposera dans toute cette partie que {\boldmath$\Gamma_p$} \textbf{est inversible
pour tout} {\boldmath $p\geq 1$}.

Supposons que les
coefficients de prédiction linéaire et la variance de l'erreur de
prédiction directe à l'ordre $p$, pour $p \geq 0$, sont connus :
\begin{gather*}
%\label{eq:definition:predictionretrograde}
   \proj{X_t}{\cH_{t-1,p}} =
   \sum_{k=1}^{p} \phi_{k,p} X_{t-k}
   \quad\mbox{et}\quad
   \sigma_{p}^2 = \| X_t - \proj{X_t}{\cH_{t-1,p}} \|^2\;,
\end{gather*}
et déterminons, à partir de la
projection à l'ordre $p$ de $X_t$, la projection de $X_t$ à
l'ordre $p+1$ sur le sous-espace $\cH_{t-1,p+1} =
\lspan{X_{t-1}, \cdots, X_{t-p-1}}$.

Pour cela, on décompose cet espace
en somme orthogonale de la façon suivante\,:
\begin{multline*}
\cH_{t-1,p+1} = \cH_{t-1,p}  \oplusperp \lspan{ X_{t-p-1} -\proj{X_{t-p-1}}{\cH_{t-1,p}}}
\\= \cH_{t-1,p} \oplusperp \lspan{\epsilon_{t-p-1,p}^-}\;,
\end{multline*}
où, de façon générale, $\epsilon_{t,p}^-$ correspond à
l'\emph{erreur de prédiction
rétrograde à l'ordre $p$} définie par :
\[
\epsilon_{t,p}^-
     = X_t - \proj{X_t}{\cH_{t+p,p}}
     = X_t - \proj{X_t}{\lspan{X_{t+p}, \cdots, X_{t+1}}}\;.
\]
Elle représente la différence entre la valeur à l'instant courant $X_t$ et
la projection orthogonale de $X_t$ sur les $p$ échantillons
\emph{qui suivent} l'instant courant $\{X_{t+1}, \cdots, X_{t+p}\}$.
Le qualificatif \emph{rétrograde} est clair\,: il
traduit le fait que l'on cherche à prédire la valeur courante
en fonction des valeurs futures.

D'après la proposition \ref{prop:projecteur},
\begin{multline*}
%\label{eq:decomposition}
  \proj{X_t}{\cH_{t-1,p+1}}
  = \proj{X_t}{\cH_{t-1,p}} + \proj{X_t}{\lspan{\epsilon_{t-p-1,p}^-}}\;,
\end{multline*}
où d'après l'exemple \ref{exe:proj1vecteur} :
$$
\proj{X_t}{\lspan{\epsilon_{t-p-1,p}^-}} = \alpha \epsilon_{t-p-1,p}^-
 \quad \mbox{avec} \quad
 \alpha=\pscal{X_t}{\epsilon_{t-p-1,p}^-}/\|\epsilon_{t-p-1,p}^-\|^2\;.
$$
On en déduit donc que :
\begin{multline}
\label{eq:decomposition}
  \proj{X_t}{\cH_{t-1,p+1}}
  = \proj{X_t}{\cH_{t-1,p}} \\
+ k_{p+1} \left[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}} \right] \eqsp,
\end{multline}
où
\begin{equation}\label{eq:k_{p+1}}
k_{p+1}=\frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t-p-1,p}^-\|^2}\;.
\end{equation}
Montrons à présent que les coefficients de prédiction rétrograde
coïncident avec les coefficients de prédiction directe.
Plus précisément, si
\begin{equation}
\label{eq:devtdirectretro1}
 \proj{X_t}{\cH_{t-1,p}} = \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;,
\end{equation}
alors
\begin{equation}
\label{eq:devtdirectretro2}
 \proj{X_{t-p-1}}{\cH_{t-1,p}} =\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k} =\sum_{k=1}^{p} \phi_{p+1-k,p} X_{t-k}\;.
\end{equation}
En effet, les coefficients des deux développements
(\ref{eq:devtdirectretro1}) et (\ref{eq:devtdirectretro2}) sont
tous les deux donnés par (\ref{eq:sol_unique}).
En utilisant \eqref{eq:devtdirectretro1} et
\eqref{eq:devtdirectretro2} dans
\eqref{eq:decomposition}, on a :
\begin{multline*}
\proj{X_{t}}{\cH_{t-1,p+1}}
  = \sum_{k=1}^{p+1} \phi_{k,p+1}X_{t-k}\\
  = \sum_{k=1}^{p} (\phi_{k,p} - k_{p+1} \phi_{p+1-k,p} ) X_{t-k} + k_{p+1} X_{t-p-1}\;.
\end{multline*}
On en déduit, par unicité, les formules de récurrence donnant les coefficients
de prédiction à l'ordre $p+1$ à partir de ceux à
l'ordre $p$\,:
\begin{equation}
 \label{eq:recursionLevinson}
\begin{cases}
\phi_{k,p+1} = \phi_{k,p} - k_{p+1} \phi_{p+1-k,p}\;, & \quad\mbox{pour}\quad k \in \{1, \cdots, p \}\;, \\
\phi_{p+1,p+1} = k_{p+1}\;. &
\end{cases}
\end{equation}
Explicitons à présent la relation (\ref{eq:k_{p+1}}) définissant
$k_{p+1}$. En utilisant \eqref{eq:devtdirectretro2}, on a :
\begin{multline*}
\pscal{X_t}{\epsilon_{t-p-1,p}^-}
            = \pscal{X_{t}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}\\
            = \gamma(p+1) - \pscal{X_t}{\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k}}
            = \gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)\;.
\end{multline*}
D'autre part,
\begin{multline}\label{eq:norme2_epsretro}
\|\epsilon_{t-p-1,p}^-\|^2=\pscal{X_{t-p-1}}{X_{t-p-1}-\sum_{k=1}^{p}
  \phi_{k,p} X_{t-p-1+k}}\\
=\gamma(0)-\sum_{k=1}^{p} \phi_{k,p}\gamma(k)
=\sigma_p^2=\|\epsilon_{t,p}^+\|^2\;,
\end{multline}
ce qui donne
\begin{equation}\label{eq:def:k_{p+1}}
  k_{p+1}=
   \frac{\gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)}
   {\sigma_p^2}\;.
 \end{equation}
 Il nous reste maintenant à déterminer l'erreur de prédiction
${\sigma_{p+1}^2}$ à l'ordre $(p+1)$ en fonction de $\sigma_p^2$.
En utilisant l'équation \eqref{eq:decomposition}, on a
\begin{multline*}
 \epsilon_{t,p+1}^+ =X_{t} - \proj{X_{t}}{\cH_{t-1,p+1}} \\
 = X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}
[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}]\\
=X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}\epsilon_{t-p-1,p}^-\;,
\end{multline*}
dont on déduit d'après \eqref{eq:norme2_epsretro}\,:
\begin{multline*}
  \sigma_{p+1}^2=\|\epsilon_{t,p+1}^+\|^2
=\sigma_p^2+ k_{p+1}^2 \sigma_p^2-2k_{p+1}\pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{\epsilon_{t-p-1,p}^-}\;.
      % = \sigma_p^2 + k_{p+1}^2 \sigma_p^2
      % - 2 k_{p+1} \pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}} \\
      %= \sigma_p^2 (1 - k_{p+1}^2)\;,
\end{multline*}
En utilisant que $\proj{X_{t}}{\cH_{t-1,p}}$ et $\epsilon_{t-p-1,p}^-$
sont orthogonaux, \eqref{eq:k_{p+1}} et \eqref{eq:norme2_epsretro}, on
obtient
\begin{equation}\label{eq:recursion_sigma_p+1}
 \sigma_{p+1}^2=\sigma_p^2 (1 - k_{p+1}^2)\;.
\end{equation}
A partir de ces récursions, nous allons à présent décrire l'algorithme
de Levinson-Durbin.



% Indiquons que l'erreur rétrograde
% joue un rôle absolument essentiel dans tous les algorithmes
% rapides de résolution des équations de Yule-Walker.

% Remarquons tout
% d'abord que les coefficients de prédiction rétrograde
% coïncident avec les coefficients de prédiction directe :
% Cette
% propriété, que nous avons rencontrée exemple \ref{exe:predAVAR},
% est fondamentalement due à la {\em propriété de réversibilité}
% des processus stationnaires au second ordre. En effet, si $Y_t=
% X_{-t}$, alors $Y_t$ a même moyenne et même fonction de
% covariance que $X_t$ (voir exemple \ref{exe:stat_retourne}
% chapitre \ref{chap:passl}) et par conséquent, en utilisant aussi
% l'hypothèse de stationnarité, on a simultanément pour tout
% $u,v\in\Zset$\,:
% \begin{multline*}
%   \proj{X_{t+u}}{\cH_{t+u-1,p}}
%          = \sum_{k=1}^{p} \phi_{k,p} X_{t+u-k}
%   \quad \text{et} \\
%   \proj{X_{t+v}}{\cH_{t+v+p,p}} = \sum_{k=1}^p \phi_{k,p} X_{t+v+k}
% \end{multline*}
% ainsi que\,:
% \begin{gather}
%  \label{eq:eplusemoins}
%  \sigma_p^2
%  =
%  \|\epsilon_{t+u,p}^+\|^2  %%=\| X_{t+u} - (X_{t+u} | \cH_{t+u-1,p}) \|^2
%  =
%  \|\epsilon_{t+v,p}^-\|^2 %%= \| X_{t+v-p-1,p} - (X_{t+v-p-1,p} | \cH_{t+v-1,p}) \|^2
% \end{gather}
% En particulier on a\,:
% \begin{equation}
%  \label{eq:devtdirectretro}
%  \begin{cases}
%  \proj{X_t}{\cH_{t-1,p}} = \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;, \\
%  \proj{X_{t-p-1}}{\cH_{t-1,p}} =\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k} =\sum_{k=1}^{p} \phi_{p+1-k,p} X_{t-k}\;.
%  \end{cases}
% \end{equation}



% Cherchons maintenant à déterminer, à partir de la
% projection à l'ordre $p$, la projection de $X_t$ à
% l'ordre $p+1$ sur le sous-espace $\cH_{t-1,p+1} =
% \lspan{X_{t-1}, \cdots, X_{t-p-1}}$. Pour cela décomposons cet espace
% en somme orthogonale de la façon suivante\,:
% \begin{multline*}
% \cH_{t-1,p+1} = \cH_{t-1,p}  \oplusperp \lspan{ X_{t-p-1} -\proj{X_{t-p-1}}{\cH_{t-1,p}}}
% \\= \cH_{t-1,p} \oplusperp \lspan{\epsilon_{t-p-1,p}^-}
% \end{multline*}
% Un calcul simple montre (voir exemple \ref{exe:proj1vecteur}) que
% $$
% \proj{X_t}{\epsilon_{t-p-1,p}^-} = \alpha \epsilon_{t-p-1,p}^-
%  \quad \mbox{avec} \quad
%  \alpha=(X_t,\epsilon_{t-p-1,p}^-)/\|\epsilon_{t-p-1,p}^-\|^2
% $$
% et donc que
% \begin{equation}
% \label{eq:decomposition}
%   \proj{X_t}{\cH_{t-1,p+1}}
%   = \proj{X_t}{\cH_{t-1,p}} + k_{p+1} \left[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}} \right] \eqsp,
% \end{equation}
% où, en utilisant aussi \eqref{eq:eplusemoins}, on peut
% écrire\,:
% \begin{equation}
% \label{eq:defkpP1}
%   k_{p+1}=\frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\sigma_p^2}=
%    \frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t+u,p}^+\| \|\epsilon_{t+v,p}^-\|} \eqsp.
% \end{equation}
% En portant à présent \eqref{eq:devtdirectretro} dans
% \eqref{eq:decomposition}, on obtient l'expression\,:
% $$
% \proj{X_{t}}{\cH_{t-1,p+1}}
%   = \sum_{k=1}^{p+1} \phi_{k,p+1}X_{t-k}
%   = \sum_{k=1}^{p} (\phi_{k,p} - k_{p+1} \phi_{p+1-k,p} ) X_{t-k} + k_{p+1} X_{t-p-1}
% $$
% On en déduit les formules de récurrence donnant les coefficients
% de prédiction à l'ordre $p+1$ à partir de ceux à
% l'ordre $p$\,:
% \begin{equation}
%  \label{eq:recursionLevinson}
% \begin{cases}
% \phi_{k,p+1} = \phi_{k,p} - k_{p+1} \phi_{p+1-k,p} & \quad\mbox{pour}\quad k \in \{1, \cdots, p \} \\
% \phi_{p+1,p+1} = k_{p+1} &
% \end{cases}
% \end{equation}
% Déterminons maintenant la formule de récurrence donnant $k_{p+1}$.
% En utilisant encore \eqref{eq:devtdirectretro} et
% \eqref{eq:decomposition}, on obtient\,:
% $$
% \pscal{X_t}{\proj{X_{t-p-1}}{\cH_{t-1,p}}}
%      =\sum_{k=1}^{p} \phi_{k,p} \PE{X_t X_{t-p-1+k}}
%      =\sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)
% $$
% Partant de l'expression de $\pscal{X_t}{\epsilon_{t-p-1,p}^-}$ on en
% déduit que\,:
% \begin{multline*}
% \pscal{X_t}{\epsilon_{t-p-1,p}^-}
%             = \pscal{X_{t}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}\\
%             = \gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)
% \end{multline*}
% et donc d'après \eqref{eq:defkpP1}\,:
% $$
%   k_{p+1}=
%    \frac{\gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)}
%    {\sigma_p^2}
% $$
% Il nous reste maintenant à déterminer l'erreur de prédiction
% ${\sigma_{p+1}^2}$ à l'ordre $(p+1)$. En utilisant l'équation
% \eqref{eq:decomposition}, on a
% \begin{multline*}
%  \epsilon_{t,p+1}^+ =X_{t} - \proj{X_{t}}{\cH_{t-1,p+1}} \\
%  = X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}
% (X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}})
% \end{multline*}
% dont on déduit d'après \eqref{eq:defkpP1}\,:
% \begin{multline*}
%   \sigma_{p+1}^2=\|\epsilon_{t,p+1}^+\|^2
%       = \sigma_p^2 + k_{p+1}^2 \sigma_p^2
%       - 2 k_{p+1} \pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}} \\
%       = \sigma_p^2 (1 - k_{p+1}^2)
% \end{multline*}

Pour initialiser l'algorithme, nous nous intéressons au cas $p=0$.
Dans ce cas, la meilleure prédiction de $X_t$ est $\PE{X_t}=0$ et la variance de
l'erreur de prédiction est donnée par $\sigma_{0}^2
=\PE{(X_t-0)^2}=\gamma(0)$. Au pas suivant on a
$k_1=\gamma(1)/\gamma(0)$, en posant $p=0$ dans (\ref{eq:def:k_{p+1}}),
$\phi_{1,1}=\gamma(1)/\gamma(0)$, en posant $p=0$ dans (\ref{eq:recursionLevinson}) et
$\sigma_1^2=\gamma(0)(1-k_1^2)$, en posant  $p=0$ dans \eqref{eq:recursion_sigma_p+1}.

L'algorithme de \emph{Levinson-Durbin} qui permet de déterminer les coefficients de prédiction
$\{\phi_{m,p}\}_{1\leq m\leq p,1\leq p\leq K}$ à partir de
$\gamma(0),\dots,\gamma(K)$ s'écrit alors de la façon suivante :


% Partant d'une suite de $(K+1)$ coefficients de covariance $\gamma(0),\dots,\gamma(K)$, l'\emph{algorithme de Levinson-Durbin}
% permet de déterminer les coefficients de prédiction
% $\{\phi_{m,p}\}_{1\leq m\leq p,1\leq p\leq K}$\,:

\begin{algorithm}[Levinson-Durbin]
\item[Initialisation] $k_1=\gamma(1)/\gamma(0)$, $\phi_{1,1}=\gamma(1)/\gamma(0)$ et $\sigma_1^2=\gamma(0)(1-k_1^2)$
\item[Récursion] Pour $p=\{2,\dots,K\}$ répéter\,:
\begin{enumerate}[label=\emph{\alph*})]
\item Calculer
\begin{align*}
&k_p=\sigma_{p-1}^{-2} \left( \gamma(p) - \sum_{k=1}^{p-1} \phi_{k,p-1} \gamma(p-k) \right) \\
&\phi_{p,p}=k_p \\
&\sigma_{p}^2=\sigma_{p-1}^2(1-k_p^2)
\end{align*}
\item Pour $m\in \{1,\cdots,p-1\}$ calculer\,:
\[
\phi_{m,p}=\phi_{m,p-1}-k_p\phi_{p-m,p-1}
\]
\end{enumerate}
\end{algorithm}

\begin{proposition}
Soit $(X_t)$ un processus stationnaire au second ordre de fonction d'autocovariance
$\gamma(h)$.
%telle que $\gamma(0)>0$ et $\gamma(h)\to 0$ lorsque $h$
%tend vers l'infini.
Le coefficient $k_{p+1}$ défini par
(\ref{eq:k_{p+1}}) vérifie, pour tout $p\geq 0$ :
\begin{equation}
 \label{eq:defkp}
 k_{p+1}=\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}
        {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}\;,
\end{equation}
et
\begin{equation}
\label{eq:maj:k_p+1}
|k_{p+1}|\leq 1\;.
\end{equation}
\end{proposition}
\begin{proof}\smartqed
En utilisant que $\proj{X_{t}}{\cH_{t-1,p}}$ est orthogonal à
$\epsilon_{t-p-1,p}^-$, (\ref{eq:k_{p+1}}) et (\ref{eq:deferreurforward}), on a
$$
k_{p+1}=\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t-p-1,p}^- \|^2}\;.
$$
Or, d'après (\ref{eq:norme2_epsretro}), $\|\epsilon_{t-p-1,p}^-
\|^2=\sigma_p^2=\|\epsilon_{t,p}^+ \|^2$, la dernière égalité venant
de (\ref{eq:var_pred_dir}), d'où l'on déduit \eqref{eq:defkp}.
L'inégalité \eqref{eq:maj:k_p+1} se déduit alors de \eqref{eq:defkp}
en utilisant l'inégalité de Cauchy-Schwarz.
\qed
\end{proof}

%\begin{proof}\smartqed
%  Notons tout d'abord que
% $\proj{X_{t}}{\cH_{t-1,p}}\perp \epsilon_{t-p-1,p}^-$ puisque
% $\proj{X_{t}}{\cH_{t-1,p}}\in\cH_{t-1,p}$ et que
% $\epsilon_{t-p-1,p}^-\perp \cH_{t-1,p}$. Partant de
% \eqref{eq:defkpP1} on peut écrire que\,:
% \begin{equation}
%  \label{eq:defkp}
%  k_{p+1}
%   =\frac{\pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}}
%     {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}
%   =\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}
%         {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}
% \end{equation}
% En utilisant l'inégalité de Schwarz, on montre que $|k_{p+1}| \leq
% 1$.
% \qed
%\end{proof}
%Dans la littérature, $k_p$ est appelé coefficient d'autocorrélation partielle.
\begin{definition}[Fonction d'autocorrélation partielle]
 \label{def:corrpar}
 Soit $(X_t)$ un processus stationnaire au second ordre de fonction d'autocovariance
$\gamma(h)$. On appelle \emph{fonction d'autocorrélation partielle} la
suite des coefficients d'autocorrélation partielle  $(k_p)_{p \geq 1}$
définie par :
\begin{equation}
 \label{eq:defcorrpart}
 k_p = \corr(X_t,X_{t-1})= \frac{\pscal{X_t}{X_{t-1}}}{\|X_t\| \,\,
   \|X_{t-1}\|},\textrm{ si } p=1\;,
\end{equation}
et
\begin{multline}
k_p=\corr(\epsilon_{t,p-1}^+,\epsilon_{t-p,p-1}^-)\\
           = \dfrac{\pscal{X_{t}-\proj{X_t}{\cH_{t-1,p-1}}}{X_{t-p}-\proj{X_{t-p}}{\cH_{t-1,p-1}}}}
            {\|X_{t}-\proj{X_t}{\cH_{t-1,p-1}}\| \,\,
              \|X_{t-p}-\proj{X_{t-p}}{\cH_{t-1,p-1}}\|},
\textrm{ si } p\geq 2\;.
\end{multline}
\end{definition}

\begin{remark}
Dans \eqref{eq:defcorrpart}, l'expression pour $p=1$ est en accord
avec celle pour $p\geq 2$ dans la mesure où on peut noter que
$\epsilon_{t,0}^+=X_t$ et que $\epsilon_{t-1,0}^-=X_{t-1}$. Notons
aussi que, dans l'expression de $k_p$, $X_t$ et $X_{t-p}$ sont
projetés sur le même sous-espace
$\lspan{X_{t-1},\dots,X_{t-p+1}}$. Le résultat remarquable est
que la suite des coefficients de corrélation partielle est donnée
par\,:
\begin{equation}
 \label{eq:parcoretcoeffpredic}
  k_p=\phi_{p,p}
\end{equation} où $\phi_{p,p}$ est défini au moyen des équations de
Yule-Walker~(\ref{eq:YW1}).
\end{remark}
Dans le cas particulier d'un
processus AR$(m)$ causal, on a alors\,:
$$
 k_p=\left\{
   \begin{matrix}
     \phi_{p,p}&\mbox{pour}& 1\leq p < m\;,\\
     \phi_m&\mbox{pour}& p = m\;,\\
     0&\mbox{pour}& p > m\;.
   \end{matrix}
   \right.
$$
% Notons enfin que contrairement à la fonction d'autocorrélation
% partielle d'un processus AR$(m)$ causal qui vérifie $k_p= 0$ pour tout $p> m$,
% nous avons pour un processus MA$(q)$, $k_p \ne 0$ pour un nombre infini de termes.
% Il est toutefois possible de montrer qu'il existe un réel $\rho$, $0 < \rho < 1$, et une constante $C$, telle
% que, pour tout $p \geq 1$, $|k_p| \leq C \rho^k$.
% \clnote{il faut montrer les résultats sur le MA(q).}
%==================================================================
%==================================================================
%==================================================================
% \section{Algorithme de Schur}
% %==================================================================
% %==================================================================

% \clnote{Peut-être qu'on pourrait mettre l'algo de Schur en exercice ?}

% Partant des coefficients d'autocorrélation, l'algorithme de
% Levinson-Durbin évalue à la fois les coefficients des
% prédicteurs linéaires optimaux et les coefficients
% d'autocorrélation partielle. Dans certains cas, seuls les
% coefficients d'autocorrélation partielle sont nécessaires. Il en
% est ainsi, par exemple, lorsque l'on cherche à calculer les
% erreurs de prédiction directe et rétrograde à partir du
% processus $X_t$. Montrons, en effet, que les erreurs de prédiction
% à l'ordre $(p+1)$ s'expriment, en fonction des erreurs de
% prédictions à l'ordre $p$, à l'aide d'une formule de
% récurrence ne faisant intervenir que la valeur du coefficient de
% corrélation partielle\,:
% \begin{equation}
%  \label{eq:celluleanalyse}
% \begin{cases}
% \epsilon_{t,p+1}^+= \epsilon_{t,p}^+ -k_{p+1}\epsilon_{(t-1)-p,p}^- \\
% \epsilon_{t-(p+1),p+1}^-=\epsilon_{(t-1)-p,p}^- -k_{p+1}\epsilon^+_{t,p}
% \end{cases}
% \end{equation}
% Reprenons les expressions de l'erreur de prédiction directe et de
% l'erreur de prédiction rétrograde\,:
% \begin{eqnarray*}
%  \epsilon_{t,p}^+ = X_t - \sum_{k=1}^p \phi_{k,p} X_{t-k}
%  &\mbox{et}&
%  \epsilon_{t-p-1,p}^- = X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k}
% \end{eqnarray*}
% En utilisant directement la récursion de Levinson-Durbin,
% équations \eqref{eq:recursionLevinson}, dans l'expression de
% l'erreur de prédiction directe à l'ordre $p+1$, nous
% obtenons\,:
% \begin{align}
% \label{eq:predictiondirecte}
%  \epsilon_{t,p+1}^+
%   &=  X_t - \sum_{k=1}^{p+1} \phi_{k,p+1} X_{t-k}
%   \nonumber
%   \\&
%   = \left( X_t - \sum_{k=1}^p \phi_{k,p} X_{t-k} \right)
%      - k_{p+1}
%      \left( X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k} \right)
%   \nonumber
%   \\
%   &= \epsilon_{t,p}^+ - k_{p+1} \epsilon_{t-p-1,p}^-
% \end{align}
% De façon similaire, nous avons\,:
% \begin{align}
% \label{eq:predictionretrograde}
%  \epsilon_{t-p-1,p+1}^-
%   &=  X_{t-p-1} - \sum_{k=1}^{p+1} \phi_{k,p+1} X_{t-p-1+k}
%   \nonumber
%   \\&
%   = \left( X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k} \right)
%      - k_{p+1}
%      \left( X_{t} - \sum_{k=1}^p \phi_{k,p} X_{t-k} \right)
%   \nonumber
%   \\
%    &= \epsilon_{t-p-1,p}^- - k_{p+1} \epsilon_{t,p}^+
% \end{align}
% Partant de la suite des autocorrélations, l'algorithme de Schur
% calcule récursivement les coefficients de corrélation partielle,
% sans avoir à déterminer les valeurs des coefficients de
% prédiction. Historiquement, l'algorithme de Schur a été introduit
% pour tester le caractère défini positif d'une suite (ou de
% façon équivalente, la positivité des matrices de Toeplitz
% construites à partir de cette suite). En effet, comme nous
% l'avons montré ci-dessus, une suite de coefficients de covariance
% est définie positive si et seulement si les coefficients de
% corrélation partielle sont de module strictement inférieur à
% $1$. Déterminons à présent cet algorithme. En faisant $t=0$
% dans l'équation \eqref{eq:predictiondirecte}, en multipliant à
% gauche par $X_m$ et en utilisant la stationnarité, il vient\,:
% \begin{equation}
%  \label{eq:kpplus}
%  \pscal{X_m}{\epsilon_{0,p+1}^+}
%  =
%  \pscal{X_m}{\epsilon_{0,p}^+} - k_{p+1} \pscal{X_{m}}{\epsilon_{-p-1,p}^-}
%  =
%  \pscal{X_m}{\epsilon_{0,p}^+} - k_{p+1} \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} \eqsp.
% \end{equation}
% En faisant $t=p+1$ dans l'équation
% \eqref{eq:predictionretrograde}, en multipliant à gauche par
% $X_{m+p+1}$ et en utilisant la stationnarité, il vient\,:
% \begin{multline}
%  \label{eq:kpmoins}
%  \pscal{X_{m+p+1}}{\epsilon_{0,p+1}^-}
%  =
%  \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} - k_{p+1}\pscal{X_{m+p+1}}{\epsilon_{p+1,p}^+}\\
%  =
%  \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} - k_{p+1}\pscal{X_{m}}{\epsilon_{0,p}^+}  \eqsp.
% \end{multline}
% En faisant $m=0$  dans \eqref{eq:kpmoins}, il vient\,:
% \begin{equation}
%   \label{eq:kpmoinsen0}
%  \pscal{X_{p+1}}{\epsilon_{0,p+1}^-}
%  =
%  \pscal{X_{p+1}}{\epsilon_{0,p}^-} - k_{p+1} \pscal{X_{p+1}}{\epsilon_{p+1,p}^+}
%  = \pscal{X_{p+1}}{\epsilon_{0,p}^-} - k_{p+1} \pscal{X_0}{\epsilon_{0,p}^+} \eqsp.
% \end{equation}
% Mais on a aussi\,:
% \[
% \pscal{X_{p+1}}{\epsilon_{0,p+1}^-}
%  = \pscal{X_{p+1}}{ X_0 - \proj{X_0}{\lspan{X_1, \cdots, X_{p+1}}}} = 0 \eqsp.
% \]
% Nous pouvons donc déduire de l'équation \eqref{eq:kpmoinsen0}\,:
% \begin{equation}
% \label{eq:kpshur}
%   k_{p+1} = \frac{\pscal{X_{p+1}}{ \epsilon_{0,p}^-}}{\pscal{X_0}{\epsilon_{0,p}^+}}
% \end{equation}
% En couplant les équations \eqref{eq:kpplus}, \eqref{eq:kpmoins} et
% \eqref{eq:kpshur} et en partant des conditions initiales\,:
% \[
%   \pscal{X_m}{\epsilon_{0,0}^+}= \gamma(m)
%   \quad\mbox{et}\quad
%   \pscal{X_{m+1}}{\epsilon_{0,0}^-}=\gamma(m+1) \eqsp.
% \]
% on peut déterminer les coefficients de corrélation partielle
% directement, sans avoir à évaluer explicitement les
% coefficients de prédiction.

% On note $u(m,p)=\pscal{X_m}{\epsilon_{0,p}^+}$ et $v(m,p)=\pscal{X_{m+p+1}}{\epsilon_{0,p}^-}$.
% Partant des $(K+1)$ coefficients de covariance
% $\{\gamma(0),\dots,\gamma(K)\}$, l'{\em algorithme de Schur}
% calcule les $K$ premiers coefficients de corrélation partielle\,:
% \begin{description}
% \item[Initialisation] Pour $m=\{0,\dots,K-1\}$\,:
% \begin{align*}
% &u(m,0)=\gamma(m) \\
% &v(m,0)=\gamma(m+1)
% \end{align*}
% \item[Récursion]
% \begin{enumerate}[label=\emph{\alph*})]
% \item Pour $p=\{1,\dots,K\}$, calculer
% \[k_p = \frac{v(0,p-1)}{u(0,p-1)} \]
% \item Pour $m=\{0,\dots,K-p-1\}$ calculer\,:
%   $$
%   \begin{cases}
%   u(m,p)=u(m,p-1)-k_pv(m,p-1) \\
%   v(m,p)=v(m+1,p-1)-k_pu(m+1,p-1)
%   \end{cases}\eqsp.
%   $$
% \end{enumerate}
% \end{description}
% La complexité de l'algorithme de Schur est équivalente à
% l'algorithme de Levinson.
% %==========================================================
% \subsubsection{Filtres en treillis}
% %==========================================================
% En notant $e(t,p)=[\epsilon_{t,p}^+\quad \epsilon_{t-p,p}^-]^T$ et
% en utilisant l'opérateur de retard $B$, les expressions
% \eqref{eq:celluleanalyse} peuvent se mettre sous la forme
% matricielle\,:
% $$
%  e(t,p+1)=
%  \left [
%  \begin{matrix}
%    1&-k_{p+1}B \cr -k_{p+1}B&1
%  \end{matrix}
%  \right ]
%  e(t,p)
% $$
% Les erreurs initiales ($p=0$) sont $e(t,0)=[X_t\quad X_t]^T$. Ces
% équations débouchent sur une structure de filtrage dite en
% treillis qui calcule, au moyen des coefficients de corrélation
% partielle, les erreurs de prédiction directe et rétrograde à
% partir du processus $\{X_t, t \in \Zset\}$. Ce filtre d'analyse est représenté figure
% \ref{fig:anatreillis}.
%  %================= FIGURE
%  %====== FIGURE
%  \figtit{\FIGPREDIC treillisanalyse}
%  {Filtre d'analyse en treillis. Ce filtre permet de construire les erreurs de
%  prédiction directe et rétrograde à partir du processus et de la donnée
%  des coefficients de corrélation partielle.}
%  {fig:anatreillis}
% Les équations \eqref{eq:celluleanalyse} peuvent encore s'écrire\,:
% $$
% \begin{cases}
% \epsilon_{t,p}^+=\epsilon_{t,p+1}^+ +k_{p+1}\epsilon_{(t-1)-p,p}^- \\
% \epsilon_{t-(p+1),p+1}^-=\epsilon_{(t-1)-p,p}^-k_{p+1}\epsilon_{t,p}^+
% \end{cases}
% $$
% qui donne le schéma de filtrage de la figure
% \ref{fig:syntreillis}.
%  %================= FIGURE
%  %====== FIGURE
%  \figtit{\FIGPREDIC treillissynthese}
%  {Filtre de synthèse en treillis. Ce filtre permet de reconstruire
%  le processus à partir de la suite des erreurs de
%  prédiction directe et de la donnée
%  des coefficients de corrélation partielle.}
%  {fig:syntreillis}
% %==========================================================
% %==========================================================
\section{Algorithme des innovations}
\label{sec:algorithmes-des-innovations}
L'algorithme des innovations est une application directe de la méthode de Gram-Schmidt et est, à cet
égard, plus élémentaire que l'algorithme de Levinson-Durbin. De plus, il ne suppose pas que le processus
$(X_t)_{t \in \Zset}$ soit stationnaire. L'espérance de $X_t$ étant
supposée nulle dans ce chapitre, nous notons
\[
\kappa(i,j)= \pscal{X_i}{X_j}= \PE{X_iX_j} \eqsp,
\]
la fonction d'autocovariance de ce processus.
% Nous supposerons dans tout ce paragraphe que
% la matrice $[\kappa(i,j)]_{i,j=1}^n$ est inversible pour tout $n \geq
% 1$.
Notons, pour $n \geq 1$,
$$\cH_n= \lspan{X_1,\dots,X_n} \textrm{ et }
\sigma_n^2= \| X_{n+1} - \proj{X_{n+1}}{\cH_n}\|^2\;.
$$
La procédure d'orthogonalisation de Gram-Schmidt permet alors d'écrire
pour tout $n \geq 1$ :
\[
\cH_n= \lspan{X_1, X_2 - \proj{X_2}{X_1}, \dots, X_n - \proj{X_n}{\cH_{n-1}}} \eqsp,
\]
où on utilise la convention suivante : $\proj{X_1}{\cH_{0}}=0$.
On a alors :
\begin{equation}
\label{eq:definition-projecteur}
\proj{X_{n+1}}{\cH_n}= \sum_{j=1}^n \theta_{n,j} \left( X_{n+1-j} - \proj{X_{n+1-j}}{\cH_{n-j}}\right) \eqsp.
\end{equation}
L'algorithme des innovations décrit dans la proposition suivante
fournit une méthode récursive permettant de calculer
$(\theta_{n,j})_{1\leq j\leq n}$ et $\sigma_n^2$ pour $n\geq 1$.

\begin{proposition}
Soit $(X_t)$ un processus à moyenne nulle tel que la matrice
$[\kappa(i,j)]_{1\leq i,j\leq n}$ soit inversible pour tout $n\geq 1$
alors
$$
\proj{X_{n+1}}{\cH_n}=
\begin{cases}
0\;, \textrm{ si } n=0\;,\\
 \sum_{j=1}^n \theta_{n,j} \left( X_{n+1-j} -
   \proj{X_{n+1-j}}{\cH_{n-j}}\right)\;, \textrm{ si } n\geq 1\;,
\end{cases}
$$
où
$$
\begin{cases}
\sigma_0^2=\kappa(1,1)\;,\\
\theta_{n,n-k}= \sigma_{k}^{-2} \left[ \kappa(n+1,k+1) -
  \sum_{j=0}^{k-1} \theta_{k,k-j} \theta_{n,n-j} \sigma_{j}^2
\right]\;,\; 0\leq k\leq n-1\;,\\
\sigma_{n}^2= \kappa(n+1,n+1) - \sum_{j=0}^{n-1} \theta^2_{n,n-j}
\sigma_{j}^2\;,\; n\geq 1 \;.
\end{cases}
$$
\end{proposition}

\begin{proof}\smartqed
Remarquons tout d'abord que les vecteurs $(X_i -
\proj{X_i}{\cH_{i-1}})_{i \geq 1}$ sont orthogonaux. En effet,
pour $i < j$, $X_i - \proj{X_i}{\cH_{i-1}} \in \cH_{j-1}$ et $X_j -
\proj{X_j}{\cH_{j-1}} \perp \cH_{j-1}$.
On en déduit, en faisant le produit scalaire de
\eqref{eq:definition-projecteur} par $X_{k+1}-\proj{X_{k+1}}{\cH_k}$
que, pour $0 \leq k < n$ :
\[
\pscal{\proj{X_{n+1}}{\cH_n}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}}= \theta_{n,n-k} \sigma_{k}^2 \eqsp.
\]
Puisque $\pscal{X_{n+1}-\proj{X_{n+1}}{\cH_n}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}}=0$, les coefficients $\theta_{n,n-k}$,
$k=0,\dots,n-1$ sont donnés par
\begin{equation}\label{eq:theta_n_n-k}
\theta_{n,n-k}= \sigma_{k}^{-2} \pscal{X_{n+1}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}} \eqsp.
\end{equation}
En utilisant la représentation \eqref{eq:definition-projecteur},
\begin{multline*}
\proj{X_{k+1}}{\cH_k}=\sum_{j=1}^k \theta_{k,j} \left( X_{k+1-j} -
  \proj{X_{k+1-j}}{\cH_{k-j}}\right) \\
=\sum_{j=0}^{k-1} \theta_{k,k-j} \left( X_{j+1} -
  \proj{X_{j+1}}{\cH_{j}}\right) \eqsp,
\end{multline*}
d'où l'on déduit que
\[
\theta_{n,n-k}= \sigma_{k}^{-2} \left( \kappa(n+1,k+1) - \sum_{j=0}^{k-1} \theta_{k,k-j} \pscal{X_{n+1}}{X_{j+1}-\proj{X_{j+1}}{\cH_j}}\right) \eqsp.
\]
D'après \eqref{eq:theta_n_n-k},
 $\pscal{X_{n+1}}{X_{j+1}-\proj{X_{j+1}}{\cH_j}}= \sigma_{j}^{2} \theta_{n,n-j}$ pour $0 \leq j < n$, nous avons donc pour
$k \in \{1,\dots,n-1\}$,
\begin{equation}
\label{eq:mise-a-jour-theta}
\theta_{n,n-k}= \sigma_{k}^{-2} \left( \kappa(n+1,k+1) - \sum_{j=0}^{k-1} \theta_{k,k-j} \theta_{n,n-j} \sigma_{j}^2 \right) \eqsp.
\end{equation}
L'équation \eqref{eq:mise-a-jour-theta} est encore valable lorsque
$k=0$ en utilisant la convention que la somme sur $j$ dans le membre
de droite est nulle dans ce cas.
Par ailleurs, la proposition \ref{prop:projecteur} (Pythagore) implique que
\begin{multline}
\label{eq:mise-a-jour-sigma}
\sigma_{n}^2= \| X_{n+1} - \proj{X_{n+1}}{\cH_n} \|^2= \| X_{n+1} \|^2 - \| \proj{X_{n+1}}{\cH_n} \|^2 \\
= \kappa(n+1,n+1) - \sum_{k=0}^{n-1} \theta^2_{n,n-k} \sigma_{k}^2 \eqsp.
\end{multline}
\qed
\end{proof}

Alors que l'algorithme de Levinson-Durbin permet de déterminer
les coefficients du développement de $\proj{X_{n+1}}{\cH_n}$ sur
$X_1,\dots,X_n$ donnés par $\proj{X_{n+1}}{\cH_n}= \sum_{j=1}^n \phi_{n,j} X_{n+1-j}$,
l'algorithme des innovations calcule les coefficients du développement
de
$\proj{X_{n+1}}{\cH_n}$ sur $X_1$, $X_2 -
\proj{X_2}{X_1}$,
$\dots$, $X_{n} - \proj{X_n}{\cH_{n-1}}$.



\begin{example}[Prédiction d'un processus MA(1)]
Considérons le processus $X_t = Z_t + \theta Z_{t-1}$ où $(Z_t) \sim \BB(0,\sigma^2)$. Nous avons donc $\kappa(i,j)= 0$ pour $|i-j| > 1$,
$\kappa(i,i)= \sigma^2(1+\theta^2)$ et $\kappa(i,i+1)= \theta
\sigma^2$. Dans ce cas, nous avons

$$
\begin{cases}
 \theta_{n,j}= 0\;,\; 2 \leq j \leq n \;, \\
 \theta_{n,1}= \sigma_{n-1}^{-2} \theta \sigma^2 \;,\\
\end{cases}
$$
et les variances des innovations qui sont données par
$$
\begin{cases}
 \sigma_0^{2} = (1+\theta^2) \sigma^2 \eqsp, \\
\sigma_{n}^{2}= [1 + \theta^2 - \sigma_{n-1}^{-2} \theta^2 \sigma^2] \sigma^2 \eqsp.\\
\end{cases}
$$
Si nous posons $r_n = \sigma_n^2/\sigma^2$, nous avons
\[
\proj{X_{n+1}}{\cH_n}= \theta \left(X_n - \proj{X_n}{\cH_{n-1}}\right)/r_{n-1} \eqsp,
\]
avec $r_0=1+\theta^2$, et pour $n \geq 1$, $r_{n+1}= 1+\theta^2-\theta^2/r_n$.
\end{example}

%======================================================================
%======================================================================
%======================================================================



%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "francais"
%%% TeX-master: "../monographie-serietemporelle"
%%% End:
