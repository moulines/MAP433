% \section{th\'eor\`eme de Projection}
%     \subsection{Espace de Hilbert}
%     \subsection{Bases orthonormales}
%     \subsection{th\'eor\`eme de projection}
% \section{Algorithmes de Levinson-Durbin}
%================================================
%================================================
\section{Pr\'ediction lin\'eaire de processus stationnaires}
Soit $(X_t)_{t\in\Zset}$ un processus stationnaire au
second ordre \`a valeurs r\'eelles, \textbf{d'esp\'erance nulle} et de fonction
d'autocovariance $\gamma(h)= \cov(X_h,X_0)$. On cherche \`a
\emph{pr\'edire} la valeur du processus \`a la date $t$ \`a partir
d'une combinaison lin\'eaire des $p$ derniers \'echantillons du pass\'e
$X_{t-1}, \dots, X_{t-p}$. La meilleure combinaison lin\'eaire
(\textit{i.e.} le pr\'edicteur lin\'eaire optimal)
est la projection orthogonale de $X_t$ sur
$\cH_{t-1,p}$  not\'ee $\proj{X_t}{\cH_{t-1,p}}$, o\`u $\cH_{t-1,p}$ est
d\'efini par :
\begin{equation}
\label{eq:cht}
\cH_{t-1,p} = \lspan{X_{t-1}, X_{t-2}, \cdots, X_{t-p}}\;.
\end{equation}
Les indices dans la notation $\cH_{t-1,p}$ doivent \^{e}tre compris ainsi
: $\cH_{t-1,p}$ est le sous-espace vectoriel engendr\'e par les
$p$ observations pr\'ec\'edant $X_{t-1}$ \`a savoir
$X_{t-1}, \dots, X_{t-p}$.
D'apr\`es le th\'eor\`eme \ref{theo:projection},
\begin{equation}\label{eq:def_phi_kp}
\proj{X_t}{\cH_{t-1,p}}=\sum_{k=1}^p \phi_{k,p} X_{t-k}\;,
\end{equation}
o\`u les coefficients $(\phi_{k,p})_{1\leq k\leq p}$ satisfont
\begin{equation}\label{eq:scal_1}
\left\langle X_t-\sum_{k=1}^p \phi_{k,p} X_{t-k},X_{t-j}\right\rangle=0\;,\;
j=1,\dots,p\;,
\end{equation}
la notation $\left\langle \cdot,\cdot\right\rangle$ correspondant au
produit scalaire dans $\ltwo\espaceproba$ d\'efini pour $X$ et $Y$
dans $\ltwo\espaceproba$ par  $\left\langle X,Y\right\rangle=\PE{XY}$.
L'\'equation (\ref{eq:scal_1}) se r\'e\'ecrit encore sous la forme
\begin{equation}\label{eq:scal_2}
\left\langle X_t,X_{t-j}\right\rangle=\sum_{k=1}^p \phi_{k,p}
\left\langle  X_{t-k},X_{t-j}\right\rangle\;,\;
j=1,\dots,p\;,
\end{equation}
soit encore
\begin{equation}\label{eq:scal_3}
\sum_{k=1}^p \phi_{k,p}\gamma(k-j)=\gamma(j)\;,\;
j=1,\dots,p\;.
\end{equation}
En posant $\Gamma_p$ la matrice de covariance
du vecteur $(X_{t-1},\dots, X_{t-p})$ d\'efinie par
\begin{equation*}
\Gamma_p =
 \left[
 \begin{matrix}
  \gamma(0)  & \gamma(1)  &  \cdots    &            & \gamma(p-1) \\
  \gamma(1)  & \gamma(0)  &  \gamma(1) &            & \vdots \\
  \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
  \vdots     &            &            &            &\gamma(1)  \\
  \gamma(p-1)& \gamma(p-2)&  \cdots    & \gamma(1)  & \gamma(0)
 \end{matrix}
 \right]\;,
\end{equation*}
on peut r\'e\'ecrire (\ref{eq:scal_3}) comme suit :
\begin{equation}\label{eq:YW1}
 \Gamma_p \bfphi_p = \bfgamma_p\;,
\end{equation}
o\`u $\bfphi_p=(\phi_{1,p},\dots,\phi_{p,p})^T$ et
$\bfgamma_p=(\gamma(1), \gamma(2), \cdots, \gamma(p))^T$.

\begin{definition}
\index{Innovation!partielle}
Nous appellerons dans la suite
\emph{erreur de pr\'ediction directe} d'ordre $p$ ou
\emph{innovation partielle} d'ordre $p$ le processus\,:
\begin{equation}
\label{eq:deferreurforward}
 \epsilon_{t,p}^+
 = X_t - \proj{X_t}{\cH_{t-1,p}}
 = X_t - \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;.
\end{equation}
La variance de l'erreur de pr\'ediction directe d'ordre $p$ est not\'ee
$\sigma_p^2$ et d\'efinie par
\begin{equation}\label{eq:var_pred_dir}
\sigma_p^2=\|X_t - \proj{X_t}{\cH_{t-1,p}}\|^2=\PE{|X_t -
  \proj{X_t}{\cH_{t-1,p}}|^2}\;.
\end{equation}
\end{definition}
D'apr\`es (\ref{eq:def_phi_kp}) et la proposition \ref{prop:projecteur},
la variance de l'erreur de pr\'ediction directe d'ordre $p$ a pour expression :
\begin{equation}
 \label{eq:YW2}
 \sigma_p^2 = \pscal{X_t}{X_t - \proj{X_t}{\cH_{t-1,p}}}
            =\gamma(0)- \sum_{k=1}^p \phi_{k,p}\gamma(k)
            =\gamma(0)-\bfphi_p^T\bfgamma_p\;.
\end{equation}
Les \'equations \eqref{eq:YW1} et \eqref{eq:YW2} sont appel\'ees les
\emph{\'equations de Yule-Walker}.
% Notons la propri\'et\'e importante
% suivante~: pour $p$ fix\'e, la suite des coefficients
% $\{\phi_{k,p}\}_{1\leq k \leq p}$ du pr\'edicteur lin\'eaire optimal
% et la variance de l'erreur minimale de pr\'ediction {\em ne
% d\'ependent pas de $t$}.

Notons que (\ref{eq:YW1}) a une unique solution si et seulement si
la matrice $\Gamma_p$ est inversible auquel cas la solution vaut :
\begin{equation}\label{eq:sol_unique}
\bfphi_p=\Gamma_p^{-1} \bfgamma_p\; .
\end{equation}
La proposition \ref{prop:Gammanrangplein} fournit les conditions suffisantes assurant
que $\Gamma_p$ est inversible pour tout $p$.  On a ainsi des conditions
sous lesquelles on peut calculer le pr\'edicteur de $X_t$ \`a partir de
$X_{t-1},\dots,X_{t-p}$.



% Ce probl\`eme est bien entendu
% un cas particulier du probl\`eme pr\'ec\'edent o\`u nous avons $X=
% X_t$ et $Y_k = X_{t-k}$, pour $k \in \{1, \dots, p \}$ et
% o\`u\,:
% \begin{equation}
% \label{eq:cht}
%   \cH_{t-1,p} = \lspan{1, X_{t-1}, X_{t-2}, \cdots, X_{t-p}}
% \end{equation}
% Formons la matrice de covariance $\Gamma_p$ du vecteur $[X_{t-1},
% \cdots, X_{t-p}]$:
% \begin{equation}
% \Gamma_p =
%  \left[
%  \begin{matrix}
%   \gamma(0)  & \gamma(1)  &  \cdots    &            & \gamma(p-1) \\
%   \gamma(1)  & \gamma(0)  &  \gamma(1) &            & \vdots \\
%   \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
%   \vdots     &            &            &            &\gamma(1)  \\
%   \gamma(p-1)& \gamma(p-2)&  \cdots    & \gamma(1)  & \gamma(0)
%  \end{matrix}
%  \right]
% \end{equation}
% Cette matrice est dite de Toeplitz, ses \'el\'ements \'etant \'egaux
% le long de ses diagonales. Notons $\bfgamma_p$ le vecteur
% $[\gamma(1), \gamma(2), \cdots, \gamma(p)]^T$ le vecteur des
% coefficients de corr\'elation. D'apr\`es l'\'equation
% \eqref{eq:Eqsnormales}, les coefficients $\{\phi_{k,p}\}_{1\leq
% k\leq p}$ du pr\'edicteur lin\'eaire optimal d\'efini par\,:
% \begin{equation}
%  \label{eq:formegenedeXtsurH}
%  \proj{X_t}{\cH_{t-1,p}} - \mu=\sum_{k=1}^p \phi_{k,p} (X_{t-k}-\mu)
% \end{equation}
% sont solutions du syst\`eme d'\'equations\,:
% \begin{equation}
%  \label{eq:YW1}
%  \Gamma_p \bfphi_p = \bfgamma_p \hspace{1cm}
% \end{equation}
% D'autre part l'erreur de pr\'ediction minimale a pour expression\,:
% \begin{eqnarray}
%  \label{eq:YW2}
%  &\sigma_p^2 &= \| X_t - \proj{X_t}{\cH_{t-1,p}}\|^2
%             = \pscal{X_t-\mu}{X_t - \proj{X_t}{\cH_{t-1,p}}} \nonumber \\
%             &&=\gamma(0)- \sum_{k=1}^p \phi_{k,p}\gamma(k)
%             =\gamma(0)-\bfphi_p^T\bfgamma_p
% \end{eqnarray}
% Les \'equations \eqref{eq:YW1} et \eqref{eq:YW2} sont appel\'ees
% \emph{\'equations de Yule-Walker}. Notons la propri\'et\'e importante
% suivante~: pour $p$ fix\'e, la suite des coefficients
% $\{\phi_{k,p}\}_{1\leq k \leq p}$ du pr\'edicteur lin\'eaire optimal
% et la variance de l'erreur minimale de pr\'ediction {\em ne
% d\'ependent pas de $t$}.

% Les \'equations \eqref{eq:YW1} et
% \eqref{eq:YW2} peuvent encore \^{e}tre r\'e\'ecrites \`a partir des
% coefficients de corr\'elation $\rho(h)=\gamma(h)/\gamma(0)$. Il
% vient\,:
% \begin{equation}
%  \label{eq:YWcorrelation}
%  \left[
%  \begin{matrix}
%   \rho(0)  & \rho(1)  &  \cdots    &            & \rho(p-1) \\
%   \rho(1)  & \rho(0)  &  \rho(1) &            & \vdots \\
%   \vdots     &\ddots      &  \ddots    &  \ddots    &     \\
%   \vdots     &            &            &            &\rho(1)  \\
%   \rho(p-1)& \rho(p-2)&  \cdots    & \rho(1)  & \rho(0)
%  \end{matrix}
%  \right]
%  \left[
%  \begin{matrix}
%   \phi_{1,p}\\
%   \phi_{2,p}\\
%   \vdots\\
%   \vdots \\
%   \phi_{p,p}
%  \end{matrix}
%  \right]
%  =
%   \left[
%  \begin{matrix}
%   \rho(1)\\
%   \rho(2)\\
%   \vdots\\
%   \vdots \\
%   \rho(p)
%  \end{matrix}
%  \right]
% \end{equation}
%========================================================
\begin{example}[Cas d'un processus AR$(m)$ causal]
Soit $(X_t)$ le processus AR$(m)$ causal solution de
l'\'equation r\'ecurrente\,:
\begin{equation}\label{eq:def_ar}
  X_t=\phi_1X_{t-1}+\cdots+\phi_m X_{t-m}+Z_t\;,
\end{equation}
o\`u $Z_t\sim \BB(0,\sigma^2)$ et o\`u
$\phi(z)=1-\sum_{k=1}^m\phi_k z^{k}\neq 0$ lorsque $|z|\leq 1$.
Dans ce cas, pour tout $p\geq m$ :
$$
 \phi_{k,p}=
 \begin{cases}
    \phi_k,&\text{lorsque }  1\leq k\leq m\;,\\
    0,& \text{lorsque }  m< k\leq p \;.
 \end{cases}
$$
En effet, $(X_t)$ \'etant causal on a, pour tout $h\geq 1$,
$\PE{Z_tX_{t-h}}=0$ et donc, d'apr\`es (\ref{eq:def_ar}),
$\PE{(X_t-\sum_{k=1}^m\phi_kX_{t-k})X_{t-h}}=0$.
Ainsi, pour tout $p\geq m$, $\sum_{k=1}^m\phi_kX_{t-k} \in
\cH_{t-1,p}$ et $(X_t-\sum_{k=1}^m\phi_kX_{t-k})\perp \cH_{t-1,p}$
et donc, d'apr\`es le th\'eor\`eme \ref{theo:projection}, pour tout $p\geq m$,
$$
\sum_{k=1}^m\phi_kX_{t-k}=\proj{X_t}{\cH_{t-1,p}}\;.
$$
% La projection orthogonale d'un AR$(m)$ causal sur son pass\'e de longueur $p\geq m$ co\"{i}ncide avec la projection
% orthogonale sur les $m$ derni\`eres valeurs et les coefficients
% de pr\'ediction sont pr\'ecis\'ement les coefficients de l'\'equation
% r\'ecurrente.
\end{example}
% Dans le cas o\`u la matrice de covariance $\Gamma_p$, suppos\'ee
% \emph{connue}, est inversible, le probl\`eme de la
% d\'etermination des coefficients de pr\'ediction $\bfphi_p$ et de la
% variance de l'erreur de pr\'ediction $\sigma_p^2$ a une solution
% unique. Rappelons que, d'apr\`es la propri\'et\'e
% \ref{prop:Gammanrangplein}, si $\gamma(0)>0$ et si $\limn
% \gamma(n)=0$, alors la matrice $\Gamma_p$ est inversible \`a
% tout ordre.

% Il est facile de d\'emontrer que\,:
% \begin{multline}
%  \label{eq:onpeutcenter}
%  \proj{X_t}{\lspan{1,X_{t-1},\dots,X_{t-p}}}
% \\ =\mu+
%  \proj{X_t-\mu}{\lspan{X_{t-1}-\mu,\dots,X_{t-p}-\mu}} \eqsp.
% \end{multline}
% Par cons\'equent, dans le probl\`eme de la pr\'ediction,
% il n'y a aucune perte de g\'en\'eralit\'e \`a consid\'erer que le
% processus est centr\'e. S'il ne l'\'etait pas, il suffirait,
% d'apr\`es l'\'equation \eqref{eq:onpeutcenter}, d'effectuer le
% calcul des pr\'edicteurs sur le processus centr\'e $X_t^c=X_t-\mu$
% puis d'ajouter $\mu$. \emph{Dans la suite, sauf indication
% contraire, les processus sont suppos\'es centr\'es}.

Les
coefficients de pr\'ediction d'un processus stationnaire au second
ordre fournissent une d\'ecomposition particuli\`ere de la matrice
de covariance $\Gamma_{p+1}$ sous la forme d'un produit de matrices
triangulaires explicit\'ee dans le th\'eor\`eme \ref{theo:choleski}.
\begin{theorem}
 \label{theo:choleski} Soit $(X_t)$ un processus stationnaire au second
ordre, centr\'e, de fonction d'autocovariance $\gamma(h)$. On
note\,:
\[
A_{p+1} = \left[
\begin{matrix}
   1            & 0             & \cdots & \cdots      & 0 \\
   - \phi_{1,1} & 1             & \ddots &             & \vdots \\
   \vdots       &               & \ddots & \ddots      & \vdots \\
   \vdots       &               &        & \ddots      & 0 \\
   - \phi_{p,p} & - \phi_{p-1,p}& \cdots &- \phi_{1,p} &1
\end{matrix}
\right] \text{ et } D_{p+1} = \left[
\begin{matrix}
\sigma^2_0 & 0          & \cdots & 0 \\
0          & \sigma_1^2 & \cdots & 0 \\
\vdots     &            &        & \vdots \\
0          &            & \cdots & \sigma_p^2
\end{matrix}
\right]\;,
\]
o\`u les coefficients $(\phi_{k,p})_{1\leq k\leq p}$ et
$(\sigma_k^2)_{1\leq k\leq p}$ sont respectivement d\'efinis dans
(\ref{eq:def_phi_kp}) et (\ref{eq:var_pred_dir}).
On a alors\,:
\begin{equation}
 \label{eq:decompocholeski}
 \Gamma_{p+1} = A_{p+1}^{-1} D_{p+1} (A_{p+1}^T)^{-1}\;.
\end{equation}
\end{theorem}
\begin{proof}\smartqed
Pour simplifier les notations, posons $\cH_k =\cH_{k,k}= \lspan{X_k, \cdots, X_1}$ et montrons tout
d'abord que, pour $k \neq \ell$, nous avons\,:
\begin{equation}
 \label{eq:erreurblanche}
 \pscal{X_k - \proj{X_k}{\cH_{k-1}}}{X_{\ell} - \proj{X_{\ell}}{\cH_{{\ell}-1}}} = 0 \eqsp.
\end{equation}
En effet, pour $k < \ell$, on a $X_{k} - \proj{X_{k}}{\cH_{k-1}} \in
\cH_k\subseteq\cH_{\ell-1}$ et $X_{\ell} -
\proj{X_{\ell}}{\cH_{\ell-1}} \perp \cH_{\ell-1}$.
D'autre part, si on note ${\bf X}_{p+1}$ le vecteur :
$(X_1,\dots,X_{p+1})^T$, alors,
par d\'efinition des coefficients de pr\'ediction (\ref{eq:def_phi_kp}), on peut \'ecrire :
\[
A_{p+1} {\bf X}_{p+1} = \left[
\begin{array}{llll}
1            & 0             & \cdots & 0 \\
- \phi_{1,1} & 1             & \cdots & 0 \\
\vdots       &               &        & \vdots \\
- \phi_{p,p} & - \phi_{p-1,p}& \cdots & 1
\end{array}
\right] \left[
\begin{array}{l}
X_{1} \\
X_{2} \\
\vdots \\
X_{p+1}
\end{array}
\right] = \left [
\begin{array}{l}
X_1 \\
X_2 - \proj{X_2}{\cH_1} \\
\vdots \\
X_{p+1} - \proj{X_{p+1}}{\cH_{p}}
\end{array}
\right ]\;,
\]
qui donne\,:
$$
 \PE{A_{p+1} {\bf X}_{p+1} {\bf X}_{p+1}^T A_{p+1}^T}
 =D_{p+1}\;,
$$
d'apr\`es \eqref{eq:erreurblanche} et (\ref{eq:var_pred_dir}).
Par ailleurs,
$$
\PE{A_{p+1} {\bf X}_{p+1} {\bf X}_{p+1}^T A_{p+1}^T}
= A_{p+1} \Gamma_{p+1}A_{p+1}^T\;,
$$
ce qui d\'emontre \eqref{eq:decompocholeski} puisque la
matrice $A_{p+1}$ est inversible, son d\'eterminant \'etant \'egal \`a
$1$.

\end{proof}
% Dans la suite nous notons
% $\cH_{t-1,p}=\lspan{X_{t-1},\dots,X_{t-p}}$ et nous appelons
% \emph{erreur de pr\'ediction directe} d'ordre $p$ ou
% \emph{innovation partielle} d'ordre $p$ le processus\,:
% \begin{equation}
% \label{eq:deferreurforward}
%  \epsilon_{t,p}^+
%  = X_t - \proj{X_t}{\cH_{t-1,p}}
%  = X_t - \sum_{k=1}^{p} \phi_{k,p} X_{t-k}
% \end{equation}
D'apr\`es l'\'equation \eqref{eq:decompocholeski} lorsque la
matrice $\Gamma_{p+1}$ est inversible, la variance
$\sigma_p^2=\|\epsilon_{t,p}^+\|^2$ est strictement positive.
D'autre part, la suite $\sigma_p^2$ est
d\'ecroissante. En effet, par d\'efinition de $\cH_{t-1,p}$,
$\cH_{t-1,p}$ est inclus dans $\cH_{t-1,p+1}$ donc
$\proj{X_{p+1}}{\cH_{t-1,p}}$ est dans $\cH_{t-1,p+1}$.
On d\'eduit donc du th\'eor\`eme \ref{theo:projection} que $\sigma_{p+1}^2\leq\sigma_{p}^2$.
La suite $(\sigma_p^2)$ \'etant d\'ecroissante et minor\'ee, elle poss\`ede
donc une limite quand $p$ tend vers l'infini. Cela conduit \`a la d\'efinition suivante,
dont nous verrons au paragraphe \ref{s:wold} qu'elle joue un r\^ole
fondamental dans la d\'ecomposition des processus stationnaires au
second ordre.

\begin{definition}[Processus r\'egulier/d\'eterministe]
 \label{def:paregulier}
 Soit $(X_t)_{t \in \Zset}$ un processus al\'eatoire stationnaire au second
ordre. On note $\sigma^2=\lim_{p\to\infty}\sigma_p^2$ o\`u
$\sigma_p^2$ est la variance de l'innovation partielle
d'ordre $p$. On dit que le processus $(X_t)$ est \emph{r\'egulier} si
$\sigma^2 > 0$ et \emph{d\'eterministe} si $\sigma^2=0$.
\end{definition}


Par ailleurs, nous pouvons remarquer que le probl\`eme de la recherche des coefficients de pr\'ediction pour
un processus stationnaire au second ordre se ram\`ene \`a
celui de la minimisation de l'int\'egrale\,:
\[
%\inf_{\psi \in \cP_p}
   \frac{1}{2\pi}
             \int_{-\pi}^{\pi} |\psi(\rme^{-\rmi\lambda})|^2 \nu_X(\rmd\lambda)
%           = \frac{1}{2\pi}
%             \int_{-\pi}^{\pi} |\phi_p(e^{\rmi x})|^2 \mu_X(dx)
%           =\sigma_p^2
\]
sur l'ensemble $\cP_p$ des polyn\^omes \`a coefficients r\'eels
de degr\'e $p$ de la forme $\psi(z) = 1 + \psi_1 z + \cdots + \psi_p
z^p$. En effet, en utilisant la relation \eqref{eq:dspfiltrage} de
filtrage des mesures spectrales, on peut \'ecrire que la variance de
$ \| \epsilon_{t,p}^+ \|^2$, qui minimise l'erreur de
pr\'ediction, a pour expression\,:
\begin{equation}
\label{eq:variance:innovation:partielle}
% \| \epsilon_{t,p}^+ \|^2
 \sigma_p^2
 = \frac{1}{2 \pi} \int_{-\pi}^{\pi} |
    \phi_p(\rme^{-\rmi\lambda})|^2 \nu_X(\rmd\lambda)
\end{equation}
o\`u\,:
\[
  \phi_p(z) = 1 - \sum_{k=1}^p \phi_{k,p} z^{-k}
\]
d\'esigne le \emph{polyn\^ome pr\'edicteur d'ordre $p$}.
\begin{theorem}
 \label{theo:procregulpredicstable}
 Si $\{ X_t \}$ est un processus r\'egulier, alors, pour
tout $p$, $\phi_p(z) \ne 0$ pour $|z|\leq 1$. Tous les z\'eros des
polyn\^omes pr\'edicteurs sont \`a l'ext\'erieur du cercle unit\'e.
\end{theorem}

\begin{proof}[Preuve du th\'eor\`eme~\ref{theo:procregulpredicstable}]
\smartqed
Nous allons tout d'abord montrer que le pr\'edicteur optimal n'a pas
de racines sur le cercle unit\'e. Raisonnons par contradiction.
Supposons que le polyn\^ome $\phi_p(z)$ ait deux racines
complexes conjugu\'ees, de la forme $\exp(\pm i\theta)$, sur le
cercle unit\'e (on traite de fa\c{c}on similaire le cas de racines
r\'eelles, $\theta= 0$ ou $\pi$). Nous pouvons \'ecrire\,:
\[
 \phi_p(z) = \phi^*_p(z) (1 - 2 \cos(\theta) z + z^2)
\]
On note $\bar{\nu}_X(d \lambda) = \nu_X(d \lambda) |\phi^*_p(\rme^{-i
\lambda})|^2$. $\bar{\nu}_X$ est une mesure positive sur
$[-\pi,\pi]$ de masse finie. On note:
\[
 \bar{\gamma}(\tau) = \frac{1}{2 \pi} \int_{- \pi}^{\pi}
 \rme^{\rmi \tau \lambda} \bar{\nu}_X(\rmd \lambda)\;.
\]
Nous avons donc\,:
\begin{multline*}
 \sigma_p^2 = \frac{1}{2 \pi} \int_{-\pi}^\pi
                  (1 - 2 \cos(\theta) \rme^{-i \lambda} + \rme^{-2 i \lambda})
                  \bar{\nu}_X(\rmd \lambda)\\
           = \inf_{\psi \in \cP_2} \frac{1}{2 \pi}
           \int_{-\pi}^\pi
           |1 + \psi_1 \rme^{-\rmi\lambda} + \psi_2 \rme^{-2 \rmi \lambda}|^2
             \bar{\nu}_X(\rmd \lambda) \eqsp.
\end{multline*}
La minimisation de $\sigma_p^2$ est équivalente à
\`a la r\'esolution des \'equations de Yule-Walker \`a
l'ordre $p=2$ pour la suite des covariances $\bar{\gamma}(h)$.
Par cons\'equent la suite des coefficients $\{1,-2\cos(\theta),1\}$
doit v\'erifier l'\'equation\,:
\[
\left[
\begin{array}{ccc}
\bar{\gamma}(0) & \bar{\gamma}(1) & \bar{\gamma}(2) \\
\bar{\gamma}(1) & \bar{\gamma}(0) & \bar{\gamma}(1) \\
\bar{\gamma}(2) & \bar{\gamma}(1) & \bar{\gamma}(0) \\
\end{array}
\right] \left[
\begin{array}{c}
1 \\
-2 \cos(\theta) \\
1
\end{array}
\right] = \left[
\begin{array}{c}
\sigma_p^2 \\
0 \\
0
\end{array}
\right]
\]
De cette \'equation il s'en suit (les premi\`ere et troisi\`eme
lignes sont \'egales) que $\sigma_p^2=0$, ce qui est contraire \`a
l'hypoth\`ese que le processus est r\'egulier.

D\'emontrons maintenant que les racines des polyn\^omes
pr\'edicteurs sont toutes \emph{strictement \`a  l'int\'erieur du
cercle unit\'e}. Raisonnons encore par l'absurde. Supposons que le
polyn\^ome pr\'edicteur \`a l'ordre $p$ ait $m$ racines $\{a_k,
|a_k| < 1, 1 \le k \leq m \}$ \`a l'int\'erieur du cercle unit\'e et
$(p-m)$ racines $\{ b_{\ell}, |b_{\ell}| > 1, 1 \leq \ell \leq p-m
\}$ \`a l'ext\'erieur du cercle unit\'e. Le polyn\^ome pr\'edicteur
\`a l'ordre $p$ s'\'ecrit donc\,:
\[
 \phi_p(z)=
 \prod_{k=1}^m ( 1 - a_k z^{-1}) \prod_{\ell=1}^{p-m} (1 - b_{\ell} z^{-1})\;.
\]
Consid\'erons alors le polyn\^ome\,:
\[
 \bar{\phi}_p(z) =
 \prod_{k=1}^m (1 - a_k z^{-1}) \prod_{\ell=1}^{p-m} (1 - (1/b_{\ell}^{*}) z^{-1})\;.
\]
Il a d'une part toutes ses racines strictement \`a l'int\'erieur
du cercle unit\'e et d'autre part il v\'erifie
$|\bar{\phi}_p(\rme^{-\rmi\lambda })|^2< |\phi_p(\rme^{-\rmi\lambda })|^2$. On a
en effet $| 1 - (1/b_\ell^*) \rme^{-\rmi\lambda } | = |b_\ell| |1 - b_\ell \rme^{-\rmi \lambda }|$ et donc
$|\bar{\phi}_p(\rme^{-\rmi \lambda })|^2 = \left (\prod_{\ell=1}^{p-m} |b_\ell|^{-2} \right )|\phi_p(\rme^{-\rmi\lambda })|^2$, ce qui d\'emontre le r\'esultat
annonc\'e puisque $|b_\ell|<1$. On en d\'eduit alors
que\,:
\begin{gather*}
\frac{1}{2 \pi} \int_{-\pi}^{\pi} | \bar{\phi}_p(\rme^{-i\lambda
})|^2 \nu_X(\rmd\lambda )   < \sigma_p^2\;,
\end{gather*}
ce qui contredit que $\phi_p(z)=\inf_{\psi \in \cP_p}(2\pi)^{-1}
\int_{-\pi}^\pi |\psi(\rme^{-i\lambda })|^2 \nu_X(\rmd\lambda )$.

\end{proof}
Une cons\'equence directe du th\'eor\`eme
\ref{theo:procregulpredicstable} est qu'\`a toute matrice de
covariance de type d\'efini positif, de dimension $(p+1)\times
(p+1)$, on peut associer un processus AR$(p)$ causal dont les
$(p+1)$ premiers coefficients de covariance sont pr\'ecis\'ement la
premi\`ere ligne de cette matrice. Ce r\'esultat n'est pas g\'en\'eral.
Ainsi il existe bien un processus AR$(2)$ causal ayant
$\gamma(0)=1$ et $\gamma(1)=\rho$, comme premiers coefficients de
covariance, \`a condition toutefois que la matrice de covariance
soit positive c'est-\`a-dire que $|\rho|<1$, tandis qu'il n'existe
pas, pour cette m\^{e}me matrice de processus MA$(1)$. Il faut en
effet, en plus du caract\`ere positif, que $|\rho|\leq 1/2$
(voir exemple \ref{exe:testposivite1}).
%============================================================================
%============================================================================
%============================================================================
\section{Algorithme de Levinson-Durbin}
\label{sec:algorithme-levinson-durbin}
%============================================================================
La solution directe du syst\`eme des \'equations de Yule-Walker
requiert de l'ordre de $p^3$ op\'erations~: la r\'esolution classique
de ce syst\`eme implique en effet la d\'ecomposition de la matrice
$\Gamma_p$ sous la forme du produit d'une matrice triangulaire
inf\'erieure et de sa transpos\'ee, $\Gamma_p = L_p L_p^T$
(d\'ecomposition de Choleski) et la r\'esolution par substitution de
deux syst\`emes triangulaires. Cette proc\'edure peut s'av\'erer
co\^uteuse lorsque l'ordre de pr\'ediction est grand (on utilise
g\'en\'eralement des ordres de pr\'ediction de l'ordre de quelques
dizaines \`a quelques centaines), ou lorsque, \`a des fins de
mod\'elisation, on est amen\'e \`a \'evaluer la qualit\'e de pr\'ediction
pour diff\'erents horizons de pr\'ediction. L'algorithme de
Levinson-Durbin exploite la structure g\'eom\'etrique particuli\`ere
des processus stationnaires au second ordre pour \'etablir une
formule de r\'ecurrence donnant les coefficients de pr\'ediction \`a
l'ordre $(p+1)$ \`a partir des coefficients de pr\'ediction
obtenus \`a l'ordre $p$. Il fournit \'egalement une relation de r\'ecurrence
entre l'erreur de pr\'ediction directe \`a l'ordre $p+1$
et l'erreur de pr\'ediction directe \`a l'ordre $p$.

On supposera dans toute cette partie que {\boldmath$\Gamma_p$} \textbf{est inversible
pour tout} {\boldmath $p\geq 1$}.

Supposons que les
coefficients de pr\'ediction lin\'eaire et la variance de l'erreur de
pr\'ediction directe \`a l'ordre $p$, pour $p \geq 0$, sont connus :
\begin{gather*}
%\label{eq:definition:predictionretrograde}
   \proj{X_t}{\cH_{t-1,p}} =
   \sum_{k=1}^{p} \phi_{k,p} X_{t-k}
   \quad\mbox{et}\quad
   \sigma_{p}^2 = \| X_t - \proj{X_t}{\cH_{t-1,p}} \|^2\;,
\end{gather*}
et d\'eterminons, \`a partir de la
projection \`a l'ordre $p$ de $X_t$, la projection de $X_t$ \`a
l'ordre $p+1$ sur le sous-espace $\cH_{t-1,p+1} =
\lspan{X_{t-1}, \cdots, X_{t-p-1}}$.

Pour cela, on d\'ecompose cet espace
en somme orthogonale de la fa\c{c}on suivante\,:
\begin{multline*}
\cH_{t-1,p+1} = \cH_{t-1,p}  \oplusperp \lspan{ X_{t-p-1} -\proj{X_{t-p-1}}{\cH_{t-1,p}}}
\\= \cH_{t-1,p} \oplusperp \lspan{\epsilon_{t-p-1,p}^-}\;,
\end{multline*}
o\`u, de fa\c{c}on g\'en\'erale, $\epsilon_{t,p}^-$ correspond \`a
l'\emph{erreur de pr\'ediction
r\'etrograde \`a l'ordre $p$} d\'efinie par :
\[
\epsilon_{t,p}^-
     = X_t - \proj{X_t}{\cH_{t+p,p}}
     = X_t - \proj{X_t}{\lspan{X_{t+p}, \cdots, X_{t+1}}}\;.
\]
Elle repr\'esente la diff\'erence entre la valeur \`a l'instant courant $X_t$ et
la projection orthogonale de $X_t$ sur les $p$ \'echantillons
\emph{qui suivent} l'instant courant $\{X_{t+1}, \cdots, X_{t+p}\}$.
Le qualificatif \emph{r\'etrograde} est clair\,: il
traduit le fait que l'on cherche \`a pr\'edire la valeur courante
en fonction des valeurs futures.

D'apr\`es la proposition \ref{prop:projecteur},
\begin{multline*}
%\label{eq:decomposition}
  \proj{X_t}{\cH_{t-1,p+1}}
  = \proj{X_t}{\cH_{t-1,p}} + \proj{X_t}{\lspan{\epsilon_{t-p-1,p}^-}}\;,
\end{multline*}
o\`u d'apr\`es l'exemple \ref{exe:proj1vecteur} :
$$
\proj{X_t}{\lspan{\epsilon_{t-p-1,p}^-}} = \alpha \epsilon_{t-p-1,p}^-
 \quad \mbox{avec} \quad
 \alpha=\pscal{X_t}{\epsilon_{t-p-1,p}^-}/\|\epsilon_{t-p-1,p}^-\|^2\;.
$$
On en d\'eduit donc que :
\begin{multline}
\label{eq:decomposition}
  \proj{X_t}{\cH_{t-1,p+1}}
  = \proj{X_t}{\cH_{t-1,p}} \\
+ k_{p+1} \left[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}} \right] \eqsp,
\end{multline}
o\`u
\begin{equation}\label{eq:k_{p+1}}
k_{p+1}=\frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t-p-1,p}^-\|^2}\;.
\end{equation}
Montrons \`a pr\'esent que les coefficients de pr\'ediction r\'etrograde
co\"{i}ncident avec les coefficients de pr\'ediction directe.
Plus pr\'ecis\'ement, si
\begin{equation}
\label{eq:devtdirectretro1}
 \proj{X_t}{\cH_{t-1,p}} = \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;,
\end{equation}
alors
\begin{equation}
\label{eq:devtdirectretro2}
 \proj{X_{t-p-1}}{\cH_{t-1,p}} =\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k} =\sum_{k=1}^{p} \phi_{p+1-k,p} X_{t-k}\;.
\end{equation}
En effet, les coefficients des deux d\'eveloppements
(\ref{eq:devtdirectretro1}) et (\ref{eq:devtdirectretro2}) sont
tous les deux donn\'es par (\ref{eq:sol_unique}).
En utilisant \eqref{eq:devtdirectretro1} et
\eqref{eq:devtdirectretro2} dans
\eqref{eq:decomposition}, on a :
\begin{multline*}
\proj{X_{t}}{\cH_{t-1,p+1}}
  = \sum_{k=1}^{p+1} \phi_{k,p+1}X_{t-k}\\
  = \sum_{k=1}^{p} (\phi_{k,p} - k_{p+1} \phi_{p+1-k,p} ) X_{t-k} + k_{p+1} X_{t-p-1}\;.
\end{multline*}
On en d\'eduit, par unicit\'e, les formules de r\'ecurrence donnant les coefficients
de pr\'ediction \`a l'ordre $p+1$ \`a partir de ceux \`a
l'ordre $p$\,:
\begin{equation}
 \label{eq:recursionLevinson}
\begin{cases}
\phi_{k,p+1} = \phi_{k,p} - k_{p+1} \phi_{p+1-k,p}\;, & \quad\mbox{pour}\quad k \in \{1, \cdots, p \}\;, \\
\phi_{p+1,p+1} = k_{p+1}\;. &
\end{cases}
\end{equation}
Explicitons \`a pr\'esent la relation (\ref{eq:k_{p+1}}) d\'efinissant
$k_{p+1}$. En utilisant \eqref{eq:devtdirectretro2}, on a :
\begin{multline*}
\pscal{X_t}{\epsilon_{t-p-1,p}^-}
            = \pscal{X_{t}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}\\
            = \gamma(p+1) - \pscal{X_t}{\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k}}
            = \gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)\;.
\end{multline*}
D'autre part,
\begin{multline}\label{eq:norme2_epsretro}
\|\epsilon_{t-p-1,p}^-\|^2=\pscal{X_{t-p-1}}{X_{t-p-1}-\sum_{k=1}^{p}
  \phi_{k,p} X_{t-p-1+k}}\\
=\gamma(0)-\sum_{k=1}^{p} \phi_{k,p}\gamma(k)
=\sigma_p^2=\|\epsilon_{t,p}^+\|^2\;,
\end{multline}
ce qui donne
\begin{equation}\label{eq:def:k_{p+1}}
  k_{p+1}=
   \frac{\gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)}
   {\sigma_p^2}\;.
 \end{equation}
 Il nous reste maintenant \`a d\'eterminer l'erreur de pr\'ediction
${\sigma_{p+1}^2}$ \`a l'ordre $(p+1)$ en fonction de $\sigma_p^2$.
En utilisant l'\'equation \eqref{eq:decomposition}, on a
\begin{multline*}
 \epsilon_{t,p+1}^+ =X_{t} - \proj{X_{t}}{\cH_{t-1,p+1}} \\
 = X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}
[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}]\\
=X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}\epsilon_{t-p-1,p}^-\;,
\end{multline*}
dont on d\'eduit d'apr\`es \eqref{eq:norme2_epsretro}\,:
\begin{multline*}
  \sigma_{p+1}^2=\|\epsilon_{t,p+1}^+\|^2
=\sigma_p^2+ k_{p+1}^2 \sigma_p^2-2k_{p+1}\pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{\epsilon_{t-p-1,p}^-}\;.
      % = \sigma_p^2 + k_{p+1}^2 \sigma_p^2
      % - 2 k_{p+1} \pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}} \\
      %= \sigma_p^2 (1 - k_{p+1}^2)\;,
\end{multline*}
En utilisant que $\proj{X_{t}}{\cH_{t-1,p}}$ et $\epsilon_{t-p-1,p}^-$
sont orthogonaux, \eqref{eq:k_{p+1}} et \eqref{eq:norme2_epsretro}, on
obtient
\begin{equation}\label{eq:recursion_sigma_p+1}
 \sigma_{p+1}^2=\sigma_p^2 (1 - k_{p+1}^2)\;.
\end{equation}
A partir de ces r\'ecursions, nous allons \`a pr\'esent d\'ecrire l'algorithme
de Levinson-Durbin.



% Indiquons que l'erreur r\'etrograde
% joue un r\^ole absolument essentiel dans tous les algorithmes
% rapides de r\'esolution des \'equations de Yule-Walker.

% Remarquons tout
% d'abord que les coefficients de pr\'ediction r\'etrograde
% co\"{i}ncident avec les coefficients de pr\'ediction directe :
% Cette
% propri\'et\'e, que nous avons rencontr\'ee exemple \ref{exe:predAVAR},
% est fondamentalement due \`a la {\em propri\'et\'e de r\'eversibilit\'e}
% des processus stationnaires au second ordre. En effet, si $Y_t=
% X_{-t}$, alors $Y_t$ a m\^{e}me moyenne et m\^{e}me fonction de
% covariance que $X_t$ (voir exemple \ref{exe:stat_retourne}
% chapitre \ref{chap:passl}) et par cons\'equent, en utilisant aussi
% l'hypoth\`ese de stationnarit\'e, on a simultan\'ement pour tout
% $u,v\in\Zset$\,:
% \begin{multline*}
%   \proj{X_{t+u}}{\cH_{t+u-1,p}}
%          = \sum_{k=1}^{p} \phi_{k,p} X_{t+u-k}
%   \quad \text{et} \\
%   \proj{X_{t+v}}{\cH_{t+v+p,p}} = \sum_{k=1}^p \phi_{k,p} X_{t+v+k}
% \end{multline*}
% ainsi que\,:
% \begin{gather}
%  \label{eq:eplusemoins}
%  \sigma_p^2
%  =
%  \|\epsilon_{t+u,p}^+\|^2  %%=\| X_{t+u} - (X_{t+u} | \cH_{t+u-1,p}) \|^2
%  =
%  \|\epsilon_{t+v,p}^-\|^2 %%= \| X_{t+v-p-1,p} - (X_{t+v-p-1,p} | \cH_{t+v-1,p}) \|^2
% \end{gather}
% En particulier on a\,:
% \begin{equation}
%  \label{eq:devtdirectretro}
%  \begin{cases}
%  \proj{X_t}{\cH_{t-1,p}} = \sum_{k=1}^{p} \phi_{k,p} X_{t-k}\;, \\
%  \proj{X_{t-p-1}}{\cH_{t-1,p}} =\sum_{k=1}^{p} \phi_{k,p} X_{t-p-1+k} =\sum_{k=1}^{p} \phi_{p+1-k,p} X_{t-k}\;.
%  \end{cases}
% \end{equation}



% Cherchons maintenant \`a d\'eterminer, \`a partir de la
% projection \`a l'ordre $p$, la projection de $X_t$ \`a
% l'ordre $p+1$ sur le sous-espace $\cH_{t-1,p+1} =
% \lspan{X_{t-1}, \cdots, X_{t-p-1}}$. Pour cela d\'ecomposons cet espace
% en somme orthogonale de la fa\c{c}on suivante\,:
% \begin{multline*}
% \cH_{t-1,p+1} = \cH_{t-1,p}  \oplusperp \lspan{ X_{t-p-1} -\proj{X_{t-p-1}}{\cH_{t-1,p}}}
% \\= \cH_{t-1,p} \oplusperp \lspan{\epsilon_{t-p-1,p}^-}
% \end{multline*}
% Un calcul simple montre (voir exemple \ref{exe:proj1vecteur}) que
% $$
% \proj{X_t}{\epsilon_{t-p-1,p}^-} = \alpha \epsilon_{t-p-1,p}^-
%  \quad \mbox{avec} \quad
%  \alpha=(X_t,\epsilon_{t-p-1,p}^-)/\|\epsilon_{t-p-1,p}^-\|^2
% $$
% et donc que
% \begin{equation}
% \label{eq:decomposition}
%   \proj{X_t}{\cH_{t-1,p+1}}
%   = \proj{X_t}{\cH_{t-1,p}} + k_{p+1} \left[X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}} \right] \eqsp,
% \end{equation}
% o\`u, en utilisant aussi \eqref{eq:eplusemoins}, on peut
% \'ecrire\,:
% \begin{equation}
% \label{eq:defkpP1}
%   k_{p+1}=\frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\sigma_p^2}=
%    \frac{\pscal{X_t}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t+u,p}^+\| \|\epsilon_{t+v,p}^-\|} \eqsp.
% \end{equation}
% En portant \`a pr\'esent \eqref{eq:devtdirectretro} dans
% \eqref{eq:decomposition}, on obtient l'expression\,:
% $$
% \proj{X_{t}}{\cH_{t-1,p+1}}
%   = \sum_{k=1}^{p+1} \phi_{k,p+1}X_{t-k}
%   = \sum_{k=1}^{p} (\phi_{k,p} - k_{p+1} \phi_{p+1-k,p} ) X_{t-k} + k_{p+1} X_{t-p-1}
% $$
% On en d\'eduit les formules de r\'ecurrence donnant les coefficients
% de pr\'ediction \`a l'ordre $p+1$ \`a partir de ceux \`a
% l'ordre $p$\,:
% \begin{equation}
%  \label{eq:recursionLevinson}
% \begin{cases}
% \phi_{k,p+1} = \phi_{k,p} - k_{p+1} \phi_{p+1-k,p} & \quad\mbox{pour}\quad k \in \{1, \cdots, p \} \\
% \phi_{p+1,p+1} = k_{p+1} &
% \end{cases}
% \end{equation}
% D\'eterminons maintenant la formule de r\'ecurrence donnant $k_{p+1}$.
% En utilisant encore \eqref{eq:devtdirectretro} et
% \eqref{eq:decomposition}, on obtient\,:
% $$
% \pscal{X_t}{\proj{X_{t-p-1}}{\cH_{t-1,p}}}
%      =\sum_{k=1}^{p} \phi_{k,p} \PE{X_t X_{t-p-1+k}}
%      =\sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)
% $$
% Partant de l'expression de $\pscal{X_t}{\epsilon_{t-p-1,p}^-}$ on en
% d\'eduit que\,:
% \begin{multline*}
% \pscal{X_t}{\epsilon_{t-p-1,p}^-}
%             = \pscal{X_{t}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}\\
%             = \gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)
% \end{multline*}
% et donc d'apr\`es \eqref{eq:defkpP1}\,:
% $$
%   k_{p+1}=
%    \frac{\gamma(p+1) - \sum_{k=1}^{p} \phi_{k,p} \gamma(p+1-k)}
%    {\sigma_p^2}
% $$
% Il nous reste maintenant \`a d\'eterminer l'erreur de pr\'ediction
% ${\sigma_{p+1}^2}$ \`a l'ordre $(p+1)$. En utilisant l'\'equation
% \eqref{eq:decomposition}, on a
% \begin{multline*}
%  \epsilon_{t,p+1}^+ =X_{t} - \proj{X_{t}}{\cH_{t-1,p+1}} \\
%  = X_{t} - \proj{X_{t}}{\cH_{t-1,p}} - k_{p+1}
% (X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}})
% \end{multline*}
% dont on d\'eduit d'apr\`es \eqref{eq:defkpP1}\,:
% \begin{multline*}
%   \sigma_{p+1}^2=\|\epsilon_{t,p+1}^+\|^2
%       = \sigma_p^2 + k_{p+1}^2 \sigma_p^2
%       - 2 k_{p+1} \pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}} \\
%       = \sigma_p^2 (1 - k_{p+1}^2)
% \end{multline*}

Pour initialiser l'algorithme, nous nous int\'eressons au cas $p=0$.
Dans ce cas, la meilleure pr\'ediction de $X_t$ est $\PE{X_t}=0$ et la variance de
l'erreur de pr\'ediction est donn\'ee par $\sigma_{0}^2
=\PE{(X_t-0)^2}=\gamma(0)$. Au pas suivant on a
$k_1=\gamma(1)/\gamma(0)$, en posant $p=0$ dans (\ref{eq:def:k_{p+1}}),
$\phi_{1,1}=\gamma(1)/\gamma(0)$, en posant $p=0$ dans (\ref{eq:recursionLevinson}) et
$\sigma_1^2=\gamma(0)(1-k_1^2)$, en posant  $p=0$ dans \eqref{eq:recursion_sigma_p+1}.

L'algorithme de \emph{Levinson-Durbin} qui permet de d\'eterminer les coefficients de pr\'ediction
$\{\phi_{m,p}\}_{1\leq m\leq p,1\leq p\leq K}$ \`a partir de
$\gamma(0),\dots,\gamma(K)$ s'\'ecrit alors de la fa\c{c}on suivante :


% Partant d'une suite de $(K+1)$ coefficients de covariance $\gamma(0),\dots,\gamma(K)$, l'\emph{algorithme de Levinson-Durbin}
% permet de d\'eterminer les coefficients de pr\'ediction
% $\{\phi_{m,p}\}_{1\leq m\leq p,1\leq p\leq K}$\,:

\begin{algorithm}[Levinson-Durbin]
\item[Initialisation] $k_1=\gamma(1)/\gamma(0)$, $\phi_{1,1}=\gamma(1)/\gamma(0)$ et $\sigma_1^2=\gamma(0)(1-k_1^2)$
\item[R\'ecursion] Pour $p=\{2,\dots,K\}$ r\'ep\'eter\,:
\begin{enumerate}[label=(\alph*)]
\item Calculer
\begin{align*}
&k_p=\sigma_{p-1}^{-2} \left( \gamma(p) - \sum_{k=1}^{p-1} \phi_{k,p-1} \gamma(p-k) \right) \\
&\phi_{p,p}=k_p \\
&\sigma_{p}^2=\sigma_{p-1}^2(1-k_p^2)
\end{align*}
\item Pour $m\in \{1,\cdots,p-1\}$ calculer\,:
\[
\phi_{m,p}=\phi_{m,p-1}-k_p\phi_{p-m,p-1}
\]
\end{enumerate}
\end{algorithm}

\begin{proposition}
Soit $(X_t)$ un processus stationnaire au second ordre de fonction d'autocovariance
$\gamma(h)$.
%telle que $\gamma(0)>0$ et $\gamma(h)\to 0$ lorsque $h$
%tend vers l'infini.
Le coefficient $k_{p+1}$ d\'efini par
(\ref{eq:k_{p+1}}) v\'erifie, pour tout $p\geq 0$ :
\begin{equation}
 \label{eq:defkp}
 k_{p+1}=\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}
        {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}\;,
\end{equation}
et
\begin{equation}
\label{eq:maj:k_p+1}
|k_{p+1}|\leq 1\;.
\end{equation}
\end{proposition}
\begin{proof}\smartqed
En utilisant que $\proj{X_{t}}{\cH_{t-1,p}}$ est orthogonal \`a
$\epsilon_{t-p-1,p}^-$, (\ref{eq:k_{p+1}}) et (\ref{eq:deferreurforward}), on a
$$
k_{p+1}=\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}{\|\epsilon_{t-p-1,p}^- \|^2}\;.
$$
Or, d'apr\`es (\ref{eq:norme2_epsretro}), $\|\epsilon_{t-p-1,p}^-
\|^2=\sigma_p^2=\|\epsilon_{t,p}^+ \|^2$, la derni\`ere \'egalit\'e venant
de (\ref{eq:var_pred_dir}), d'o\`u l'on d\'eduit \eqref{eq:defkp}.
L'in\'egalit\'e \eqref{eq:maj:k_p+1} se d\'eduit alors de \eqref{eq:defkp}
en utilisant l'in\'egalit\'e de Cauchy-Schwarz.

\end{proof}

%\begin{proof}\smartqed
%  Notons tout d'abord que
% $\proj{X_{t}}{\cH_{t-1,p}}\perp \epsilon_{t-p-1,p}^-$ puisque
% $\proj{X_{t}}{\cH_{t-1,p}}\in\cH_{t-1,p}$ et que
% $\epsilon_{t-p-1,p}^-\perp \cH_{t-1,p}$. Partant de
% \eqref{eq:defkpP1} on peut \'ecrire que\,:
% \begin{equation}
%  \label{eq:defkp}
%  k_{p+1}
%   =\frac{\pscal{X_{t} - \proj{X_{t}}{\cH_{t-1,p}}}{X_{t-p-1} - \proj{X_{t-p-1}}{\cH_{t-1,p}}}}
%     {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}
%   =\frac{\pscal{\epsilon_{t,p}^+}{\epsilon_{t-p-1,p}^-}}
%         {\|\epsilon_{t,p}^+ \| \,\, \|\epsilon_{t-p-1,p}^- \|}
% \end{equation}
% En utilisant l'in\'egalit\'e de Schwarz, on montre que $|k_{p+1}| \leq
% 1$.
%
%\end{proof}
%Dans la litt\'erature, $k_p$ est appel\'e coefficient d'autocorr\'elation partielle.
\begin{definition}[Fonction d'autocorr\'elation partielle]
 \label{def:corrpar}
 Soit $(X_t)$ un processus stationnaire au second ordre de fonction d'autocovariance
$\gamma(h)$. On appelle \emph{fonction d'autocorr\'elation partielle} la
suite des coefficients d'autocorr\'elation partielle  $(k_p)_{p \geq 1}$
d\'efinie par :
\begin{equation}
 \label{eq:defcorrpart}
 k_p = \corr(X_t,X_{t-1})= \frac{\pscal{X_t}{X_{t-1}}}{\|X_t\| \,\,
   \|X_{t-1}\|},\textrm{ si } p=1\;,
\end{equation}
et
\begin{multline}
k_p=\corr(\epsilon_{t,p-1}^+,\epsilon_{t-p,p-1}^-)\\
           = \dfrac{\pscal{X_{t}-\proj{X_t}{\cH_{t-1,p-1}}}{X_{t-p}-\proj{X_{t-p}}{\cH_{t-1,p-1}}}}
            {\|X_{t}-\proj{X_t}{\cH_{t-1,p-1}}\| \,\,
              \|X_{t-p}-\proj{X_{t-p}}{\cH_{t-1,p-1}}\|},
\textrm{ si } p\geq 2\;.
\end{multline}
\end{definition}

\begin{remark}
Dans \eqref{eq:defcorrpart}, l'expression pour $p=1$ est en accord
avec celle pour $p\geq 2$ dans la mesure o\`u on peut noter que
$\epsilon_{t,0}^+=X_t$ et que $\epsilon_{t-1,0}^-=X_{t-1}$. Notons
aussi que, dans l'expression de $k_p$, $X_t$ et $X_{t-p}$ sont
projet\'es sur le m\^{e}me sous-espace
$\lspan{X_{t-1},\dots,X_{t-p+1}}$. Le r\'esultat remarquable est
que la suite des coefficients de corr\'elation partielle est donn\'ee
par\,:
\begin{equation}
 \label{eq:parcoretcoeffpredic}
  k_p=\phi_{p,p}
\end{equation} o\`u $\phi_{p,p}$ est d\'efini au moyen des \'equations de
Yule-Walker~(\ref{eq:YW1}).
\end{remark}
Dans le cas particulier d'un
processus AR$(m)$ causal, on a alors\,:
$$
 k_p=\left\{
   \begin{matrix}
     \phi_{p,p}&\mbox{pour}& 1\leq p < m\;,\\
     \phi_m&\mbox{pour}& p = m\;,\\
     0&\mbox{pour}& p > m\;.
   \end{matrix}
   \right.
$$
%==================================================================
%==================================================================
%==================================================================
% \section{Algorithme de Schur}
% %==================================================================
% %==================================================================


% Partant des coefficients d'autocorr\'elation, l'algorithme de
% Levinson-Durbin \'evalue \`a la fois les coefficients des
% pr\'edicteurs lin\'eaires optimaux et les coefficients
% d'autocorr\'elation partielle. Dans certains cas, seuls les
% coefficients d'autocorr\'elation partielle sont n\'ecessaires. Il en
% est ainsi, par exemple, lorsque l'on cherche \`a calculer les
% erreurs de pr\'ediction directe et r\'etrograde \`a partir du
% processus $X_t$. Montrons, en effet, que les erreurs de pr\'ediction
% \`a l'ordre $(p+1)$ s'expriment, en fonction des erreurs de
% pr\'edictions \`a l'ordre $p$, \`a l'aide d'une formule de
% r\'ecurrence ne faisant intervenir que la valeur du coefficient de
% corr\'elation partielle\,:
% \begin{equation}
%  \label{eq:celluleanalyse}
% \begin{cases}
% \epsilon_{t,p+1}^+= \epsilon_{t,p}^+ -k_{p+1}\epsilon_{(t-1)-p,p}^- \\
% \epsilon_{t-(p+1),p+1}^-=\epsilon_{(t-1)-p,p}^- -k_{p+1}\epsilon^+_{t,p}
% \end{cases}
% \end{equation}
% Reprenons les expressions de l'erreur de pr\'ediction directe et de
% l'erreur de pr\'ediction r\'etrograde\,:
% \begin{eqnarray*}
%  \epsilon_{t,p}^+ = X_t - \sum_{k=1}^p \phi_{k,p} X_{t-k}
%  &\mbox{et}&
%  \epsilon_{t-p-1,p}^- = X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k}
% \end{eqnarray*}
% En utilisant directement la r\'ecursion de Levinson-Durbin,
% \'equations \eqref{eq:recursionLevinson}, dans l'expression de
% l'erreur de pr\'ediction directe \`a l'ordre $p+1$, nous
% obtenons\,:
% \begin{align}
% \label{eq:predictiondirecte}
%  \epsilon_{t,p+1}^+
%   &=  X_t - \sum_{k=1}^{p+1} \phi_{k,p+1} X_{t-k}
%   \nonumber
%   \\&
%   = \left( X_t - \sum_{k=1}^p \phi_{k,p} X_{t-k} \right)
%      - k_{p+1}
%      \left( X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k} \right)
%   \nonumber
%   \\
%   &= \epsilon_{t,p}^+ - k_{p+1} \epsilon_{t-p-1,p}^-
% \end{align}
% De fa\c{c}on similaire, nous avons\,:
% \begin{align}
% \label{eq:predictionretrograde}
%  \epsilon_{t-p-1,p+1}^-
%   &=  X_{t-p-1} - \sum_{k=1}^{p+1} \phi_{k,p+1} X_{t-p-1+k}
%   \nonumber
%   \\&
%   = \left( X_{t-p-1} - \sum_{k=1}^p \phi_{k,p} X_{t-p-1+k} \right)
%      - k_{p+1}
%      \left( X_{t} - \sum_{k=1}^p \phi_{k,p} X_{t-k} \right)
%   \nonumber
%   \\
%    &= \epsilon_{t-p-1,p}^- - k_{p+1} \epsilon_{t,p}^+
% \end{align}
% Partant de la suite des autocorr\'elations, l'algorithme de Schur
% calcule r\'ecursivement les coefficients de corr\'elation partielle,
% sans avoir \`a d\'eterminer les valeurs des coefficients de
% pr\'ediction. Historiquement, l'algorithme de Schur a \'et\'e introduit
% pour tester le caract\`ere d\'efini positif d'une suite (ou de
% fa\c{c}on \'equivalente, la positivit\'e des matrices de Toeplitz
% construites \`a partir de cette suite). En effet, comme nous
% l'avons montr\'e ci-dessus, une suite de coefficients de covariance
% est d\'efinie positive si et seulement si les coefficients de
% corr\'elation partielle sont de module strictement inf\'erieur \`a
% $1$. D\'eterminons \`a pr\'esent cet algorithme. En faisant $t=0$
% dans l'\'equation \eqref{eq:predictiondirecte}, en multipliant \`a
% gauche par $X_m$ et en utilisant la stationnarit\'e, il vient\,:
% \begin{equation}
%  \label{eq:kpplus}
%  \pscal{X_m}{\epsilon_{0,p+1}^+}
%  =
%  \pscal{X_m}{\epsilon_{0,p}^+} - k_{p+1} \pscal{X_{m}}{\epsilon_{-p-1,p}^-}
%  =
%  \pscal{X_m}{\epsilon_{0,p}^+} - k_{p+1} \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} \eqsp.
% \end{equation}
% En faisant $t=p+1$ dans l'\'equation
% \eqref{eq:predictionretrograde}, en multipliant \`a gauche par
% $X_{m+p+1}$ et en utilisant la stationnarit\'e, il vient\,:
% \begin{multline}
%  \label{eq:kpmoins}
%  \pscal{X_{m+p+1}}{\epsilon_{0,p+1}^-}
%  =
%  \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} - k_{p+1}\pscal{X_{m+p+1}}{\epsilon_{p+1,p}^+}\\
%  =
%  \pscal{X_{m+p+1}}{\epsilon_{0,p}^-} - k_{p+1}\pscal{X_{m}}{\epsilon_{0,p}^+}  \eqsp.
% \end{multline}
% En faisant $m=0$  dans \eqref{eq:kpmoins}, il vient\,:
% \begin{equation}
%   \label{eq:kpmoinsen0}
%  \pscal{X_{p+1}}{\epsilon_{0,p+1}^-}
%  =
%  \pscal{X_{p+1}}{\epsilon_{0,p}^-} - k_{p+1} \pscal{X_{p+1}}{\epsilon_{p+1,p}^+}
%  = \pscal{X_{p+1}}{\epsilon_{0,p}^-} - k_{p+1} \pscal{X_0}{\epsilon_{0,p}^+} \eqsp.
% \end{equation}
% Mais on a aussi\,:
% \[
% \pscal{X_{p+1}}{\epsilon_{0,p+1}^-}
%  = \pscal{X_{p+1}}{ X_0 - \proj{X_0}{\lspan{X_1, \cdots, X_{p+1}}}} = 0 \eqsp.
% \]
% Nous pouvons donc d\'eduire de l'\'equation \eqref{eq:kpmoinsen0}\,:
% \begin{equation}
% \label{eq:kpshur}
%   k_{p+1} = \frac{\pscal{X_{p+1}}{ \epsilon_{0,p}^-}}{\pscal{X_0}{\epsilon_{0,p}^+}}
% \end{equation}
% En couplant les \'equations \eqref{eq:kpplus}, \eqref{eq:kpmoins} et
% \eqref{eq:kpshur} et en partant des conditions initiales\,:
% \[
%   \pscal{X_m}{\epsilon_{0,0}^+}= \gamma(m)
%   \quad\mbox{et}\quad
%   \pscal{X_{m+1}}{\epsilon_{0,0}^-}=\gamma(m+1) \eqsp.
% \]
% on peut d\'eterminer les coefficients de corr\'elation partielle
% directement, sans avoir \`a \'evaluer explicitement les
% coefficients de pr\'ediction.

% On note $u(m,p)=\pscal{X_m}{\epsilon_{0,p}^+}$ et $v(m,p)=\pscal{X_{m+p+1}}{\epsilon_{0,p}^-}$.
% Partant des $(K+1)$ coefficients de covariance
% $\{\gamma(0),\dots,\gamma(K)\}$, l'{\em algorithme de Schur}
% calcule les $K$ premiers coefficients de corr\'elation partielle\,:
% \begin{description}
% \item[Initialisation] Pour $m=\{0,\dots,K-1\}$\,:
% \begin{align*}
% &u(m,0)=\gamma(m) \\
% &v(m,0)=\gamma(m+1)
% \end{align*}
% \item[R\'ecursion]
% \begin{enumerate}[label=(\alph*)]
% \item Pour $p=\{1,\dots,K\}$, calculer
% \[k_p = \frac{v(0,p-1)}{u(0,p-1)} \]
% \item Pour $m=\{0,\dots,K-p-1\}$ calculer\,:
%   $$
%   \begin{cases}
%   u(m,p)=u(m,p-1)-k_pv(m,p-1) \\
%   v(m,p)=v(m+1,p-1)-k_pu(m+1,p-1)
%   \end{cases}\eqsp.
%   $$
% \end{enumerate}
% \end{description}
% La complexit\'e de l'algorithme de Schur est \'equivalente \`a
% l'algorithme de Levinson.
% %==========================================================
% \subsubsection{Filtres en treillis}
% %==========================================================
% En notant $e(t,p)=[\epsilon_{t,p}^+\quad \epsilon_{t-p,p}^-]^T$ et
% en utilisant l'op\'erateur de retard $B$, les expressions
% \eqref{eq:celluleanalyse} peuvent se mettre sous la forme
% matricielle\,:
% $$
%  e(t,p+1)=
%  \left [
%  \begin{matrix}
%    1&-k_{p+1}B \cr -k_{p+1}B&1
%  \end{matrix}
%  \right ]
%  e(t,p)
% $$
% Les erreurs initiales ($p=0$) sont $e(t,0)=[X_t\quad X_t]^T$. Ces
% \'equations d\'ebouchent sur une structure de filtrage dite en
% treillis qui calcule, au moyen des coefficients de corr\'elation
% partielle, les erreurs de pr\'ediction directe et r\'etrograde \`a
% partir du processus $\{X_t, t \in \Zset\}$. Ce filtre d'analyse est repr\'esent\'e figure
% \ref{fig:anatreillis}.
%  %================= FIGURE
%  %====== FIGURE
%  \figtit{\FIGPREDIC treillisanalyse}
%  {Filtre d'analyse en treillis. Ce filtre permet de construire les erreurs de
%  pr\'ediction directe et r\'etrograde \`a partir du processus et de la donn\'ee
%  des coefficients de corr\'elation partielle.}
%  {fig:anatreillis}
% Les \'equations \eqref{eq:celluleanalyse} peuvent encore s'\'ecrire\,:
% $$
% \begin{cases}
% \epsilon_{t,p}^+=\epsilon_{t,p+1}^+ +k_{p+1}\epsilon_{(t-1)-p,p}^- \\
% \epsilon_{t-(p+1),p+1}^-=\epsilon_{(t-1)-p,p}^-k_{p+1}\epsilon_{t,p}^+
% \end{cases}
% $$
% qui donne le sch\'ema de filtrage de la figure
% \ref{fig:syntreillis}.
%  %================= FIGURE
%  %====== FIGURE
%  \figtit{\FIGPREDIC treillissynthese}
%  {Filtre de synth\`ese en treillis. Ce filtre permet de reconstruire
%  le processus \`a partir de la suite des erreurs de
%  pr\'ediction directe et de la donn\'ee
%  des coefficients de corr\'elation partielle.}
%  {fig:syntreillis}
% %==========================================================
% %==========================================================
\section{Algorithme des innovations}
\label{sec:algorithmes-des-innovations}
L'algorithme des innovations est une application directe de la m\'ethode de Gram-Schmidt et est, \`a cet
\'egard, plus \'el\'ementaire que l'algorithme de Levinson-Durbin. De plus, il ne suppose pas que le processus
$(X_t)_{t \in \Zset}$ soit stationnaire. L'esp\'erance de $X_t$ \'etant
suppos\'ee nulle dans ce chapitre, nous notons
\[
\kappa(i,j)= \pscal{X_i}{X_j}= \PE{X_iX_j} \eqsp,
\]
la fonction d'autocovariance de ce processus.
% Nous supposerons dans tout ce paragraphe que
% la matrice $[\kappa(i,j)]_{i,j=1}^n$ est inversible pour tout $n \geq
% 1$.
Notons, pour $n \geq 1$,
$$\cH_n= \lspan{X_1,\dots,X_n} \textrm{ et }
\sigma_n^2= \| X_{n+1} - \proj{X_{n+1}}{\cH_n}\|^2\;.
$$
La proc\'edure d'orthogonalisation de Gram-Schmidt permet alors d'\'ecrire
pour tout $n \geq 1$ :
\[
\cH_n= \lspan{X_1, X_2 - \proj{X_2}{X_1}, \dots, X_n - \proj{X_n}{\cH_{n-1}}} \eqsp,
\]
o\`u on utilise la convention suivante : $\proj{X_1}{\cH_{0}}=0$.
On a alors :
\begin{equation}
\label{eq:definition-projecteur}
\proj{X_{n+1}}{\cH_n}= \sum_{j=1}^n \theta_{n,j} \left( X_{n+1-j} - \proj{X_{n+1-j}}{\cH_{n-j}}\right) \eqsp.
\end{equation}
L'algorithme des innovations d\'ecrit dans la proposition suivante
fournit une m\'ethode r\'ecursive permettant de calculer
$(\theta_{n,j})_{1\leq j\leq n}$ et $\sigma_n^2$ pour $n\geq 1$.

\begin{proposition}
Soit $(X_t)$ un processus \`a moyenne nulle tel que la matrice
$[\kappa(i,j)]_{1\leq i,j\leq n}$ soit inversible pour tout $n\geq 1$
alors
$$
\proj{X_{n+1}}{\cH_n}=
\begin{cases}
0\;, \textrm{ si } n=0\;,\\
 \sum_{j=1}^n \theta_{n,j} \left( X_{n+1-j} -
   \proj{X_{n+1-j}}{\cH_{n-j}}\right)\;, \textrm{ si } n\geq 1\;,
\end{cases}
$$
o\`u
$$
\begin{cases}
\sigma_0^2=\kappa(1,1)\;,\\
\theta_{n,n-k}= \sigma_{k}^{-2} \left[ \kappa(n+1,k+1) -
  \sum_{j=0}^{k-1} \theta_{k,k-j} \theta_{n,n-j} \sigma_{j}^2
\right]\;,\; 0\leq k\leq n-1\;,\\
\sigma_{n}^2= \kappa(n+1,n+1) - \sum_{j=0}^{n-1} \theta^2_{n,n-j}
\sigma_{j}^2\;,\; n\geq 1 \;.
\end{cases}
$$
\end{proposition}

\begin{proof}\smartqed
Remarquons tout d'abord que les vecteurs $(X_i -
\proj{X_i}{\cH_{i-1}})_{i \geq 1}$ sont orthogonaux. En effet,
pour $i < j$, $X_i - \proj{X_i}{\cH_{i-1}} \in \cH_{j-1}$ et $X_j -
\proj{X_j}{\cH_{j-1}} \perp \cH_{j-1}$.
On en d\'eduit, en faisant le produit scalaire de
\eqref{eq:definition-projecteur} par $X_{k+1}-\proj{X_{k+1}}{\cH_k}$
que, pour $0 \leq k < n$ :
\[
\pscal{\proj{X_{n+1}}{\cH_n}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}}= \theta_{n,n-k} \sigma_{k}^2 \eqsp.
\]
Puisque $\pscal{X_{n+1}-\proj{X_{n+1}}{\cH_n}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}}=0$, les coefficients $\theta_{n,n-k}$,
$k=0,\dots,n-1$ sont donn\'es par
\begin{equation}\label{eq:theta_n_n-k}
\theta_{n,n-k}= \sigma_{k}^{-2} \pscal{X_{n+1}}{X_{k+1}-\proj{X_{k+1}}{\cH_k}} \eqsp.
\end{equation}
En utilisant la repr\'esentation \eqref{eq:definition-projecteur},
\begin{multline*}
\proj{X_{k+1}}{\cH_k}=\sum_{j=1}^k \theta_{k,j} \left( X_{k+1-j} -
  \proj{X_{k+1-j}}{\cH_{k-j}}\right) \\
=\sum_{j=0}^{k-1} \theta_{k,k-j} \left( X_{j+1} -
  \proj{X_{j+1}}{\cH_{j}}\right) \eqsp,
\end{multline*}
d'o\`u l'on d\'eduit que
\[
\theta_{n,n-k}= \sigma_{k}^{-2} \left( \kappa(n+1,k+1) - \sum_{j=0}^{k-1} \theta_{k,k-j} \pscal{X_{n+1}}{X_{j+1}-\proj{X_{j+1}}{\cH_j}}\right) \eqsp.
\]
D'apr\`es \eqref{eq:theta_n_n-k},
 $\pscal{X_{n+1}}{X_{j+1}-\proj{X_{j+1}}{\cH_j}}= \sigma_{j}^{2} \theta_{n,n-j}$ pour $0 \leq j < n$, nous avons donc pour
$k \in \{1,\dots,n-1\}$,
\begin{equation}
\label{eq:mise-a-jour-theta}
\theta_{n,n-k}= \sigma_{k}^{-2} \left( \kappa(n+1,k+1) - \sum_{j=0}^{k-1} \theta_{k,k-j} \theta_{n,n-j} \sigma_{j}^2 \right) \eqsp.
\end{equation}
L'\'equation \eqref{eq:mise-a-jour-theta} est encore valable lorsque
$k=0$ en utilisant la convention que la somme sur $j$ dans le membre
de droite est nulle dans ce cas.
Par ailleurs, la proposition \ref{prop:projecteur} (Pythagore) implique que
\begin{multline}
\label{eq:mise-a-jour-sigma}
\sigma_{n}^2= \| X_{n+1} - \proj{X_{n+1}}{\cH_n} \|^2= \| X_{n+1} \|^2 - \| \proj{X_{n+1}}{\cH_n} \|^2 \\
= \kappa(n+1,n+1) - \sum_{k=0}^{n-1} \theta^2_{n,n-k} \sigma_{k}^2 \eqsp.
\end{multline}

\end{proof}

Alors que l'algorithme de Levinson-Durbin permet de d\'eterminer
les coefficients du d\'eveloppement de $\proj{X_{n+1}}{\cH_n}$ sur
$X_1,\dots,X_n$ donn\'es par $\proj{X_{n+1}}{\cH_n}= \sum_{j=1}^n \phi_{n,j} X_{n+1-j}$,
l'algorithme des innovations calcule les coefficients du d\'eveloppement
de
$\proj{X_{n+1}}{\cH_n}$ sur $X_1$, $X_2 -
\proj{X_2}{X_1}$,
$\dots$, $X_{n} - \proj{X_n}{\cH_{n-1}}$.



\begin{example}[Pr\'ediction d'un processus MA(1)]
Consid\'erons le processus $X_t = Z_t + \theta Z_{t-1}$ o\`u $(Z_t) \sim \BB(0,\sigma^2)$. Nous avons donc $\kappa(i,j)= 0$ pour $|i-j| > 1$,
$\kappa(i,i)= \sigma^2(1+\theta^2)$ et $\kappa(i,i+1)= \theta
\sigma^2$. Dans ce cas, nous avons

$$
\begin{cases}
 \theta_{n,j}= 0\;,\; 2 \leq j \leq n \;, \\
 \theta_{n,1}= \sigma_{n-1}^{-2} \theta \sigma^2 \;,\\
\end{cases}
$$
et les variances des innovations qui sont donn\'ees par
$$
\begin{cases}
 \sigma_0^{2} = (1+\theta^2) \sigma^2 \eqsp, \\
\sigma_{n}^{2}= [1 + \theta^2 - \sigma_{n-1}^{-2} \theta^2 \sigma^2] \sigma^2 \eqsp.\\
\end{cases}
$$
Si nous posons $r_n = \sigma_n^2/\sigma^2$, nous avons
\[
\proj{X_{n+1}}{\cH_n}= \theta \left(X_n - \proj{X_n}{\cH_{n-1}}\right)/r_{n-1} \eqsp,
\]
avec $r_0=1+\theta^2$, et pour $n \geq 1$, $r_{n+1}= 1+\theta^2-\theta^2/r_n$.
\end{example}

%======================================================================
%======================================================================
%======================================================================



%%% Local Variables:
%%% mode: latex
%%% ispell-local-dictionary: "francais"
%%% TeX-master: "../monographie-serietemporelle"
%%% End:
