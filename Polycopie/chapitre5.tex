\chapter{Filtrage de Wiener}
\label{wiener-chap}

Les donn\'ees $Y[n]$ que l'on cherche
s'obtiennent souvent par une estimation bas\'ee sur 
des mesures bruit\'ees $X[n]$
corr\'el\'ees avec $Y[n]$.
La r\'eduction de bruit et la 
pr\'ediction sont des examples d'estimation.
Nous introduisons le 
filtrage de Wiener qui 
calcule la meilleure estimation lin\'eaire
d'un processus sationnaire 
$Y[n]$ \'etant donn\'ees $N$ mesures pass\'ees 
$\{X[n-k]\}_{1 \leq k \leq N}$, en minimisant l'esp\'erance
quadratique de l'erreur.
Le filtrage de Wiener permet entre autres de faire des calculs de
pr\'ediction lin\'eaire, de r\'eduire les bruits additifs, et
d'optimiser la mod\'elisation autor\'egressive
de processus stationnaires.


\section{Filtrage de Wiener}
\label{wiener}

Un filtrage d'ordre $N$ calcule une estimation
d'un processus $Y[n]$ en fonction 
de $N+1$ mesures $X[n]$, ...., $X[n-N]$, o\`u $X$ est un processus
corr\'el\'e avec $Y$.
On sait \cite{neveu} que l'estimateur optimal $\tilde Y[n]$ 
de $Y[n]$ sachant $\{X[n], ...., X[n-N]\}$, qui minimise
l'esp\'erance quadratique de l'erreur
\begin{equation}
\label{erreur}
E \{ (Y[n] - \tilde Y[n])^2 \} 
\end{equation}
est l'esp\'erance conditionnelle
\begin{equation}
\label{condit}
\tilde Y [n] = E \{Y[n]~\BS~ X[n], X[n-1],..., X[n-N]\} .
\end{equation}
Dans le cas particulier o\`u
les variables $Y[n]$ et $\{ X[n-k]\}_\ZkN$ sont
conjointement gaussiennes, cette esp\'erance conditionnelle
est une fonction lin\'eaire des donn\'ees $\{ X[n-k]\}_\ZkN$.
Cependant, dans les cas non gaussiens,
l'esp\'erance conditionnelle (\ref{condit}) est souvent
une fonction
non-lin\'eaire compliqu\'ee de $\{X[k]\}_\ZkN$,
qu'il est tr\`es difficile de calculer. 
Le filtrage de Wiener
simplifie le probl\`eme en calculant la combinaison lin\'eaire
$\tilde Y[n]$ 
des variables al\'eatoires $\{X[k]\}_\ZkN$ 
qui minimise l'erreur quadratique (\ref{erreur}).

\subsection{R\'egression lin\'eaire optimale}

Le probl\`eme d'estimation lin\'eaire optimale peut \^etre r\'esolu
de fa\c{c}on g\'eom\'etrique en se 
pla\c{c}ant dans l'espace $\LDP$ 
des variables al\'eatoires
d\'efinies sur $(\Omega , \cal A , \rm P)$, dont l'esp\'erance
quadratique est finie. On rappelle que le
produit scalaire de deux variables al\'eatoires est
\begin{equation}
<A,B>_\LDP = E\{A B\} .
\end{equation}
\\
\noindent{\bf Projection orthogonale}
On suppose que $E\{Y[k]^2\} < +\infty$ 
et que $E\{X[k]^2\} < +\infty$ pour tout $k \in \Z$.
L'espace $\cX_N$ des variables al\'eatoires 
qui sont des combinaisons
lin\'eaires des $\{ X[n-k] \}_\ZkN$ est un sous espace de $\LDP$.
L'estimateur lin\'eaire
\begin{equation}
\label{est}
\tilde Y[n] = \sum_{l=0}^{N} h[n,n-l] X[n-l] .
\end{equation}
qui minimise $\| Y[n] - \tilde Y[n] \|_\LDP$ est
donc la projection orthogonale de $Y[n]$ sur $\cX_N$
que l'on note
\[
\tilde Y[n] = \E \{Y[n] \BS \cX_N \}
\]
L'erreur de pr\'ediction $W [n] = Y[n] - \tilde Y[n]$
est orthogonale \`a $\cX_N$ et donc \`a chacune des donn\'ees $X[n-k]$
pour $0 \leq k \leq N$
\begin{equation}
\label{orth}
<W [n] , X[n-k] > _\LDP = 
E\{Y[n] X[n-k]\} - 
\sum_{l=0}^{N} h[n,n-l] E \{ X[n-l] X[n-k] \} = 0 .
\end{equation}
Pour simplifier l'explication, nous supposons pour l'instant
que $X$ et $Y$ sont des processus de moyenne nulle.
Les \'equations (\ref{orth}) d\'efinissent
un syst\`eme lin\'eaire sur les covariances
$R_{YX} [n,m] = E\{Y[n] X[m]\}$ et
$R_{X} [n,m] = E\{X[n] X[m]\}$ 
\begin{equation}
\label{system}
\sum_{l=0}^{N} h[n,n-l] R_X [n-l,n-k] = R_{YX} [n,n-k] .
\end{equation}
\\
\noindent{\bf Equations de Wiener-Hopf}
Si l'on suppose que les processus $X$ et $Y$ sont conjointement
stationnaires au sens large, alors 
\[
R_{YX} [n,m]= R_{YX} [n-m] ~~~\mbox{et}~~~
R_{X} [n,m]= R_{X} [n-m] .
\]
En ins\'erant ces \'equations dans (\ref{system}), 
on s'aper\c{c}oit que les constantes
$h[n,n-l]$ satisfont un syst\`eme d'\'equations ind\'ependant
de $n$ et peuvent donc s'\'ecrire $h[n,n-l] = h[l]$.
On obtient alors le syst\`eme d'\'equations de Wiener-Hopf
\begin{equation}
\sum_{l=0}^{N} h[l] R_X [k-l] = R_{YX} [k] .
\end{equation}
Les coefficients de r\'egression $h[l]$ se calculent en
r\'esolvant ce syst\`eme lin\'eaire.
En rempla\c{c}ant de m\^eme $h[n,n-l]$ par $h[l]$ dans (\ref{est}), 
l'estimateur lin\'eaire optimal s'\'ecrit comme une
convolution de $X[n]$ avec le filtre causal de r\'eponse 
impulsionnel $h[l]$
\begin{equation}
\label{filtre-wiener}
\tilde Y[n] = \sum_{l=0}^{N} h[l] X[n-l] = h \star X [n] .
\end{equation}

Il est souvent utile d'exprimer le syst\`eme de
Wiener-Hopf sous forme matricielle 
\begin{equation}
\label{wiener-hopf}
{ \bR_X} \bh = \bR_{YX} ,
\end{equation}
en utilisant la matrice sym\'etrique de Toeplitz
\[
{{ \bR_X}} = 
\left( \begin{array}{ccccc}
R_X[0] &R_X[1] & ...&R_X[N-1] &R_X[N] \\
R_X[-1]&R_X[0]& ...&R_X[N-2]&R_X[N-1]\\
. &. &...&. &. \\
.&.&...&.&.\\
.&.&...&.&.\\
R_X[-N-1]&R_X[-N+2]&...&R_X[0]&R_X[1]\\
R_X[-N]&R_X[-N+1]&...&R_X[-1]&R_X[0]\\
\end{array}
\right)
\]
et les vecteurs
\[
{{ \bR_{YX}}} = 
\left( \begin{array}{c}
R_{YX}[0] \\
R_{YX}[1] \\
. \\
. \\
. \\
R_{YX}[N-1] \\
R_{YX}[N]
\end{array}
\right)
~~~
{{ \bh}} = 
\left( \begin{array}{c}
h[0] \\
h[1] \\
. \\
. \\
. \\
h[N-1] \\
h[N]
\end{array}
\right) .
\]
Si 
on suppose que les variables al\'eatoires $X[0]$, $X[1]$, ...,$X[N]$
sont lin\'eairement ind\'ependantes, ce qui veut dire que la
matrice ${ \bR_X}$ est inversible, alors
\begin{equation}
\label{bh}
\bh = { \bR_X}^{-1} \bR_{YX} .
\end{equation}
Si elles ne sont pas ind\'ependantes, on diminue $N$
jusqu'\`a ce qu'elles soient ind\'ependantes, pour minimiser l'ordre
de la r\'egression lin\'eaire.
\\
\\
\noindent{\bf Erreur d'estimation}
L'erreur d'estimation $W [n] = Y[n] - \tilde Y[n]$
\'etant orthogonale \`a $\tilde Y[n]$, le th\'eor\`eme de Pythagore
prouve que sa variance est 
\[
E\{W [n]^2\} = E\{Y [n]^2\}  - E\{\tilde Y [n]^2\} .
\]
Par ailleurs, l'orthogonalit\'e implique que 
$E\{\tilde Y[n] \tilde Y[n]\} = E\{ \tilde Y[n] Y[n]\}$.
En ins\'erant (\ref{filtre-wiener}) on obtient
\begin{equation}
\label{erreur-reg}
E\{W [n]^2\} = E\{Y [n]^2\}  - 
\sum_{l=0}^{N} h[l] R_{YX} [l] .
\end{equation}
\\
\noindent{\bf Moyenne non-nulle}
Lorsque les processus $X[n]$ et $Y[n]$ sont de moyenne non-nulle,
$E\{Y[n]\} = \mu_Y$ et $E\{X[n]\} = \mu_X$,
on se ram\`ene \`a des processus de moyenne nulle en soustrayant
leur moyenne. La formule d'estimation optimale 
(\ref{filtre-wiener}) s'applique alors pour ces processus 
\[
\tilde Y[n] = \mu_Y +  \sum_{l=0}^{N} h[l] (X[n-l] - \mu_X) .
\]
Le syst\`eme d'\'equations de Wiener-Hopf qui d\'efinit
les param\`etres $h[l]$ reste alors le m\^eme.


\subsection{Pr\'ediction lin\'eaire}
\label{predict-lin-se}

La pr\'ediction progressive lin\'eaire
est un cas particulier important de r\'egression
qui se r\'esoud par filtrage de Wiener.
On veut estimer $Y[n]= X[n]$ \`a partir des $N$ 
donn\'ees pr\'ec\'edentes
$\{X[n-k]\}_\UkN$.
L'estimateur lin\'eaire est donn\'e par la projection orthogonale
\begin{equation}
\label{est_prog}
\tilde Y[n] = X^p_N [n]
= \E \{ X[n] \BS X[n-1] , ..., X[n-N] \} = 
\sum_{k=1}^N a_N [k] X[n-k] .
\end{equation}
Les coefficients du filtre optimal $a_N[k]$ sont calcul\'es
de fa\c{c}on \`a ce que l'erreur de pr\'ediction
\begin{equation}
\label{error_formula}
W^p_N [n] = X[n] - X^p_N [n] = X[n] - \sum_{k=1}^N a_N [k] X[n-k] 
\end{equation}
soit orthogonale aux variables al\'eatoires 
$\{X[n-k]\}_\UkN$.
Le syst\`eme d'\'equations de Wiener-Hopf fait donc
intervenir l'autocovariance $R_X[l]$ des variables
$\{X[n-k]\}_\UkN$ et la covariance
\[
R_{YX} [k ] = E\{ X[n] X[n-k]\} = R_X[k] .
\]
On obtient
\begin{equation}
\label{WH_pred}
\left\{
\begin{array}{ccccl}
R_X[0]  a_N[1] & + R_X[1] a_N [2] &+  ...& + R_X[N-1] a_N [N] & = R_X[1]\\
R_X[1]  a_N[1] & + R_X[0] a_N [2] &+  ...& + R_X[N-2] a_N [N] & = R_X[2]\\
. & . & . & . & = . \\
R_X[N-1]  a_N[1] & + R_X[N-2] a_N [2] &+ ... & +  R_X[0] a_N [N] & = R_X[N]\\
\end{array}
\right.
\end{equation}

La variance de l'erreur d'estimation se d\'eduit de 
(\ref{erreur-reg}) avec $E[Y[n]^2 ] = E[X[n]^2 ] = R_X[0]$
\begin{equation}
\label{error_pred}
\epsilon_N = E\{|W [n]|^2\} = 
R_X[0]  - \sum_{k=1}^N a_N[k] R_X[k] .
\end{equation}
\\
\\
{\bf Mod\'elisation Autor\'egressive}
Les \'equations de
Wiener-Hopf (\ref{WH_pred}) pour une pr\'ediction lin\'eaire
sont identiques aux
\'equations de Yule-Walker (\ref{yule-walker})
qui caract\'erisent l'autocovariance
d'un processus AR 
(\`a un signe pr\`es d\^u \`a un changement de signe dans la
d\'efinition des coefficients $a_N [k]$).
Cela prouve que l'\'equation de diff\'erences d'un processus AR est
une r\'egression lin\'eaire optimale.

La variance de l'erreur d'estimation (\ref{error_pred})
est la m\^eme que celle
obtenue en (\ref{variance-bruit}) 
pour un processus AR (en changeant le signe des
coefficients de r\'egression).
Lorsque
$X[n]$ n'est pas un processus AR d'ordre $N$,
l'erreur $W [n]$ n'est pas un bruit blanc.
L'existence de corr\'elations dans l'erreur d'approximation montre
que le processus est d'un ordre sup\'erieur. 

\section{Algorithme de Levinson-Durbin}

La r\'esolution du syst\`eme lin\'eaire (\ref{wiener-hopf}) de $N+1$
\'equations \`a $N+1$ inconnues demande en g\'en\'eral
$O(N^3)$ op\'erations. 
En utilisant la structure Toeplitz de la matrice
de covariance $\R_X$,
l'algorithme de Levinson-Durbin 
factorise $\R_X$ avec $O(N^2)$ op\'erations sous forme
\begin{equation}
\label{factor}
\D = \L \R_X \L^t ,
\end{equation}
o\`u $\D$ est une matrice diagonale et $\L$ est une
matrice triangulaire inf\'erieure. 
Les $N+1$ coefficients du filtre de Wiener se calculent
alors par r\'esolution de l'\'equation de Wiener-Hopf
\[
\bh = \bR_X^{-1} \bR_{YX} = \bf L^t \bf D^{-1} \bf L \bR_{YX} 
\]
avec un total de $O(N^2)$ op\'erations.

L'algorithme de Levinson-Durbin calcule la factorisation
(\ref{factor}) par une orthogonalisation de
Gram-Schmidt de la famille $\{X[n+k]\}_\ZkN$. Cette 
orthogonalisation est \'equivalente \`a une pr\'ediction lin\'eaire.
Nous pr\'esentons un algorithme rapide qui calcule les
coefficients de pr\'ediction par r\'ecurrence en utilisant
les propri\'et\'es de la pr\'ediction progressive et r\'etrograde.

\subsection{Pr\'ediction r\'etrograde}
\label{levions-pred}

La pr\'ediction lin\'eaire r\'etrograde estime $X[n]$
\`a partir de son futur $\{X[n+k]\}_\UkN$
\[
X^r_N [n] = \sum_{k=1}^N a_N[k] X[n+k] = \E \{ X[n] \BS
X[n+1], ..., X[n+N] \} .
\]
Les facteurs $a_N [k]$ de pr\'ediction r\'etrograde sont les
m\^emes que pour la
pr\'ediction progressive (\ref{est_prog}) car 
les syst\`emes d'\'equations de Wiener-Hopf sont les m\^emes 
dans les
deux cas.
Cela se v\'erifie facilement en observant que
$R_X[l] = R_X[-l]$.

L'erreur de pr\'ediction r\'etrograde est
\begin{equation}
\label{error-retro}
W_N^r [n] = X[n] - X^r_N [n] = X[n] - \sum_{k=1}^N a_N[k] X[n+k] .
\end{equation}
Les variances des erreurs d'estimation progressive et r\'etrograde, 
calcul\'ees avec (\ref{erreur-reg}), sont aussi les m\^emes
\[
\epsilon_N = E\{W_N^p [n]^2 \}  = 
E\{W_N^r [n]^2 \} = R_X[0]  - \sum_{k=1}^N a_N[k] R_X[k] .
\]
Le th\'eor\`eme suivant \'etablit deux relations de r\'ecurrence
entre ces erreurs d'estimation, qui permettent de les calculer
rapidement.

\begin{theorem}
\label{levinson}
\begin{equation}
\label{erreur-rec}
\left\{ \begin{array}{l}
W_N^p [n] = W_{N-1}^p [n] - 
K_N ~W^r_{N-1} [n-N] \\
W_N^r [n-N] = W_{N-1}^r [n-N] - 
K_N ~W^p_{N-1} [n] 
\end{array}
\right.
\end{equation}
Si $\epsilon_{N-1} = 0$ alors $K_N = 0$, sinon 
\begin{equation}
\label{reflection}
K_N = \frac {R_X [N] - \sum_{k=1}^{N-1} a_{N-1} [k] R_X[N-k]}
{\epsilon_{N-1}} \leq 1
\end{equation}
et
\begin{equation}
\label{energie-kn}
\epsilon_N = e _{N-1} (1 - K_N^ 2 )  .
\end{equation}
\end{theorem}

\noindent{\bf D\'emonstration de (\ref{erreur-rec})}\\
L'erreur 
\[
W^r_{N-1}[n-N] = X[n-N] - 
\E \{ X[n-N] \BS X[n-N+1] , ... , X[n-1] \} 
\]
est le compl\'ement orthogonal de $X[n-N]$ par rapport
\`a $X[n-1]$, ... , $X[n-N+1]$. 
On peut donc d\'ecomposer la projection
\[
X^p_N [n] =\E \{ X[n] \BS X[n-1] , ... , X[n-N] \} 
\]
comme somme de deux projecteurs
\[
X^p_N [n] 
=\E \{ X[n] \BS X[n-1] , ... , X[n-N+1]\}
+ \E\{ X[n] \BS W^r_{N-1}[n-N] \} ,
\]
ce qui prouve que
\begin{equation}
\label{dcom}
X^p_N [n] 
= X^p_{N-1}[n] 
+ \E\{ X[n] \BS W^r_{N-1}[n-N] \} .
\end{equation}
Comme $W^r_{N-1}[n-N]$ est orthogonale \`a
$X[n-1]$, ... , $X[n-N+1]$, la projection de $X[n]$ sur
$W^r_{N-1}[n-N]$ est \'egale \`a la projection
du compl\'ement de $X[n]$ par rapport 
$X[n-1]$, ... , $X[n-N+1]$ sur 
$W^r_{N-1}[n-N]$ et donc
\begin{equation}
\label{Kn}
\E\{ X[n] \BS W^r_{N-1}[n-N] \} = 
\E\{ W^p_{N-1}[n] \BS W^r_{N-1}[n-N] \} 
= K_N W^r_{N-1}[n-N] .
\end{equation}
En ins\'erant cette \'equation dans (\ref{dcom}) on obtient 
\begin{equation}
X_N^p [n] = X_{N-1}^p [n] +
K_N W^r_{N-1} [n-N] .
\end{equation}
Soustraire chaque c\^ot\'e de cette \'egalit\'e \`a $X[n]$ donne
\[
W_N^p [n] = W_{N-1}^p [n] -  
K_N W^r_{N-1} [n-N] .
\]
La d\'emonstration de la seconde \'equation
se fait exactement de la m\^eme mani\`ere
en inversant les erreurs r\'etrogrades et progressives dans
ces \'equations.

\noindent{\bf D\'emonstration de (\ref{reflection})}\\
Si $\epsilon_{N-1} = 0$ et donc $W^r_{N-1}[n-N] = W^p_{N-1}[n-N] = 
0$, et comme
$W^r_{N}[n-N] = W^p_{N}[n-N] = 0$ 
on peut choisir $K_N = 0$.
Dans le cas o\`u
\[
\epsilon_{N-1} = E \{ W_{N-1}^p [n]^2 \} = 
E\{ W_{N-1}^r [n-N]^2 \} \neq 0 ,
\]
la constante de projection $K_N$ 
d\'efinie en (\ref{Kn}) est alors \'egale au 
produit scalaire normalis\'e des deux vecteurs de la projection
\begin{equation}
\label{express}
K_N = \frac  {E \{ W_{N-1}^p [n]W_{N-1}^r [n-N] \}}
{E\{ W_{N-1}^r [n-N]^2 \}} = 
\frac  {E \{ W_{N-1}^p [n]W_{N-1}^r [n-N] \}} 
{\epsilon_{N-1}} \leq 1 .
\end{equation}
Par ailleurs
\begin{equation}
\label{intermed}
{E \{ W_{N-1}^p [n]W_{N-1}^r [n-N] \}} = 
{E \{ W_{N-1}^p [n]X [n-N] \}} .
\end{equation}
En effet
$W^r_{N-1} [n-N]$ est la composante de $X[n-N]$
qui est orthogonale \`a $X[n-1], X[n-2], ... , X[n-N+1]$,
et $W_{N-1}^p [n]$ est orthogonale \`a ces m\^emes variables 
al\'eatoires.
En rempla\c{c}ant $W_{N-1}^p [n]$ par son expression
(\ref{error_formula}) on d\'eduit de (\ref{express}-\ref{intermed}) que
\[
K_N = \frac {R_X[N] - \sum_{k=1}^{N-1} a_{N-1} [k] R_X[N-k]} 
{\epsilon_{N-1}} .
\]

\noindent {\bf D\'emonstration de (\ref{energie-kn})}\\
Nous avons prouv\'e que
\begin{equation}
\label{error-rec}
W_{N-1}^p [n] = W_N^p [n] + 
K_N W^r_{N-1} [n-N] .
\end{equation}
Comme $W_N^p [n]$ et $W^r_{N-1} [n-N]$
sont deux variables al\'eatoires orthogonales et que
\[
\epsilon_{N-1} = E\{W_{N-1}^p [n]^2\} = 
E\{W_{N-1}^r [n-N]^2\} , 
\]
le th\'eor\`eme de Pythagore appliqu\'e \`a (\ref{error-rec}) 
nous donne
\[
\epsilon_{N-1} = \epsilon_N +  K_N^2 \epsilon_{N-1} . ~~~~\Box
\]
\\
\\
{\bf Coefficients de r\'egression}
Pour calculer les coefficients de r\'egression $a_N[k]$,
on \'etablit une relation de r\'ecurrence qui est une cons\'equence
des
\'equations (\ref{erreur-rec}) du Th\'eor\`eme 
\ref{levinson}.
Rappellons que
\[
W^p_{N}[n] = X[n] - \sum_{k=1}^N a_N[k] X[n-k] ~~
\]
et 
\[
W^r_{N}[n-N] = X[n-N] - 
\sum_{k=1}^{N} a_{N}[k] X[n-N+k] .
\]
Si $\epsilon_{N-1} \neq 0$ alors 
les variables al\'eatoires $(X[n-k])_\ZkN$
sont lin\'eairement
ind\'ependantes, et (\ref{erreur-rec}) se traduit par une
\'egalit\'e sur les facteurs devant chaque variable al\'eatoire.
On obtient 
\begin{equation}
\label{rec_ak}
\left\{ \begin{array}{l}
a_N[k] =  a_{N-1} [k] - K_N a_{N-1} [N-k] ~~,~~
1 \leq k \leq N-1 \\
a_N[N] =  K_N
\end{array}
\right.
\end{equation}
L'algorithme de Levinson-Durbin utilise les \'equations 
(\ref{rec_ak}-\ref{reflection}-\ref{energie-kn})
pour calculer 
$\epsilon_N$, $K_N$ et $a_N$ par r\'ecurrence \`a partir
de l'autocovariance $R_X$.
\vspace{5mm}

\noindent {\bf Algorithme de Levinson-Durbin}\\
{\it Initialisation:}  $\epsilon_{0} = R_X[0] $.\\
{\it Boucle:} Pour $m$ allant de $1$ \`a $N$\\
Calcul de $K_m$\\
\indent Si $\epsilon_{m-1} > 0$\\
\begin{equation}
K_m = \frac {R_X [m] - \sum_{k=1}^{m-1} a_{m-1} [k] R_X[m-k]} 
{\epsilon_{m-1} } 
\end{equation}
\indent Sinon
\[
K_m = 0 .
\]
Calcul de $a_m$\\
\[
\left\{ \begin{array}{l}
a_m[k] =  a_{m-1} [k] - K_m a_{m-1} [m-k] ~~,~~
1 \leq k \leq m-1 \\
a_m[m] =  K_m
\end{array}
\right.
\]
Calcul de $\epsilon_m$\\
\[
\epsilon_m = \epsilon_{m-1} (1 - K_m^2 ) .
\]
\\
Cet algorithme
demande $O(N^2 )$ op\'erations
pour calculer $a_{N}[k]$ pour $1 \leq k \leq N$.


\subsection{Filtrage en treillis}

Au-del\`a du probl\`eme de pr\'ediction, l'algorithme de
Levinson-Durbin permet de r\'esoudre en $O(N^2)$ op\'erations les
\'equations de Wiener-Hopf pour l'estimation de tout
processus $Y[n]$ conjointement stationnaire avec
$\{X [n] , X[n-1] , ..., X[n-N] \}$.
\\
\\
{\bf Orthogonalisation rapide}
Les erreurs de pr\'ediction r\'etrogrades 
\[
\{W^r_0 [n] , W^r_1 [n] , ..., W^r_N [n] \}
\]
peuvent s'interpr\'eter comme
une orthogonalisation de Gram-Schmidt des
variables al\'eatoires
$\{X [n] , X[n-1] , ..., X[n-N] \}$
\[
\begin{array}{l}
W^r_0 [n] = X[n] \\
W^r_1 [n-1] = X[n-1] - \E \{ X[n-1] \BS X[n] \} \\
...\\
W^r_N [n-N] = 
X[n-N] - \E \{ X[n-N] \BS X[n],..., X[n-N+1] \} \\
\end{array}
\]

L'algorithme de Levinson-Durbin effectue un calcul rapide
de cette orthogonalisation gr\^ace aux \'equations r\'ecurrentes
\begin{equation}
\label{erreur-rec2}
\left\{ \begin{array}{l}
W_k^p [n] = W_{k-1}^p [n] - 
K_k ~W^r_{k-1} [n-k] \\
W_k^r [n-k] = W_{k-1}^r [n-k] - 
K_k ~W^p_{k-1} [n] 
\end{array}
\right.
\end{equation}
Ces \'equations s'impl\'ementent 
par un filtrage en treillis illustr\'e
par la figure \ref{bloque-treillis}.

\begin{figure}[bhtp]
%\centerline{
%	\leavevmode\epsfbox{/home/mallat/X/TREX/figures/SigFig/FIG5.3.EPS.txt}}
\vspace{5cm}
\caption{Bloque \'el\'ementaire d'un filtrage 
en treillis pour calculer les erreurs de pr\'ediction progressives
et r\'etrogrades.}
\label{bloque-treillis}
\end{figure}
\\
\\
{\bf Treillis}
Plut\^ot que d'exprimer l'estimation $\tilde Y [n]$ 
de $Y[n]$ comme combinaison
lin\'eaire des variables 
$\{X [n] , X[n-1] , ..., X[n-N] \}$ par le
filtre de Wiener $\bh$, il est souvent
plus efficace de d\'ecomposer cette estimation sur les
variables orthogonales
$\{ W^r_0[n], ..., W_N^r[n] \}$ qui g\'en\`erent le
m\^eme espace et que l'on
calcule efficacement par un filtre en treillis.
\[
\tilde Y [n] = \E \{ Y[n] \BS X[n], ..., X[n-N] \} = 
\E \{ Y[n] \BS W^r_0[n], ..., W_N^r[n-N] \} .
\]
Comme ces variables al\'eatoires sont orthogonales
\begin{equation}
\label{estim-some}
\tilde Y [n] = 
\sum_{k=0}^N \E \{ Y[n] \BS W_k^r[n-k] \} =
\sum_{k=0}^N c[k] W_k^r[n-k] ,
\end{equation}
avec
\[
c[k] = \frac {E \{Y[n]  W_k^r[n-k] \} } 
{E \{ W_k^r[n-k]^2 \} } .
\]
En rempla\c{c}ant $W_k^r[n-k]$ par son expression (\ref{error-retro})
on obtient
\[
c[k] = \frac {R_{YX} [0] - \sum_{i=1}^k a_k[i] R_{YX} [k-i]\} } 
{\epsilon_k } .
\]
Si l'on a d\'ej\`a calcul\'e les coefficients $a_k[n]$ avec
l'algorithme de Levinson-Durbin, le
calcule de $c[k]$ pour $0 \leq k \leq N$ demande
$O(N^2)$ op\'erations.

Les \'equations de r\'ecurrence (\ref{erreur-rec}) 
du th\'eor\`eme \ref{levinson} montrent
que $W_k^r[n-k]$ se calcule par une cascade
de filtres en treillis illustr\'e en figure \ref{fitre-treillis}.
La somme (\ref{estim-some}) de l'estimation $\tilde Y [n]$ 
s'obtient donc par cette architecture en treillis.\\

\begin{figure}[bhtp]
%\centerline{
%	\epsfxsize=14cm	
%	\leavevmode\epsfbox{/home/mallat/X/TREX/figures/SigFig/FIG5.1.EPS.txt}}
\vspace{8cm}
\caption{Calcul par filtrage en treillis de
l'estimateur lin\'eaire optimal $\tilde Y[n]$ de $Y[n]$ sachant
$X[n] ... X[n-N]$}
\label{fitre-treillis}
\end{figure}

