\chapter{Traitement du signal al\'eatoire}
\label{aleatoire-chap}

Pour analyser les propri\'et\'es d'une 
classe de signaux, tels que des signaux de paroles en g\'en\'eral
ou le son ``s'' prononc\'e par diff\'erents locuteurs,
on utilise une mod\'elisation par des processus stochastiques qui
refl\`ete les propri\'et\'es communes de ces signaux.
Cette mod\'elisation permet de s\'eparer le
signal d'un bruit dont les caract\'eristiques stochastiques
sont diff\'erentes, de coder efficacement les signaux d'une 
classe, ou m\^eme de les identifier.
Nous ne consid\'erons ici que des signaux 
r\'eels stationnaires, dont 
les propri\'et\'es ne changent
pas au cours du temps.

L'hypoth\`ese de stationnarit\'e nous ram\`ene dans le champ de la
transform\'ee de Fourier, qui permet de d\'efinir la
puissance spectrale d'un processus. Nous nous
concentrons sur les propri\'et\'es du second ordre des
processus stationnaires, dont la
mod\'elisation se r\'eduit \`a la recherche
d'un filtre param\'etr\'e. Nous \'etudions plus 
particuli\`erement les processus autor\'egressifs dont les
param\`etres se calculent par la r\'esolution d'un syst\`eme
lin\'eaire.

\section{Processus stationnaires au sens large}

Un processus discret \`a valeurs r\'eelles est une suite 
de variables al\'eatoires $\{X[n]\}_\nZ$ 
d\'efinies sur un espace de probabilit\'e
$(\Omega , {\cal A} , {\rm P})$.
Ce processus est caract\'eris\'e par la loi de probabilit\'e
\[
P(X[{n_1}] \in [a_1,b_1] , ... , X[{n_k}] \in [a_k,b_k] )
\]
de tout sous-ensemble de $k$ variables al\'eatoires, pour tout
intervalle $[a_i,b_i]$ avec $1 \leq i \leq k$.

La r\'ealisation d'un tel processus est un signal discret
$x[n]$ qui donne les valeurs prises par chaque
variable al\'eatoire $X[n]$ lors d'une observation.
On mod\'elise une classe de signaux par un processus al\'eatoire
dont les r\'ealisations correspondent \`a 
l'ensemble de tous les signaux de cette classe.
Par exemple, des enregistrements de temp\'erature en
diff\'erents points de la terre
peuvent \^etre mod\'elis\'es par un processus discret. 
Une r\'ealisation
de ce processus est l'ensemble des enregistrements de 
temp\'eratures \`a un moment donn\'e.
\\
\\
{\bf Sationnarit\'e}
Le processus est 
strictement stationnaire si 
sa loi de probabilit\'e ne change pas avec un d\'ecalage temporel.
Pour tout $p \in \Z$ et
tout sous-ensemble de 
$k$ variables al\'eatoires, la loi de probabilit\'e de
$\{X[{n_1}], ... , X[{n_k}] \}$ est la m\^eme que celle de
$\{X[{n_1}+p], ... , X[{n_k}+p] \}$. 
Cela implique que la moyenne ne d\'epend pas de
l'instant
\begin{equation}
\label{moyenne}
\mu[n] = E\{X[n]\} =  \mu ,
\end{equation}
et que l'autocovariance ne d\'epend que de la diff\'erence 
de position
\begin{equation}
\label{variance}
\Cov(X[n],X[m]) = E\{(X[n] - \mu[n]) ( X[m] -  \mu [m])\} = R[n-m] .
\end{equation}

En traitement du signal, 
on observe le plus souvent une seule
r\'ealisation, \`a partir de laquelle on
veut estimer certains param\`etres du processus sous-jacent.
La pauvret\'e de cette information num\'erique 
nous limite g\'en\'eralement 
\`a l'\'etude de la moyenne et de l'autocovariance.
Puisque l'on n'\'etudie que les moments d'ordre 2 du
processus, au lieu d'imposer que le processus soit strictement 
stationnaire, nous supposons simplement que la moyenne et
la covariance satisfont aux propri\'et\'es de stationnarit\'e
(\ref{moyenne}) et (\ref{variance}).
Si de plus la variance du processus est finie
\begin{equation}
\label{variance-finie}
E\{|X[n]|^2\}  < +\infty ,
\end{equation}
on dit que le processus est
stationnaire au sens large (SSL).\\
\\

\noindent {\bf Exemple}
Un processus $X[n]$ est 
{\it gaussien} si et seulement si pour tout
$k$ et $n_1$, ..., $n_k$, le vecteur de $k$ variables 
al\'eatoires
$(X[{n_1}], ... , X[{n_k}])$ est gaussien.
On rappelle \cite{neveu} qu'un
vecteur de $p$ variables al\'eatoires $(X_1, ... ,X_p )$
est gaussien si et seulement
si la densit\'e de probabilit\'e peut s'\'ecrire sous la forme
\[
p(x_1 , ... , x_p ) = \frac 1 {(2 \pi)^{p/2} |\bR|^{1/2}}
\exp\left[ -\frac 1 2 ( {\bf x} - {\bf m})^t \rm \bR^{-1} 
({\bf x} - {\bf m})\right] ,
\]
o\`u ${\bf x} = (x_1 , ... , x_p )$ est le vecteur de valeurs,
${\bf m} = (\mu_1 , ... , \mu_p )$ est le vecteur de moyennes avec
$\mu_i = E\{X_i\}$,
${\bf R}= ( \Cov ( X_n, X_m ))_{1 \leq n \leq p ,1 \leq m \leq p }$
est la matrice de covariance des $p$ variables al\'eatoires,
${\bf R}^{-1}$ son inverse
et $|\bR|$ son d\'eterminant. 
Un processus gaussien est donc
enti\`erement d\'efini par sa moyenne et sa covariance.
Si $X[n]$ est gaussien et stationnaire au sens large (SSL),
on v\'erifie facilement qu'il est aussi stationnaire au sens 
strict. 

En l'absence d'information sur les moments d'ordre
sup\'erieur \`a 2, on suppose souvent par d\'efaut que le processus
\'etudi\'e est gaussien. Les processus gaussiens
apparaissent dans de nombreux ph\'enom\`enes physiques 
\`a cause du th\'eor\`eme de limite centrale 
\cite{neveu} qui d\'emontre
que la somme de variables al\'eatoires ind\'ependantes 
converge vers une variable al\'eatoire gaussienne.

\subsection {Estimation de la moyenne et de l'autocovariance}
\label{est-moy}

Si le processus est stationnaire, on peut tenter d'estimer 
la moyenne $\mu$ et la fonction 
d'autocorr\'elation \`a partir de moyennes temporelles
d'une r\'ealisation.
\\
\\
{\bf Moyenne}
L'estimateur classique
de la moyenne \`a partir de $N$ \'echantillons
d'une r\'ealisation de $X[n]$ est la variable al\'eatoire 
d\'efinie par
\[
 \mu_N = \frac 1 N \sum_{n=0}^{N-1} X[n] .
\]
Cet estimateur est non biais\'e puisque
\[
E\{ \mu_N\} = \mu .
\]

Lorsque $N$ augmente il est n\'ecessaire que la variance de
$ \mu_N$ d\'ecroisse vers z\'ero,
de fa\c{c}on \`a am\'eliorer l'estimation de $\mu$.
On dit alors que le processus est ergodique pour la moyenne.
La proposition suivante donne une condition sur l'autocovariance
pour que cette propri\'et\'e soit satisfaite.

\begin{proposition}
Le processus est ergodique pour la moyenne si et seulement si
\[
\lim_{N \rightarrow +\infty} E\{(\mu -  \mu_N)^2\} = 
\lim_{N \rightarrow +\infty} 
\frac 1 N \sum_{l=-N+1}^{N-1} (1 - \frac {|l|} N) R[l] = 0.
\]
\end{proposition}

\noindent{\bf D\'emonstration}
La variance de $ \mu_N$ est
\begin{eqnarray}
E\left \{ \left( \mu_N - \mu \right)^2\right\}
& = & \frac 1 {N^2} 
E \left \{ \left(\sum_{n=0}^{N-1} (X[n] - \mu)\right)^2 \right\} \\
& = & \frac 1 {N^2} 
E \left \{ \sum_{n=0}^{N-1} \sum_{m=0}^{N-1} 
(X[n] - \mu) (X[m] - \mu) \right \} \\
& = & \frac 1 {N^2} 
\sum_{n=0}^{N-1} \sum_{m=0}^{N-1} R[n-m] \\ 
& = &
\frac 1 N \sum_{l=-N+1}^{N-1} (1 - \frac {|l|} N) R[l].
\end{eqnarray}
$\Box$\\
\\

La fonction d'autocovariance doit avoir une d\'ecroissance
suffisamment rapide ce qui signifie que les corr\'elations 
longue-port\'ee sont faibles. Le cas le plus favorable correspond \`a
des variables $X[n]$ qui sont deux \`a deux d\'ecorr\'el\'ees, 
si bien que
$R[l] = R[0] \delta[l]$.
On a alors 
\[
E\{(\mu -  \mu_N)^2\} = \frac {R[0]} N .
\]
\\
\\
{\bf Autocovariance}
Pour tout $k$, l'autocovariance $R[k]$ de $X[n]$
peut s'estimer \`a partir
d'une r\'ealisation en utilisant la variable al\'eatoire
\begin{equation}
\label{biais}
 R_N [k] = \frac 1 {N}
\sum_{n=0}^{N-1-|k|} (X[n] -  \mu_N) (X[n+|k|] -  \mu_N).
\end{equation}
Si on remplace $ \mu_N$ par la vraie moyenne $\mu$, on
s'aper\c{c}oit que cet estimateur est biais\'e
\[
E\{  R_N [k]\} = \frac {N-|k|} N R[k].
\]
Le biais 
\[
E\{R_N [k]\} - R[k] = \frac {k R [k]} {N} 
\]
est toujours petit si $R[k]$ d\'ecro\^it rapidement lorsque $k$
augmente.
Dans le cas d'un processus Gaussien, on peut montrer [Priestley] que
si $R[k] \in \lD$ alors la variance de $R_N [k]$
est en $O(\frac 1 N)$.
L'erreur quadratique moyenne de l'estimateur est \'egale 
\`a la somme du biais au carr\'e et de la variance.
Si $R[k] = O(\frac 1 {\sqrt k})$ alors
$E\{|R_N [k] - R[k]|^2\} =  O(\frac 1 N)$.

On peut d\'efinir un estimateur non biais\'e 
\begin{equation}
\label{non-biais}
\tilde R_N [k] = \frac N {N-|k|} R_N [k] ,
\end{equation}
mais la variance de cet estimateur 
est en $O(\frac 1 {N-|k|})$. 
Cette variance est grande lorsque
$k$ est de l'ordre de $N$.
Si $R[k] = O(\frac 1 {\sqrt k})$ et $k$ est de l'ordre de $N$, 
l'erreur quadratique moyenne
de l'estimateur $ R_N [k]$ est plus faible que
celle de l'estimateur non biais\'e $\tilde R_N [k]$.
On utilise donc plut\^ot l'estimateur biais\'e (\ref{biais}) que
l'estimateur (\ref{non-biais}).

\subsection{Espace hilbertien de variables al\'eatoires}

N'ayant acc\`es qu'aux moments du second ordre, il est n\'ecessaire
de d\'efinir des notions de distance et
de convergence \`a partir de ces moments.
Pour cela, on travaille sur l'espace $\LDP$ 
des variables al\'eatoires $A$
d\'efinies sur l'espace de probabilit\'e 
$(\Omega , \cal A , \rm P)$ et
dont l'esp\'erance
quadratique est finie
\begin{equation}
\label{norme-P}
|| A ||_{\LDP} ^2 = E \{A^2\} < + \infty .
\end{equation}
Comme $E\{A\} ^2 \leq E \{A^2\}$ et que la variance de A est
\[
\Cov(A,A) = E\{(A - E\{A\})(A - E\{A\})\} = E\{A^2\} - E\{A\}^2, 
\]
on peut remplacer (\ref{norme-P})
par une condition sur la moyenne et la variance
\begin{equation}
\label{covar-cond}
E\{A\} < +\infty ~~~\mbox{et}~~~~
\Cov(A,A) < +\infty .
\end{equation}


On d\'emontre \cite{neveu}
que $\LDP$ est un espace de Hilbert pour
le produit scalaire
\begin{equation}
<A,B>_\LDP = E\{A B \} .
\end{equation}
Par d\'efinition, une suite de variables al\'eatoires 
$\{A_n\}$ converge vers $A_\infty$
au sens de l'esp\'erance quadratique si et seulement si
\[
\lim_{n \rightarrow + \infty} || A_\infty - A_n ||^2 = 
\lim_{n \rightarrow + \infty} E\{(A_\infty - A_n)^2\} = 0.
\]

\subsection{Op\'erateur de covariance}
Le traitement du signal al\'eatoire utilise 
essentiellement des combinaisons
lin\'eaires de variables al\'eatoires correspondant aux valeurs
d'un processus \`a diff\'erents instants.
Nous montrons que l'autocovariance de $X[n]$ 
\[
R_X[n,m]= \Cov (X[n],X[m]) = 
E\{(X[n] - E\{X[n]\})(X[m] - E\{X[m]\})\}
\]
permet de facilement calculer la covariance de combinaisons
lin\'eaires des $X[n]$.

Soient
\[
A = \sum_{n=-\infty}^{+\infty} a[n] X[n] ~~\mbox{et}~~~
B = \sum_{n=-\infty}^{+\infty} b[n] X[n] .
\]
Les moyennes de ces variables al\'eatoires \'etant
\[
E\{A\} = \sum_{n=-\infty}^{+\infty} a[n] E\{X[n]\}  
~~\mbox{et}~~~
E\{B\} = \sum_{n=-\infty}^{+\infty} b[n] E\{X[n]\}  
\]
on d\'eduit 
\[
\Cov(A,B)  = 
E \left\{ \sum_{n=-\infty}^{+\infty} a[n] (X[n] - E\{X[n]\}) 
\sum_{m=-\infty}^{+\infty} b[m] (X[m] - E\{X[m]\}) 
\right\}  
\]
et donc
\begin{equation}
\label{covar}
\Cov(A,B)  = 
\sum_{n=-\infty}^{+\infty} 
a[n] \sum_{m=-\infty}^{+\infty} b[m] R_X [n,m] .
\end{equation}
Soit $C$ l'op\'erateur sym\'etrique de covariance d\'efini par
\begin{equation}
\label{cov}
Cb[n] = \sum_{m=-\infty}^{+\infty} b[m] R_X [n,m] ,
\end{equation}
on peut r\'e\'ecrire (\ref{covar}) 
comme un produit scalaire dans $\lD$
\[
\Cov(A,B) = ~< a , Cb >~.
\]
L'op\'erateur de covariance est sym\'etrique et positif car
\[
<a,Ca> = \Cov(A,A) \geq 0 . 
\]


\subsection{Puissance spectrale}

L'autocovariance de $X[n]$ est caract\'eris\'ee
par les vecteurs propres et valeurs propres de $C$.
Si $X[n]$ est stationnaire au sens large 
\[
R_X [n,m] = R_X [n-m], 
\]
donc
\[
Ca[n] = \sum_{m=-\infty}^{+\infty} a[m] R_X[n-m] = a \star R_X [n] .
\]
est un op\'erateur de convolution discret.
Nous avons vu dans
le paragraphe \ref{sec-conv} que les vecteurs propres d'une
convolution discr\`ete sont les exponentielles
$e_\om [n] = \enom$ 
\[
C e_\om [n] = \hat R_X (e^{i \om}) e_\om [n] .
\]
Les valeurs propres associ\'ees sont positives et donn\'ees
par la s\'erie de Fourier
\[
\hat R_X(\eom) = \sum_{n=-\infty}^{+\infty} R_X[n] \emnom  \geq 0 .
\]
La fonction $\hat R_X(\eom)$ est appel\'ee 
{\it puissance spectrale} du processus car nous verrons que
cela mesure l'\'energie moyenne du processus par unit\'e de
fr\'equence. 
La puissance spectrale caract\'erise compl\`etement
l'autocovariance du processus que l'on retrouve par
transform\'ee de Fourier inverse (\ref{recons_discret}). 
En particulier, 
la variance du processus est
\[
\sigma^2 = 
R_X[0] = \frac 1 {2 \pi} \int_{0}^{2 \pi} \hat R_X(\eom) d \om .
\]
\\
\\
{\bf Exemple}
On appelle {\em bruit blanc} tout processus SSL dont les
valeurs \`a des instants diff\'erents sont d\'ecorr\'el\'es,
ce qui signifie que
\[
R_X[n] = \sigma^2 \delta [n] .
\]
La puissance spectrale d'un bruit blanc est donc constante
\[
\hat R_X(\eom) = \sigma^2 .
\]

\subsection{Filtrage homog\`ene}

Une convolution discr\`ete \'etant homog\`ene dans le temps,
on peut s'attendre \`a ce que cela ne modifie pas la stationnarit\'e
d'un processus. Le th\'eor\`eme suivant 
relie la puissance spectrale
d'un processus filtr\'e \`a la puissance spectrale originale.

\begin{theorem}
\label{cov_conv_th} 
Soient $h[n]$ et
$g [n]$ les r\'eponses impulsionnelles de deux filtres dans
$\lD$, dont les fonctions de transfert sont born\'ees.
Soit $X[n]$ un processus SSL tel que $R_X [n] \in \lU$. 
Les deux processus
\[
Y[k] = h \star X [k] = \sum_{n=-\infty}^{+\infty} h[k-n] X[n] 
\]
et 
\[ 
Z[l] = g \star X [l] = \sum_{m=-\infty}^{+\infty} g[l-m] X[m]  
\]
sont stationnaires au sens large et
\begin{equation}
\label{cov_conv}
\Cov(Y[k],Z[l]) = h \star \tilde g \star R_X [k-l] ,
\end{equation}
avec $\tilde g[n] = g[-n]$. La puissance spectrale de $Y[n]$ est
\begin{equation}
\label{spect}
\hat R_{Y} (\eom) = |\hat h (\eom)|^2 \hat R_X (\eom) .
\end{equation}
\end{theorem}

\noindent {\bf D\'emonstration}\\
En identifiant $Y[n]$ et $Z[m]$ respectivement \`a $A$ et $B$
dans (\ref{covar}) on obtient
\[
\Cov(Y[k], Z[l]) = \sum_{n=-\infty}^{+\infty} h[k-n]
\sum_{m=-\infty}^{+\infty} g[l-m] R_X[n-m].
\]
Le changement de variables $(n,m) \rightarrow (k-n',l+m')$
donne 
\[
\Cov(Y[k], Z[l]) = \sum_{n=-\infty}^{+\infty} h[n']
\sum_{m=-\infty}^{+\infty} \tilde g[m'] R_X[k-l-n'-m]
\]
qui est \'equivalent \`a (\ref{cov_conv}). 

Pour montrer que $Y[n]$ et $Z[n]$ sont stationnaires 
au sens large, on v\'erifie d'abord que leur moyennes sont 
constantes et finies. Concentrons nous sur $Y[n]$
\[
E\{ Y[k]\} = \sum_{n=-\infty}^{+\infty} h[k-n] E \{ X[n] \} =
E \{ X[n] \} \hat h (1) < + \infty.
\]
On calcule sa covariance en prenant $Z = Y$ dans (\ref{cov_conv}) 
\begin{equation}
\label{cov-Y}
\Cov(Y[k], Y[l]) = R_Y [k-l] = h \star \tilde h \star R_X [k-l] ,
\end{equation}
ce qui d\'emontre sa stationnarit\'e au sense large.
Comme la transform\'ee de Fourier de $h \star \tilde h[n]$
est $|\hat h (\eom)|^2$, 
la puissance spectrale (\ref{spect}) de $Y$ se d\'eduit de
(\ref{cov-Y}) par le th\'eor\`eme de convolution discret. $\Box$\\
\\

Ce th\'eor\`eme montre que la notion de filtrage par convolution
reste valable pour les processus stationnaires puisque les
composantes fr\'equentielles du processus filtr\'e sont
att\'enu\'ees ou amplifi\'ees par $|\hat h(\eom)|^2$.
\\
\\
{\bf Densit\'e d'\'energie}
On appelle
$\hat R_X (e^{i \xi})$ puissance spectrale
car on
peut l'interpr\'eter comme une densit\'e d'\'energie
le long de l'axe des fr\'equences.

Soit $h^\xi_\Delta[n]$ le filtre r\'eel 
dont la fonction de transfert est
\[
\hat h_{\Delta}^\xi (\eom) = 
   \left \{ \begin{array}{ll} 
            \frac 1 {\pi \Delta} & 
\mbox{si $| |\om| - \xi| \leq \Delta/2$ }\\
            0 & \mbox{si $| |\om| - \xi | > \Delta/2$}
            \end{array}
   \right.  
\]
L'\'energie de ce filtre est normalis\'ee
\[
\|h_{\Delta}^\xi \|^2 = \frac 1 {2 \pi} \int_{-\pi}^{+\pi}
|\hat h_{\Delta}^\xi (\eom) |^2 d\om = 1 .
\]
Soit 
\[
X^\xi_\Delta [n] = X \star h^\xi_\Delta [n] 
\]
le processus obtenu en ne gardant que les composantes
fr\'equentielles dans le voisinage de $\xi$.
Le r\'esultat (\ref{spect}) du
th\'eor\`eme \ref{cov_conv_th} permet de montrer que
la variance de ce processus est
\begin{eqnarray*}
E\{|X^\xi_\Delta[n]|^2\} &=& R_{X^\xi_\Delta} [0] = \frac 1 {2 \pi} 
\int_{-\pi}^{+\pi}\hat R_{X^\xi_\Delta} (e^{i \om}) d\om\\
& = & \frac 1 2
\int_{||\om| - \xi| \leq \frac {\Delta} 2}
\frac {\hat R_{X} (e^{i \om})} {\Delta} d\om . 
\end{eqnarray*}
Comme $R_X [n] \in \lU$, $\hat R_X (\eom )$ est continue et
$\hat R_X (e ^{i \xi}) = \hat R_X (e ^{-i \xi})$ si bien que
\[
\lim_{\Delta \rightarrow 0} R_{X^\xi_\Delta} [0] =
\hat R_X(e ^{i \xi})  . 
\]
La puissance spectrale \`a la fr\'equence $\xi$ est 
donc proportionnelle
\`a la densit\'e d'\'energie du processus filtr\'e
autour de la fr\'equence $\xi$.

%\subsection{Filtrage adapt\'e}
%
%Le th\'eor\`eme de filtrage des processus SSL permet de calculer
%les filtres optimaux pour d\'etecter un signal connu d\'egrad\'e
%par un bruit additif, stationnaire.
%Par exemple,
%toute cible rep\'er\'ee par une onde sonar ou radar produit
%une onde r\'efl\'echie dont la signature est souvent 
%connue \`a priori.
%Cette signature est cependant contamin\'ee par le bruit provoqu\'e
%par les structures al\'eatoires du milieu de propagation.
%Ce bruit peut \^etre approxim\'e par un processus
%stationnaire dont on conna\^{\i}t la puissance spectrale.
%Pour d\'etecter la pr\'esence 
%et la position d'un signal particulier,
%on filtre
%le signal bruit\'e de fa\c{c}on \`a amplifier la r\'eponse 
%du signal 
%par rapport \`a celle du bruit. La position du signal est alors
%simplement d\'etect\'ee par seuillage.
%
%Le signal bruit\'e est un processus
%\[
%Y[n] = f[n-p] + W[n] ,
%\]
%o\`u $W[n]$ est un bruit de puissance spectrale 
%$\hat R_W (\eom)$ connue et de moyenne nulle,
%tandis que $f[n-p]$ est le signal 
%d\'eterministe centr\'e en $p$, que l'on cherche \`a rep\'erer. 
%Soit $h[n]$ la r\'eponse impulsionnelle du
%filtre
%\[
%Y \star h [n] = f \star h [n-p] + W \star h [n] .
%\]
%Lorsque $n=p$, on veut maximiser le rapport entre la r\'eponse
%du signal et l'\'energie moyenne du bruit
%\[
%\frac S B = \frac {(f \star h [0])^2} {E\{(W \star h [p])^2\}} .
%\]
%
%\begin{theorem}
%Si 
%\[
%D = \{ \om \in \R~ |~\hat R_W (\eom) = 0 ~~~
%\mbox{et}~~~\hat f (\eom) \neq 0 \}
%\]
%est de mesure non nulle, alors on prend
%$\hat h (\eom) = 1_D (\om)$ et $\frac S B = +\infty$.
%Sinon
%la fonction de transfert du
%filtre qui maximise le rapport signal sur bruit est
%\begin{equation}
%\label{optimal}
%\hat h (\eom) = \frac {\hat f^* (\eom)} { \hat R_W (\eom)} ,
%\end{equation}
%avec
%\begin{equation}
%\label{noise}
%\frac S B = \frac 1 {2 \pi} 
%\int_{0}^{2\pi} \frac {|\hat f (\eom)|^2} {\hat R_W
%(\eom)} d\om .
%\end{equation}
%\end{theorem}
%
%\noindent{\bf D\'emonstration}
%Le bruit \'etant stationnaire, la puissance spectrale de
%$W \star h [p]$ est $\hat R_W (\eom) |\hat h (\eom)|^2$ et
%sa variance est
%\[
%E\{(W \star h [p])^2\} = \frac 1 {2 \pi} 
%\int_{0}^{2\pi} \hat R_W (\eom) |\hat h (\eom)|^2 d \om .
%\]
%Par ailleurs
%\[
%f \star h [0] =  \frac 1 {2 \pi} 
%\int_{0}^{2\pi} \hat f (\eom) \hat h (\eom) d\om.
%\]
%Donc
%\begin{equation}
%\label{S/B}
%\frac S B = \frac {\left(\int_{0}^{2\pi} \hat f (\eom) \hat h (\eom)
%d\om 
%\right)^2}
%{ 2 \pi \int_{0}^{2\pi} \hat R_W (\eom) |\hat h (\eom)|^2 d \om } .
%\end{equation}
%Si 
%\[
%D = \{ \om \in \R~ |~\hat R_W (\eom) = 0 ~~~
%\mbox{et}~~~\hat f (\eom) \neq 0 \}
%\]
%est de mesure non nulle, alors 
%la fonction indicatrice $\hat h (\eom) = 1_D (\om)$ 
%produit un rapport signal sur bruit infini.
%Si $D$ est de mesure nulle,
%l'in\'egalit\'e de Schwartz montre que
%\begin{eqnarray*}
%\left( \int_{0}^{2\pi} \hat f (\eom) \hat h (\eom) d\om\right)^2 &= &
%\left( 
%\int_{0}^{2\pi} \frac {\hat f (\eom)} {\sqrt {\hat R_W (\eom)}}
% \hat h (\eom) {\sqrt {\hat R_W (\eom)}} d\om \right)^2 \\
%&\leq &
%\int_{0}^{2\pi} \frac {|\hat f (\eom)|^2} {\hat R_W (\eom)} d\om
%\int_{0}^{2\pi}  |\hat h (\eom)|^2 {\hat R_W (\eom)} d\om . 
%\end{eqnarray*}
%Ins\'erer cette in\'egalit\'e dans (\ref{S/B}) nous donne
%\begin{equation}
%\frac S B \leq \frac 1 {2 \pi} 
%\int_{0}^{2\pi} \frac {|\hat f (\eom)|^2} {\hat R_W
%(\eom)} d\om .
%\end{equation}
%On obtient une \'egalit\'e lorsque l'in\'egalit\'e de Schwartz est
%une \'egalit\'e, ce qui se produit si et seulement si
%$\hat h (\eom) {\sqrt {\hat R_W (\eom)}}$ est proportionnelle \`a
%$\frac {\hat f^* (\eom)} {\sqrt {\hat R_W (\eom)}}$,
%ce qui prouve (\ref{optimal}).
%$\Box$\\
%\\
%
%Si le bruit $W[n]$ est un bruit blanc, $\hat R_W (\eom) = 1$,
%auquel cas $\hat h (\eom) = {\hat f^* (\eom)}$ et
%donc $h[n] = f[-n]$. 
%
%Si $\frac S B > 1$, on peut d\'etecter la pr\'esence et la position
%du signal par un seuillage
%en rep\'erant les valeurs de $n$ pour lesquelles 
%\[
%|Y \star h [n]| \geq 
%\frac 1 2 |f \star h[0]| =
%\frac 1 { {4 \pi}}
%{ \int_{0}^{2 \pi} 
%\frac {|\hat f (\eom)|^2} {\hat R_W (\eom)} d \om} .
%\]
%Les probabilit\'es de fausse d\'etection ou de non-d\'etection
%peuvent \^etre calcul\'ees dans le cas d'un bruit gaussien.

\section{Mod\'elisation}

Pour analyser les r\'ealisations d'un processus 
il est souvent n\'ecessaire d'approximer ce processus avec un
mod\`ele faisant intervenir le moins possible de param\`etres.
Dans le cas des processus stationnaires au sens large,
on utilise des mod\`eles obtenus par filtrage d'un bruit blanc.
Un bruit blanc $W[n]$ filtr\'e par une r\'eponse impulsionnelle $h[n]$
\[
\tilde X [n] = W \star h [n]
\]
d\'efinit un processus SSL dont la puissance spectrale est
\[
\hat R_{\tilde X} (\eom ) = |\hat h (\eom)|^ 2 .
\]
Nous \'etudions les mod\`eles obtenus avec des filtres 
autor\'egressifs.

\subsection{Processus autor\'egressifs}
\label{proc-autoregre-se}

La classe de mod\`eles les plus utilis\'es en traitement du signal
est constitu\'ee de processus autor\'egressifs (AR) qui satisfont
une \'equation r\'ecurrente
\begin{equation}
\label{AR}
X[n] + \sum_{k=1}^N a[k] X[n-k] =  W[n] ,
\end{equation}
o\`u $W [n]$ est un bruit blanc.
Cette \'equation est
une r\'egression lin\'eaire de $X[n]$
sur $N$ valeurs pass\'ees, auquelles est
ajout\'ee $W [n]$ qui est l'{\em innovation} au temps $n$, non
pr\'evue par la r\'egression.
Les constantes de r\'egression $(a[k] )_{1 \leq k \leq N}$ sont les
param\`etres du processus.
L'\'equation (\ref{AR}) peut \^etre \'ecrite comme une convolution
\begin{equation}
\label{eq}
X \star a[n] = W[n]
\end{equation}
avec $a[0] = 1$. La fonction de transfert 
du filtre est
\[
\hat a (z) = \sum_{k=0}^N a[k] z^{-k} .
\]
On dit que le filtre r\'ecursif $a[n]$ ``blanchit'' le processus $X$.
\\
\\
{\bf Composante stationnaire}
Soit $a_i [n]$ le 
filtre inverse de $a[n]$ dont la 
fonction de transfert est
\[
\hat a_i (z) = \frac 1 {\hat a (z)} = \frac 1 
{\sum_{k=0}^N a[k] z^{-k}} .
\]
Pour tout $n \geq 0$
\[
X[n] = a_i \star W [n] + Y [n] .
\]
Le processus $a_i \star W [n]$ est la composante stationnaire
de $X[n]$
tandis que $Y[n]$ est satisfait
\[
Y \star a [n] = 0 .
\]
Le th\'eor\`eme suivant donne une condition n\'ecessaire et
suffisante pour que $Y[n]$ soit une composante transitoire qui
converge vers z\'ero et donc que $X[n]$
soit asymptotiquement stationnaire.

\begin{theorem}
Le processus $X[n]$ converge vers $W \star a_i [n]$
si et seulement si les z\'eros de
$\hat a (z)$ sont strictement inclus dans le cercle unit\'e
$|z| < 1$.
\end{theorem}

\noindent{\bf D\'emonstration}
Pour $n \geq 0$, 
$Y[n]$ est la solution homog\`ene de l'\'equation (\ref{eq})
\begin{equation}
\label{homogene}
Y \star a [n] = \sum_{k=0}^N a[k] Y[n-k] = 0 .
\end{equation}
La solution $Y[n]$ d'un tel syst\`eme d'\'equation peut s'\'ecrire
\[
Y[n] = \sum_{k=0}^{N-1} A_k (c_k)^n ~,
\]
o\`u les variables $(A_k)_{0 \leq k < N}$ 
sont des combinaisons lin\'eaires des
valeurs initiales $X[k]$ pour $0 \geq k > -N$,
tandis que $(c_k)_{0 \leq k < N}$ sont 
les racines de 
l'\'equation caract\'eristique 
\begin{equation}
\label{caracteristique}
\hat a (z) = \sum_{k=0}^N a[k] z^{-k} = 0 .
\end{equation}

Pour que le processus 
converge vers sa composante stationnaire $W \star a_i [n]$
il faut que
\[
\lim_{n \rightarrow + \infty} Y[n] = 0
\]
ce qui veut dire que 
pour tout $0 \leq k < N$, $|c_k| < 1$. $\Box$\\
\\
Le th\'eor\`eme \ref{cov_conv_th} montre
que la puissance spectrale de la composante stationnaire
$a_i \star W [n]$ de $X[n]$ est 
\[
\hat R_X (\eom) = \hat R_W(\eom) |\hat a_i (\eom)|^2 = 
\frac {\sigma^2} {|\sum_{k=0}^N a[k] e^{-ik\om}|^2} ,
\]
car $W[n]$ est un bruit blanc et a donc une puissance spectrale
constante
\[
R_W (\eom) = E\{W[n]^2\} = \sigma^2 .
\]
Dans la suite nous negligeons la composante transitoire de
$X[n]$ et nous consid\'erons ce processus comme stationnaire.

\subsection{Autocovariance des processus autor\'egressifs}

La fonction
d'autocovariance d'un processus AR est reli\'ee aux param\`etres
$(a[k] )_{1 \leq k \leq N}$ par un syst\`eme d'\'equations
lin\'eaires. Cela permet d'identifier ces param\`etres en
r\'esolvant un syst\`eme lin\'eaires.

L'autocovariance se calcule 
directement \`a partir de l'\'equation aux diff\'erences
\begin{equation}
\label{eq-rec}
X[n] + \sum_{k=1}^N a[k] X[n-k] = W[n] .
\end{equation}
En multipliant par $X[n-l]$ de chaque c\^ot\'e de (\ref{eq-rec})
et en calculant l'esp\'erance on obtient
\[
E\{X[n] X[n-l]\} + 
E\left\{\sum_{k=1}^N a[k] X[n-k] X[n-l]\right\} = 
E\{W[n] X[n-l]\} . 
\] 
Comme $X[n-k]$ est une combinaison lin\'eaire de 
$\{ W[l] \}_{l \leq n-k}$ et que $W$ est un
bruit blanc, pour $l > 0$ 
\[
E\{W [n] X[n-l]\} = 0
\]
et donc
\begin{equation}
\label{cov-rec}
R_X[l] + \sum_{k=1}^N a[k] R_X[l-k] = 0 .
\end{equation}

Si l'on r\'e\'ecrit (\ref{cov-rec}) pour $ 1 \leq l \leq N$, on
obtient le syst\`eme de $N$ \'equations 
de Yule-Walker qui s'\'ecrit sous forme matricielle
\begin{equation}
\label{yule-walker}
\left( \begin{array}{cccc}
R_X[0] &R_X[1] & ...&R_X[N-1] \\
R_X[-1]&R_X[0]& ...&R_X[N-2]\\
. &. &...&.  \\
.&.&...&.\\
.&.&...&.\\
R_X[-N+1]&R_X[-N+2]&...&R_X[0]\\
\end{array}
\right)
\left( \begin{array}{c}
a[1]\\
a[2]\\
.\\
.\\
.\\
a[N]\\
\end{array}
\right)
= -
\left( \begin{array}{c}
R_X[1]\\
R_X[2]\\
.\\
.\\
.\\
R_X[N]\\
\end{array}
\right) .
\end{equation}
Si cette matrice sym\'etrique $\bf R_X$ n'est pas singuli\`ere 
alors
le vecteur de coefficients $(a[k])_{1 \leq k \leq N}$ 
s'obtient \`a partir de $R_X [n]$
en inversant la matrice $\bf R_X$.
Etant donn\'e une fonction d'autocovariance $R[l]$ quelconque,
les \'equations de Yule-Walker permettent de calculer les coefficients
$a[k]$ du processus AR d'ordre $N$ tel que $R_X [l] = R[l]$ pour 
$-N \leq l \leq N$. 

La fonction d'autocorr\'elation satisfait une \'equation
r\'ecurrente (\ref{cov-rec})
identique \`a (\ref{homogene}). La solution peut 
donc aussi s'\'ecrire
\[
R_X[l] = \sum_{k=0}^{N-1} r_k (c_k)^ l ,
\]
o\`u les $c_k$ sont les z\'eros de $\hat a (z)$.
Comme on a impos\'e que $|c_k| < 1$ pour tout $0 \leq k < N$,
$\lim _{l \rightarrow + \infty} R_X [l] = 0$.

La variance du bruit blanc $W [n]$ 
se calcule \`a partir de (\ref{eq-rec}) 
en observant que comme $W[n]$ est ind\'ependant de $X[n-l]$ 
lorsque $l > 0$ et donc
\begin{equation}
\label{variance-bruit}
\sigma^2 = E\{ W [n] ^ 2 \} = 
E\{ W [n] X[n] \} = 
R_X[0] + \sum_{k=1}^N a[k] R_X[k] .
\end{equation}
Comme les coefficients $a[k]$ ne d\'ependent que de $R_X[l]$,
la variance du bruit blanc est aussi enti\`erement d\'efinie
par $R_X [l]$.
Si l'on sp\'ecifie les param\`etres $a[k]$ du processus $AR$,
(\ref{variance-bruit}) avec les $N$ equations de Yule-Walker
d\'efinit un syst\`eme de taille $N+1$ qui permet de calculer la
covariance $R_X [n]$ du processus.
\\
\\
{\bf Applications}
Nous verrons dans le chapitre \ref{parole-chap} que
les processus AR sont utilis\'es pour mod\'eliser 
des sons non vois\'es tels que le
``f'' dans ``facile'' ou ``ch'' dans ``charme''.
Ces sons 
sont g\'en\'er\'es par un flot d'air turbulent 
stationnaire, que l'on 
approxime par un bruit blanc gaussien, 
filtr\'e par le conduit vocal que l'on d\'eforme pour produire
le son d\'esir\'e. Pour des applications de codage efficace, on
estime l'autocovariance d'un son non-vois\'e et l'on 
approxime ce son par un processus gaussien AR dont
les coefficients de r\'egression $(a[k])_{1 \leq k \leq N}$
sont donn\'es par la solution du syst\`eme
d'\'equations de Yule Walker. Un tel son enregistr\'e
sur plusieurs centaines d'\'echantillons est alors cod\'e par 
seulement $N$ coefficients, o\`u $N$ est de l'ordre de 10.












