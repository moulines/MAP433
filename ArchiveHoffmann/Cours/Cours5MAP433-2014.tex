
\documentclass{beamer}
\usetheme[right,hideothersubsections]{Berkeley}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext}
\usepackage{amsfonts}

%\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[applemac]{inputenc}


\newenvironment{disarray}{\everymath{\displaystyle\everymath{}}\array} {\endarray}
\newtheorem{theo}{Théorème}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{cor}{Corollary}[theo]
\newtheorem{lm}{Lemma}
\newtheorem{nota}{Notation}
\newtheorem{rk}{Remark}
\newtheorem{remarque}{Remarque}
\newtheorem{exa}{Example}
\newtheorem{df}{Définition}
\newtheorem{hypothese}{Hypothèse}
\newtheorem{terminologie}{Terminologie}
\newenvironment{dem}{\textbf{Proof}}{\flushright$\blacksquare$\\}

\DeclareMathOperator{\E}{{\mathbb E}}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\IF}{{\mathbb I}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Pour le modèle linéaire

\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bx}{\boldsymbol{x}}
\DeclareMathOperator{\vp}{\boldsymbol{p}}
\DeclareMathOperator{\vq}{\boldsymbol{q}}
\DeclareMathOperator{\estMC}{\widehat \vartheta_n^{\,\,{\tt mc}}}
\DeclareMathOperator{\estMCNL}{\widehat \vartheta_n^{\,\,{\tt mcnl}}}
\DeclareMathOperator{\estMV}{\widehat \vartheta_n^{\,\,{\tt mv}}}
\DeclareMathOperator{\design}{\mathbb{M}}
\DeclareMathOperator{\est}{\widehat \vartheta_{\mathnormal{n}}}
\DeclareMathOperator{\var}{\mathrm{Var}}
\DeclareMathOperator{\estMVc}{\widehat \vartheta_{n,0}^{\,{\tt mv}}}
\DeclareMathOperator{\Xbar}{\overline{\mathnormal{X}}_\mathnormal{n}}

\def\ind{\mathrel{\bot\mskip -8,5mu\bot}}


\title{MAP 433 : Introduction aux méthodes statistiques. Cours 5}
%\author{M. Hoffmann}
%\institute{Université Paris-Est and ETG}
\begin{document}
\date{7 mars 2014}
\maketitle



\begin{frame}
\frametitle{Aujourd'hui}
\tableofcontents
\end{frame}


\section{Méthode d'estimation dans le modèle de régression}

\subsection{Modèle de régression, notion de \og design\fg{}}

\begin{frame}
\frametitle{Influence d'une variable sur une autre}
\begin{itemize}
\item \underline{Principe} : on part de {\color{red}l'observation} d'un $n$-échantillon
$$Y_1,\ldots, Y_n\;\;\;(Y_i \in \R)$$
\item A chaque observation $Y_i$ est associée une {\color{red} observation auxiliaire} $\bX_i {\color{red} \in \R^k}$.
\item On {\color{red}suspecte} l'échantillon
$$\bX_1,\ldots, \bX_n\;\;\;{\color{red} (\bX_i \in \R^k)}$$
de contenir la \og majeure partie de la variabilité des $Y_i$ \fg{}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modélisation de l'influence}
\begin{itemize}
\item Si $\bX_i$ contient {\color{red}toute la variabilité} de $Y_i$, alors $Y_i$ est mesurable par rapport à $\bX_i$ : il existe $r:\R^k\rightarrow \R$ telle que
$$Y_i = r\big(\bX_i\big),$$
mais peu réaliste (ou alors {\color{red}problème d'interpolation
numérique}).
\item \underline{Alternative} : représentation précédente avec {\color{red} erreur additive} : on {\color{red} postule}
$$Y_i = r\big(\bX_i\big)+\xi_i,$$
$\xi_i$ erreur al\'eatoire centrée (pour des raisons
d'identifiabilité).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Motivation: meilleure approximation $L^2$}
\begin{itemize}
\item \underline{Meilleure approximation $L^2$}. Si
$\E\big[Y^2\big]<+\infty$, la meilleure approximation de $Y$ par une
variable aléatoire $\bX$-mesurable est donnée par
{\color{red}l'espérance conditionnelle} $\E\big[Y|\bX\big]$ :
$$\E\big[\big(Y-r(\bX)\big)^2\big] = \min_h \E\big[\big(Y-h(\bX)\big)^2\big]$$
\item o\`u
$$r(\bx) = \E\big[Y|\bX=\bx\big],\;\;\bx \in \R^k.$$
\item On appelle $r(\cdot)$ {\bf fonction de r\'egression de $Y$ sur
$\bX$}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Régression}
\begin{itemize}
\item On d\'efinit:
$$\xi = Y-\E\big[Y|\bX \big]\;\;\Longrightarrow\; \E\big[\xi\big]=0.$$
\item On a alors naturellement la repr\'esentation d\'esir\'ee
$$Y=r(\bX)+\xi, \quad \E\big[\xi\big]=0$$
si l'on pose
$$\boxed{r(\bx) = \E\big[Y|\bX=\bx\big],\;\;\bx \in \R^k}$$

\item On observe alors un $n$-échantillon
$$(\bX_1,Y_1),\ldots, (\bX_n,Y_n)$$
où
$$Y_i = r(\bX_i)+\xi_i,\;\;\E\big[\xi_i\big]=0$$
avec comme {\color{red}paramètre la fonction $r(\cdot)$}+ un {\color{red}jeu d'hypothèses} sur la loi des $\xi_i$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle de régression à design aléatoire}
\begin{df}
Modèle de régression {\bf à design aléatoire} = donnée de
l'observation
$$(\bX_1,Y_1),\ldots, (\bX_n,Y_n)$$
avec $(Y_i,\bX_i)\in \R\times \R^k$ {\color{red}i.i.d.},
et\\\vspace{3mm} \centerline{$Y_i =
r({\color{red}\vartheta},\bX_i)+\xi_i,\;\;
\E\big[\xi_i|\bX_i\big]=0,\;\;{\color{red}
\vartheta \in \Theta \subset \R^d}.$}
\begin{itemize}
\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
$\vartheta$ près.
\item $\bX_i$ = variables explicatives, co-variables, pr\'edicteurs;
$(\bX_1,\ldots,\bX_n)$ = {\bf design}.
\end{itemize}
\end{df}
\end{frame}

%\begin{frame}
%\frametitle{Remarques}
%\begin{itemize}
%\item Si $(X_i,Y_i )\in \R \times \R$ {\color{red}estimer} $\vartheta$ revient à {\color{red} rechercher} la fonction dans la famille $\{r(\vartheta,\cdot),\vartheta \in \Theta\}$ qui approche le mieux les points $(X_i,Y_i)$ pour un certain {\color{red}critère à définir}\vspace{2mm}.
%(Pb d'approximation, {\color{red}pas} d'interpolation).
%\item Si $Y_i \in \{0,1\}$ modèle à {\color{red} réponse binaire}.
%
%\underline{Exemple} : $Y_i$ = présence/absence d'une maladie chez l'individu $i$, et $\bX_i$ = vecteur de marqueurs biologiques.
%\item Dans cette acceptation du modèle, {\color{red} le statisticien ne choisit pas la valeur des covariables}.
%\end{itemize}
%\end{frame}

\subsection{R\'egression à design déterministe}


\begin{frame}
\frametitle{Modèle alternatif : signal+bruit}
\begin{itemize}
\item \underline{Principe} : {\color{red}sur un exemple}. On observe
$$Y_i = r(\vartheta, i/n)+\xi_i,\;\;i=1,\ldots,n$$
où $r({\color{red}\vartheta},\cdot):[0,1]\rightarrow \R$ est une
fonction connue au paramètre ${\color{red}\vartheta \in \Theta
\subset \R^d}$ près, et les $\xi_i$ sont i.i.d.,
$\E\big[\xi_i\big]=0$.
\item {\color{red} But} : reconstruire $r({\color{red}\vartheta},\cdot)$ c'est-à-dire {\color{red} estimer $\vartheta$}.
\item Plus généralement, on observe
$$Y_i = r(\vartheta, \bx_i)+\xi_i,\;i=1,\ldots, n$$
où $\bx_1,\ldots, \bx_n$ sont des points de $\R^k$ {\color{red}
d\'eterministes}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle de régression à design déterministe}
\begin{df}
Modèle de régression {\bf à design déterministe} = donnée de
l'observation
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
avec $Y_i \in \R, \bx_i\in \R^k$, et \\\vspace{3mm} \centerline{$Y_i
=
r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;\E\big[\xi_i\big]=0,\;\;{\color{red}
\vartheta \in \Theta \subset \R^d}.$}
\begin{itemize}
%\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
%$\vartheta$ près.
\item $\bx_i$ déterministes, donnés (ou choisis) : plan d'expérience, points du \og design\fg{}.
\item Hypothèses sur les $\xi_i$ : à débattre. {\color{red}Pour simplifier}, les $\xi_i$ sont i.i.d. {\color{red} (hypothèse restrictive)}.
\item {\color{red} Attention !} Les $Y_i$ ne sont {\color{red}pas identiquement distribuées}.
\end{itemize}
\end{df}

 \underline{Question}: Comment estimer $\vartheta$ dans ce mod\`ele?

\end{frame}

\begin{frame}
\frametitle{Régression gaussienne}


\begin{itemize}
\item  Modèle de régression à design déterministe :
$$Y_i =
r({\vartheta},\bx_i)+\xi_i,\;\;\vartheta \in \Theta\subset  \R^d.$$
\item  Supposons: $\xi_i \sim {\mathcal N}(0,\sigma^2)$, i.i.d.
\item On a alors le mod\`ele de {\color{red}régression gaussienne}.
Comment estimer $\vartheta$?  {\color{red}On sait expliciter la loi
de l'observation} $Z=(Y_1,\dots,Y_n)$ $\Longrightarrow$ appliquer le
principe du maximum de vraisemblance.

\item La loi de $Y_i$:
\begin{align*}
\PP^{Y_i}(dy) & = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\big
(-\frac{1}{2\sigma^2}(y-r({\vartheta},\bx_i))^2\big)dy \\
& \ll dy.
\end{align*}

%d'où
%$$\PP^{(Y_1,\ldots, Y_n)}(dy_1\ldots dy_n) = \Big(\prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}\exp\big(-\frac{1}{2\sigma^2}(y-\vartheta^T\bx_i)\big)\Big) dy_1\ldots dy_n$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{EMV pour régression gaussienne}
\begin{itemize}
\item  Le modèle $\{\PP_\vartheta^n {\color{red} = \text{loi de }\;(Y_1,\ldots, Y_n)},\vartheta \in \R^k\}$ est {\color{red}dominé} par
$\mu^n(dy_1\ldots dy_n) = dy_1\ldots dy_n.$
\item D'où
\begin{align*}
 \frac{d\PP_\vartheta^n}{d\mu^n}(y_1,\ldots, y_n)
  =\; &\prod_{i=1}^n \tfrac{1}{\sqrt{2\pi \sigma^2}}\exp
  \big(-\tfrac{1}{2\sigma^2}(y_i-r({\vartheta},\bx_i))^2\big) \\
\;= & \tfrac{1}{(\sqrt{2\pi \sigma^2})^{n}}
\exp\big(-\tfrac{1}{2\sigma^2}\sum_{i =
1}^n\big(y_i-r({\vartheta},\bx_i)\big)^2\big).
\end{align*}
\item La fonction de vraisemblance
$$\boxed{{\mathcal L}_n(\vartheta, Y_1,\ldots, Y_n)
\propto \exp\Big(-\frac{1}{2\sigma^2}\sum_{i = 1}^n\big(Y_i-
r({\vartheta},\bx_i)\big)^2\Big)}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimateur des moindres carrés} Maximiser la
{\color{red} vraisemblance} en r\'egression gaussienne = minimiser
la somme des carr\'es: $$ \sum_{i = 1}^n
\big(Y_i-r({\vartheta},\bx_i)\big)^2 \to \min_{\vartheta \in\Theta}.
$$
\begin{df}
Estimateur des {\color{red}moindres carrés} : tout estimateur
$\estMC$ t.q. \centerline{$\estMC \in \arg \min_{\vartheta \in
\Theta}\sum_{i = 1}^n \big(Y_i-r({\vartheta},\bx_i)\big)^2.$}
\end{df}
\begin{itemize}
\item  L'EMC est un M-estimateur. Pour le
mod\`ele de r\'egression gaussienne: $\boxed{\rm EMV = EMC}$.
\item {\color{red} Existence, unicit\'e.}
\item Propri\'et\'es remarquables si la r\'egression est lin\'eaire:
$r({\vartheta},\bx_i) = \vartheta^T\bx_i$.
\end{itemize}
\end{frame}

\subsection{La droite des moindres carrés}

\begin{frame}
\frametitle{Droite de régression}
\begin{itemize}
\item \underline{Modèle le plus simple}
$\boxed{r(\vartheta,x)=a +bx}$
$$\boxed{Y_i = a\,+ b\, x_i+\xi_i,\;\;i=1,\ldots,n}$$
avec ${\color{red}\vartheta = (a,b)^T \in \Theta = \R^2}$ et les
$(x_1,\ldots, x_n)$ donnés.
%\item Erreurs centr\'ees: $\E\big[\xi_i\big]=0$, de variances $\E\big[\xi_i^2\big]$ finies.
\item L'estimateur des moindres carrés:
$${\color{red}\estMC}=(\hat a, \hat b) =
\arg \min_{(a,b)\in \R^2}\sum_{i = 1}^n \big(Y_i-a - b x_i\big)^2.$$
%\item \underline{Résidu} : si $\est$ est un estimateur de $\vartheta$
%$$\widehat \xi_i = Y_i - r(\est, x_i)\;\;\text{{\color{red}résidu} au point}\;i.$$
%\item \underline{RSS} : (Residual Sum of Squares)
%$$\|\widehat \xi\|^2 =\|\widehat \xi(\est)\|^2 = \sum_{i = 1}^n\big(Y_i - r(\est,x_i)\big)^2.$$
\item {\color{red}Solution explicite} existe toujours, sauf cas pathologique quand tous les $x_i$ sont les
m\^emes (Poly, page 112).
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Estimateur des moindres carrés}
%\begin{df}
%\end{df}
%\begin{itemize}
%\item Solution {\color{red}explicite}
%$$\boxed{\estMC=(\widehat \vartheta_{n,0}^{\,{\tt mc}},\widehat \vartheta_{n,1}^{\,{\tt mc}})=\Big(\overline{Y}_n-\widehat \vartheta_{n,1}^{\,{\tt mc}}\overline{x}_n, \frac{\langle x_\cdot-\overline{x}_n, Y_\cdot -\overline{Y}_n\rangle_n}{\|x_\cdot -\overline{x}_n\|_n^2}\Big)}$$
%$\langle u_\cdot,v\cdot\rangle_n = \sum_{i = 1}^n u_iv_i$. (on
%retrouvera directement ces formules...)
%\end{itemize}
%\end{frame}


\frame{ \frametitle{R\'egression lin\'eaire simple}
  \begin{figure}[h]
\begin{center}
\includegraphics[height=\textheight]{x-y1.eps}
\end{center}
\end{figure}
}

\frame{

\frametitle{R\'egression lin\'eaire simple}
  \begin{figure}[h]
\begin{center}
\includegraphics[height=\textheight]{x-y2.eps}
\end{center}
\end{figure}
}

\subsection{R\'egression lin\'eaire multiple}

\begin{frame}
\frametitle{R\'egression lin\'eaire multiple (=Modèle linéaire)}
\begin{itemize}
\item La fonction de r\'egression est $r(\vartheta,\bx_i) = \vartheta^T\bx_i$.
On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
avec
$$\boxed{Y_i = \vartheta^T\bx_i+\xi_i,\;\;i=1,\ldots, n}$$
où $\vartheta \in \Theta = \R^k,\;\;\bx_i \in \R^k$.
\item {\color{red}Matriciellement}
$$\boxed{\boldsymbol{Y} = \mathbb{M}\vartheta + \boldsymbol{\xi}}$$
avec $\boldsymbol{Y} = (Y_1 \cdots Y_n)^T$, $\boldsymbol{\xi} =
(\xi_1 \cdots \xi_n)^T$ et $\mathbb{M}$ la matrice $(n\times k)$
dont les {\color{red} lignes} sont les $\bx_i$.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Réduction \og design\fg{} aléatoire $\longrightarrow$ déterministe}
%\begin{itemize}
%\item Les modèles de régression à \og design\fg{} déterministe ou aléatoire se traitent  {\color{red} essentiellement de la même manière} :
%\end{itemize}
%\begin{hypothese}[Ancillarité des covariables]
%On suppose que la loi $\PP^{\bX}$ des $\bX_i$ ne dépend pas du paramètre inconnu ${\color{red}\vartheta}$.
%\end{hypothese}
%\begin{itemize}
%\item Sous l'hypothèse d'ancillarité, le caractère aléatoire des $\bX_i$ -- observés -- ne joue aucun rôle : on peut faire l'étude mathématique du modèle {\color{red}conditionnellement aux $\bX_i$}.
%\item {\color{red} Désormais} : on se place  dans le modèle de régression à \og design \fg{} {\color{red}déterministe}.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{\og design\fg{} aléatoire vs. déterministe}
%\begin{remarque}
%{\it A posteriori} pourquoi considérer le modèle de régression à \og design \fg{} aléatoire et ne pas se placer d'emblée en signal + bruit ?
%\begin{itemize}
%\item {\color{red}Le design aléatoire} fournit une interprétation en terme de fonction de régression obtenue via l'espérance conditionnelle.
%\item {\color{red}Essentiel} pour le traitement des {\color{red}modèles à réponse binaire} (ou multiple), voir plus loin.
%\end{itemize}
%\end{remarque}
%\end{frame}





%\begin{frame}
%\frametitle{Estimation de $\sigma^2$}
%\begin{itemize}
%\item {\color{red}Estimation de $\sigma$} (ou $\sigma^2$) à partir des observations
%$$\boxed{Y_i = \vartheta_0\,+\vartheta_1\,x_i+{\color{red}\sigma}\, \varepsilon_i,\;\;i=1,\ldots,n}$$
%{\color{red}avec}
%$$\boxed{\E_\vartheta\big[\varepsilon_i\big]=0,\;\E_\vartheta\big[\varepsilon_i^2\big]=1}$$
%\item \underline{Estimateur naturel} de $\sigma^2$ :
%$$\widehat \sigma_n^2 = \frac{1}{n}\sum_{i = 1}^n\big(Y_i-r(\estMC, x_i)\big)^2$$
%\item Somme de variables aléatoires {\color{red}non indépendantes}.
%\item Difficile de progresser {\color{red}sans hypothèse supplémentaire}. Si les $\varepsilon_i$ sont i.i.d. ${\mathcal N}(0,1)$, alors {\color{red}on sait} \og résoudre \fg{} le problème... plus loin.
%\end{itemize}
%\end{frame}





%\subsection{EMV et EMC}

\begin{frame}
\frametitle{EMC en régression linéaire multiple}
\begin{itemize}
\item Estimateur des {\color{red}moindres carrés} en régression
linéaire multiple : tout estimateur $\estMC$ satisfaisant
$$\sum_{i = 1}^n
\big(Y_i-(\estMC)^T\bx_i\big)^2 = \min_{\vartheta \in \R^k}\sum_{i =
1}^n \big(Y_i-{\vartheta}^T\bx_i\big)^2.$$
\item En notation matricielle :
\begin{eqnarray*} \|\boldsymbol{Y}-\design\estMC\|^2 &=& \min_{\vartheta \in
\R^k}\|\boldsymbol{Y}-\design\vartheta\|^2\\
&=& \min_{v \in V}\|\boldsymbol{Y}-v\|^2
\end{eqnarray*}
o\`u $V=\text{Im}(\design) = \{v\in \R^n: v=\design\vartheta, \
\vartheta\in \R^k\}$.
 Projection orthogonale sur $V$.
 \end{itemize}
 \end{frame}

% \subsection{G\'eometrie de l'EMC}

 \begin{frame}
\frametitle{G\'eom\'etrie de l'EMC}
 \begin{itemize}
 \item L'EMC vérifie
$$\boxed{\design {\estMC} = P_V \boldsymbol{Y}}$$
o\`u $P_V$ est le projecteur orthogonal sur $V$.
\item Mais $\design^T  P_V= \design^T  P_V^T = ( P_V\design)^T =
\design^T$. On en d\'eduit {\color{red}les \'equations normales des
moindres carr\'es}:
$$\boxed{\design^T\design {\estMC} =
\design^T\boldsymbol{Y}.}$$
\item \underline{Remarques.}
  \begin{itemize}
  \item L'EMC est un $Z$-estimateur.
  \item Pas d'{\color{red}unicit\'e} de $\estMC$ si la matrice
  $\design^T\design$ n'est pas inversible.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame} \frametitle{G\'eom\'etrie de l'EMC}
\begin{prop}
Si $\design^T\design$ (matrice $k \times k$) inversible, alors
$\estMC$ {\color{red}est unique} et
$$\boxed{\estMC = \big(\design^T\design\big)^{-1}\design^T \boldsymbol{Y}}$$
\end{prop}
\begin{itemize}
\item Contient le cas précédent de la droite de régression simple.
\item Résultat g\'eometrique, {\color{red}non stochastique}.
\item $\design^T\design\ge0$; \ \ $\design^T\design$
inversible $\Longleftrightarrow$ $\design^T\design>0$;
$$\design^T\design>0 \ \Longleftrightarrow \ {\rm rang}(\design)=k
\ \Longleftrightarrow \ {\rm dim}(V)=k.$$
$$\design^T\design>0 \quad \Longrightarrow \quad {\color{red} n \geq k}.$$
\end{itemize}
\end{frame}


\begin{frame} \frametitle{G\'eom\'etrie de l'EMC}
Soit $\design^T\design>0$. Alors, la matrice $n\times n$
$$
A = \design\big(\design^T\design\big)^{-1}\design^T
$$
est dite {\bf matrice chapeau} (\texttt{hat matrix}).
%
\begin{prop}
Si $\design^T\design>0$, alors $A$ est le projecteur sur
$V$:\\\vspace{2mm} \centerline{$A=P_V$} et ${\rm rang}(A)=k$.
\end{prop}
\underline{Preuve} :
\begin{itemize}
\item  $A=A^T$, $A=A^2$, donc $A$ est un
projecteur.
\item ${\rm Im}(A) = V$, donc $A=P_V$; \
${\rm rang}(P_V)={\rm dim}(V)=k$.
\end{itemize}
{\color{red}\og Chapeau \fg{}}, car $A$ g\'en\`ere la pr\'evison de
$\design\vartheta$ not\'ee $\widehat{\boldsymbol{Y}}$ :
$$\widehat{\boldsymbol{Y}}= \design\estMC= A\boldsymbol{Y}.$$
\end{frame}














\subsection{Le cas gaussien}





%\section{Régression lin\'eaire : rappels}
%
%\subsection{Régression à \og design \fg{} déterministe}
%
%\begin{frame}
%\frametitle{Modèle de régression}
%\begin{df}
%Modèle de régression (à \og design\fg{} déterministe) = donnée de l'observation
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
%avec $Y_i \in \R, \bx_i\in \R^k$, et
%$$Y_i = r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;\E\big[\xi_i\big]=0,\;\;{\color{red} \vartheta \in \Theta \subset \R^d}.$$
%\begin{itemize}
%%\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
%%$\vartheta$ près.
%\item $\bx_i$ déterministes, donnés (ou choisis) : plan d'expérience, points du \og design\fg{}.
%\item Hypothèse : les $\xi_i$ sont i.i.d. (pour simplifier). {\color{red} Attention : } les $Y_i$ ne sont {\color{red}pas} identiquement distribuées.
%\item Ecriture de la vraisemblance du modèle...
%\end{itemize}
%\end{df}
%\end{frame}
%
%
%\begin{frame}
%\frametitle{Modèle linéaire}
%\begin{itemize}
%\item On observe
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
%avec
%$$\boxed{Y_i = \vartheta^T\bx_i+\xi_i,\;\;i=1,\ldots, n}$$
%où $\vartheta \in \Theta = \R^k,\;\;\bx_i \in \R^k$.
%\item {\color{red}Matriciellement}
%$$\boxed{\boldsymbol{Y} = \mathbb{M}\vartheta + \boldsymbol{\xi}}$$
%avec $\boldsymbol{Y} = (Y_1,\ldots, Y_n)^T$, $\boldsymbol{\xi} = (\xi_1,\ldots, \xi_n)^T$ et $\mathbb{M}$ la matrice $(n\times k)$ dont les {\color{red} lignes} sont les $\bx_i$.
%\end{itemize}
%\end{frame}
%
%%%\subsection{EMV et EMC}
%
%\begin{frame}
%\frametitle{Estimateur des moindres carrés}
%\begin{df}
%{\color{red}Estimateur des moindres carrés} (EMC) : tout estimateur
%$\estMC$ satisfaisant $\estMC \in \arg \min_{\vartheta \in
%\R^k}\sum_{i = 1}^n \big(Y_i-\vartheta^T\bx_i\big)^2$.
%\end{df}
%
%\begin{itemize}
%\item
%EMC = cas particulier de $M$-estimateur et de $Z$-estimateur
%\end{itemize}
%
%\begin{prop}
%Si $\design^T\design$ (matrice $k \times k$) inversible (cond. néc. : ${\color{red} n \geq k}$), alors $\estMC$ {\color{red}est unique} et
%$$\boxed{\estMC = \big(\design^T\design\big)^{-1}\design^T \boldsymbol{Y}}$$
%\end{prop}
%\end{frame}
%



\subsection{Modèle linéaire gaussien}

\begin{frame}
\frametitle{Régression gaussienne} {\bf Régression gaussienne} : on
suppose $\boldsymbol{\xi} \sim {\mathcal
N}(0,\sigma^2\mathrm{Id}_n)$. Alors on a plusieurs prori\'et\'es
remarquables:
\begin{itemize}
\item Estimateur des moindres carrés $\estMC$ et
estimateur du maximum de vraisemblance
{\color{red}coïncident}.\\\vspace{1mm} {\it Preuve} : \'ecriture de
la fonction de vraisemblance.
\item  On sait expliciter la loi {\color{red} exacte} (non-asymptotique!)
de~$\estMC$.\\\vspace{1mm} {\it Ingrédient} : {\color{red}loi des
vecteurs gaussiens sont caractérisés par leur moyenne et matrice de
variance-covariance}.
\end{itemize}
\end{frame}

%\subsection{Propriétés statistiques de l'EMC : cas gaussien}

\begin{frame}
\frametitle{Cadre gaussien : loi des estimateurs}
\begin{itemize}
\item \underline{Hyp. 1} : $\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2\mathrm{Id}_n)$.
\item \underline{Hyp. 2} : $\design^T \design>0$.
\end{itemize}
\begin{prop}
\begin{itemize}
\item[(i)] $\estMC \sim {\mathcal N}\big(\vartheta, \sigma^2 \big(\design^T\design\big)^{-1}\big)$
\item[(ii)] $\|\boldsymbol{Y}-\design \estMC\|^2 \sim
\sigma^2\chi^2(n-k)$ {\color{red} loi du Chi 2 à $n-k$ degrés de liberté}
\item[(iii)] $\estMC$ et $\boldsymbol{Y}-\design \estMC$ sont indépendants.
\end{itemize}
\end{prop}
\begin{itemize}
\item \underline{Preuve} : {\color{red}Thm. de Cochran} (Poly, page 18). Si
$\boldsymbol{\xi}\sim {\mathcal N}(0,\mathrm{Id}_n)$ et $A_j$
matrices $n \times n$ projecteurs t.q. $A_jA_i=0$ pour $i\neq j$,
alors : $A_j\,\boldsymbol{\xi} \sim {\mathcal N}\big(0,A_j\big)$,
{\color{red}indépendants}, $\|A_j\boldsymbol{\xi}\|^2\sim
\chi^2(\mathrm{Rang}(A_j))$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de la proposition}
\begin{itemize}
\item (i)
$\estMC = \vartheta + \big(\design^T\design\big)^{-1}\design^T
\boldsymbol{\xi}$.

On vérifie: $\ \ \E[\estMC]=\vartheta$,
\begin{align*}
&\E\big[ \big(\design^T\design\big)^{-1}\design^T \boldsymbol{\xi} \big(\big(\design^T\design\big)^{-1}\design^T \boldsymbol{\xi}\big)^T\big] \\
=\; &\sigma^2\big(\design^T\design\big)^{-1}.
\end{align*}
\item (ii)
\begin{align*}
\boldsymbol{Y}-\design \estMC & = \design\big(\vartheta - \estMC\big)+\boldsymbol{\xi} \\
& = -\design\big(\design^T\design\big)^{-1}\design^T\boldsymbol{\xi}+\boldsymbol{\xi} \\
& =
\sigma(\text{Id}_n-A)\boldsymbol{\xi}',\;\boldsymbol{\xi}'\sim{\mathcal
N}(0,\mathrm{Id}_n).
\end{align*}
\item (iii) le vecteur $(\estMC,\boldsymbol{Y}-\design \estMC)$ est gaussien. On calcule explicitement sa matrice de variance-covariance.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Propriétés de l'EMC: cadre gaussien}

Estimateur de la variance $\sigma^2$:
$$\boxed{\widehat \sigma_n^2 = \frac{\|\boldsymbol{Y}-\design \estMC\|^2}{n-{\color{red}k}} = \frac{1}{n-{\color{red}k}}\sum_{i = 1}^n\big(Y_i-(\estMC)^T\bx_i\big)^2}$$
D'apr\`es la derni\`ere Proposition :
\begin{itemize}
\item $\widehat
\sigma_n^2/\sigma^2 \sim \chi^2(n-k)$ {\color{red} loi du Chi 2 à
$n-k$ degrés de liberté}
\item C'est un estimateur {\color{red}sans biais}: $$\E_\vartheta\big[\widehat
\sigma_n^2\big]=\sigma^2.$$
\item $\widehat
\sigma_n^2$ est {\color{red}ind\'ependant} de $\estMC$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propriétés de l'EMC: cadre gaussien}
\begin{itemize}
\item Lois des coordonn\'ees de $\estMC$:
$$
(\estMC)_j -\vartheta_j \sim {\mathcal N}\big(0, \sigma^2 b_j)
$$
o\`u $b_j$ est le $j$\`eme \'el\'ement diagonal de $\big(\design^T
\design\big)^{-1}$. $$ \frac{(\estMC)_j -\vartheta_j}{\widehat
\sigma_n \sqrt{b_j}} \sim t_{n-k}$$ {\color{red}loi de Student \`a
$n-k$ degr\'es de libert\'e}.
$$ t_q = \frac{\xi}{\sqrt{\eta/q}}$$
o\`u $q\ge 1$ un entier, $\xi\sim {\mathcal N}\big(0,1)$, $\eta\sim
\chi^2(q)$ et\\ $\xi$ {\color{red}ind\'ependant} de $\eta$.
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Exemple de donn\'ees de r\'egression}
\begin{center}
\vspace{-2.5cm}
\includegraphics[height=2\textheight]{cours4_data1.pdf}\hspace{4cm}
\end{center}
\end{frame}

\begin{frame}
\frametitle{R\'esultats de traitement statistique initial}
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline
(Intercept) &$152.133$&$2.576$&$59.061$&$< 2e-16***$\\
age&$-10.012$&$59.749$&$ -0.168$&$0.867000$\\\hline
sex &$-239.819$&$61.222$&$-3.917$&$0.000104***$\\
bmi&$519.840$&$66.534$&$7.813$&$4.30e-14***$\\\hline
map&$324.390$&$65.422$&$4.958$&$1.02e-06***$\\
tc&$-792.184$&$416.684$&$-1.901$&$0.057947$\\\hline
ldl&$476.746$&$339.035$&$1.406$&$0.160389$\\
hdl&$101.045$&$212.533 $&$0.475$&$0.634721$\\\hline
tch&$177.064$&$161.476$&$ 1.097$&$0.273456$\\
ltg&$751.279$&$ 171.902$&$4.370$&$ 1.56e-05***$\\\hline
glu&$67.625$&$ 65.984$&$1.025$&$0.305998$\\\hline
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{Questions statistiques}
\begin{itemize}
\item {\bf S\'election de variables.} Lesquelles parmi les 10
variables:\\\vspace{3mm}
\centerline{\texttt{age,sex,bmi,map,tc,ldl,hdl,tch,ltg,glu}}\vspace{3mm}
sont significatives? Formalisation math\'ematique: trouver (estimer)
l'ensemble $N= \{j: \vartheta_{j}\ne 0\}$.
\item {\bf Pr\'evison.} Un nouveau patient arrive avec son vecteur
des 10 variables ${\bf x}_0\in \R^{10}$. Donner la pr\'evison de la
r\'eponse $Y$ =\'etat du patient dans 1 an.
\end{itemize}
\end{frame}

\section{S\'election de variables}

\begin{frame}
\frametitle{RSS (Residual Sum of Squares)} Mod\`ele de
r\'egression\vspace{2mm} \centerline{$ Y_i= r(\vartheta, {\bf
x}_i)+\xi_i, \quad i=1,\dots,n.$}
\begin{itemize}
\item {\bf Résidu:} si $\est$ est un estimateur de
$\vartheta$,
$$\widehat \xi_i = Y_i - r(\est, {\bf x}_i)
\;\;\text{{\color{red}résidu} au point}\;i.$$
\item {\bf RSS:} {\color{red} Residual Sum of Squares}, somme
r\'esiduelle des carr\'es. Caract\'erise la qualit\'e
d'approximation.
$${\rm RSS}(={\rm RSS}_{\est})=\|\widehat \xi\|^2
= \sum_{i = 1}^n\big(Y_i - r(\est,{\bf x}_i)\big)^2.$$
\item En r\'egression {\color{red}lin\'eaire}:
$\boxed{{\rm RSS}= \|{\bf Y}-\design\est\|^2.}$
\end{itemize}
\end{frame}

\subsection{Backward Stepwise Regression}

\begin{frame}
\frametitle{S\'election de variables : Backward Stepwise Regression}
\begin{itemize}
\item On se donne un crit\`ere d'\'elimination de variables
{\color{red}(plusieurs choix de crit\`ere possibles...)}.
\item On \'elimine une
variable, la moins significative du point de vue du crit\`ere
choisi.
\item On calcule l'EMC $\widehat\vartheta_{n,k-1}^{\rm mc}$ dans le nouveau mod\`ele, avec seulement
les $k-1$ param\'etres restants, ainsi que le RSS:\vspace{1mm}
\centerline{${\rm RSS}_{k-1}=\|{\bf
Y}-\design\widehat\vartheta_{n,k-1}^{\rm mc}\|^2$.}\vspace{1mm}
\item On continue \`a \'eliminer des variables, une par une,
jusqu'\`a la {\color{red}stabilisation de RSS}: ${\rm
RSS}_{m}\approx {\rm RSS}_{m-1}$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\begin{itemize}
\item {\bf S\'election "na\"{\i}ve"} : \
\{\texttt{sex,bmi,map,ltg}\}
\item {\bf S\'election par Backward Regression}:\\
 {\color{red}Crit\`ere
d'\'elimination: plus grande valeur de} Pr($>|t|$).
\end{itemize}
%\vspace{2mm}


{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline
(Intercept) &$152.133$&$2.576$&$59.061$&$< 2e-16***$\\
{\color{red}age}&$-10.012$&$59.749$&$
-0.168$&${\color{red}0.867000}$\\\hline
sex &$-239.819$&$61.222$&$-3.917$&$0.000104***$\\
bmi&$519.840$&$66.534$&$7.813$&$4.30e-14***$\\\hline
map&$324.390$&$65.422$&$4.958$&$1.02e-06***$\\
tc&$-792.184$&$416.684$&$-1.901$&$0.057947$\\\hline
ldl&$476.746$&$339.035$&$1.406$&$0.160389$\\
hdl&$101.045$&$212.533 $&$0.475$&$0.634721$\\\hline
tch&$177.064$&$161.476$&$ 1.097$&$0.273456$\\
ltg&$751.279$&$ 171.902$&$4.370$&$ 1.56e-05***$\\\hline
glu&$67.625$&$ 65.984$&$1.025$&$0.305998$\\\hline
\end{tabular}
}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\centerline{\bf Backward Regression: It\'eration 2.}

%\vspace{3mm}

\begin{center}

{\color{red}Crit\`ere d'\'elimination: plus grande valeur de}
Pr($>|t|$).

\vspace{4mm}

{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline (Intercept)
&$152.133$&$2.573$&$59.128$&$< 2e-16$
\\\hline
sex &$-240.835$&$60.853$&$-3.958$&$0.000104$\\
bmi&$519.905$&$64.156$&$5.024$&$8.85e-05$\\\hline
map&$322.306$&$65.422$&$4.958$&$7.43e-07$\\
tc&$-790.896$&$416.144$&$-1.901$&$0.058$\\\hline
ldl&$474.377$&$338.358$&$1.402$&$0.162$\\
{\color{red} hdl}&$99.718$&$212.146 $&$0.470$&${\color{red}
0.639}$\\\hline
tch&$177.458$&$161.277$&$ 1.100$&$0.272$\\
ltg&$749.506$&$ 171.383$&$4.373$&$ 1.54e-05$\\\hline glu&$67.170$&$
65.336$&$1.013$&$0.312$\\\hline
\end{tabular}
}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\centerline{\bf Backward Regression: It\'eration 5 (derni\`ere).}

%\vspace{3mm}

\begin{center}

Variables s\'electionn\'ees:\\\vspace{2mm}
\{\texttt{sex,bmi,map,{\color{red}tc,ldl},ltg}\}


\vspace{4mm}

{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline (Intercept)
&$152.133$&$2.572$&$59.159$&$< 2e-16$
\\\hline
sex &$-226.511$&$59.857$&$-3.784$&$0.000176$\\
bmi&$529.873$&$65.620$&$8.075$&$6.69e-15$\\\hline
map&$327.220$&$62.693$&$5.219$&$2.79e-07$\\
tc&$-757.938$&$160.435$&$-4.724$&$3.12e-06$\\\hline
ldl&$538.586$&$146.738$&$3.670$&$0.000272$\\
ltg&$804.192$&$80.173$&$10.031$&$< 2e-16$\\\hline
\end{tabular}
}
\end{center}
\end{frame}

\begin{frame}
\frametitle{S\'election de variables : Backward Regression}

Discussion de \texttt{Backward Regression}:

\begin{itemize}
\item M\'ethode de s\'election purement empirique, pas de justification
th\'eorique.
\item Application d'autres crit\`eres d'\'elimination en
\texttt{Backward Regression} peut amener aux r\'esultats diff\'erents.\\
\underline{Exemple.} {\color{red}Crit\`ere $C_p$} de Mallows--Akaike
: on \'elimine la variable $j$ qui r\'ealise
$$
\min_j \Big({\rm RSS}_{m, (-j)} + 2\widehat\sigma^2_n m\Big).
$$
\end{itemize}
\end{frame}

\subsection{LASSO}

\begin{frame}
\frametitle{S\'election de variables : LASSO}

LASSO = Least Absolute Shrinkage and Selection Operator

\begin{itemize}
\item {\color{red}Estimateur LASSO}: tout estimateur $\widehat\vartheta^{L}_n$
v\'erifiant
$$\widehat\vartheta^{L}_n \in \arg \min_{\vartheta \in \R^k}\left(\sum_{i = 1}^n
\big(Y_i-\vartheta^T\bx_i\big)^2 + \lambda \sum_{j =
1}^k|\vartheta_j|\right) \ \ \text{avec} \ \lambda>0.
$$
\item Si $\design^T\design>0$, l'estimateur LASSO $\widehat\vartheta^{L}_n$ est unique.
\item Estimateur des moindres carr\'es {\color{red}p\'enalis\'e}.
P\'enalisation par $\sum_{j = 1}^k|\vartheta_j|$, la norme $\ell_1$
de $\vartheta$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{S\'election de variables : LASSO}
\begin{itemize}
\item Deux utilisations de
LASSO:
\begin{itemize}
\item {\color{red}Estimation de $\vartheta$}: alternative \`a
$\estMC$ si $k>n$.
\item {\color{red}S\'election de variables}: on ne retient que les
variables qui correspondent aux coordonn\'ees non-nulles du vecteur
$\widehat\vartheta^{L}_n$.
\end{itemize}
\item LASSO admet une {\color{red}justification th\'eorique}: sous certaines hypoth\`eses sur la
matrice $\design$,
$$
\lim_{n\to\infty} \PP\{ \widehat N_n = N \} =1,
$$
o\`u $N= \{j: \vartheta_{j}\ne 0\}$ et $\widehat N_n= \{j:
\widehat\vartheta^{L}_{n,j}\ne 0\}$.
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Application de LASSO: "regularization path"}
\begin{center}
\vspace{-1cm}
\includegraphics[height=1.35\textheight,angle=-90]{cours4_sacha_graphe.eps}\hspace{3cm}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : LASSO}

Application aux donn\'ees de diab\`ete.

\begin{itemize}
\item  L'ensemble de variables s\'electionn\'e par LASSO:
$$
\{\texttt{sex,bmi,map,tc,hdl,ltg,glu}\}
$$
\item \texttt{Backward Regression}:
$$
\{\texttt{sex,bmi,map,tc,ldl,ltg}\}
$$
\item S\'election na\"{\i}ve:
$$
\{\texttt{sex,bmi,map,tc}\}
$$
\end{itemize}
\end{frame}


%\section{Pr\'evision}

\begin{frame}
\frametitle{Pr\'evision}

Mod\`ele de r\'egression \vspace{2mm} \centerline{$ Y_i=
r(\vartheta, {\bf x}_i)+\xi_i, \quad i=1,\dots,n.$} R\'egression
{\color{red}lin\'eaire}: $r(\vartheta, {\bf x}_i)=\vartheta^T{\bf
x}_i$. Exemple: ${\bf x}_i$ vecteur de 10 variables explicatives
(\texttt{age,sex,bmi,...}) pour patient $i$.
\begin{itemize}
\item {\bf Probl\`eme de pr\'evision}:
Un nouveau patient arrive avec son vecteur des 10 variables ${\bf
x}_0\in \R^{10}$. Donner la pr\'evison de la valeur de fonction de
r\'egression $r(\vartheta, {\bf x}_0)=\vartheta^T{\bf x}_0$\\
(=\'etat du patient dans 1 an).
\item Soit $\est$ un estimateur de $\vartheta$. {\color{red}Pr\'evision par
substitution:}
 \centerline{$\boxed{ \widehat Y = r(\est, {\bf x}_0).}$}
\item \underline{Question statistique}: quelle est la qualit\'e de la pr\'evision?
{\color{red}Intervalle de confiance} pour $r(\vartheta, {\bf x}_0)$
bas\'e sur $\widehat Y$?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Pr\'evision: mod\`ele lin\'eaire gaussienne}
\begin{itemize}
\item Traitement sur l'exemple: $r(\vartheta, {\bf x})=\vartheta^T{\bf
x}$, r\'egression {\color{red}lin\'eaire gaussienne} et
$\est=\estMC$. $\Longrightarrow$ $\boxed{\widehat Y = {\bf
x}_0^T\estMC}$
\item \underline{Hyp. 1} : $\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2\mathrm{Id}_n)$.
\item \underline{Hyp. 2} : $\design^T \design>0$.
\end{itemize}
\begin{prop}
\begin{itemize}
\item[(i)] $\widehat Y \sim {\mathcal N}\big({\bf
x}_0^T\vartheta, \sigma^2 {\bf
x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0\big)$
\item[(ii)] $\widehat Y-{\bf
x}_0^T\vartheta$ et $\boldsymbol{Y}-\design \estMC$ sont
indépendants.
\end{itemize}
\end{prop}
Rappel: $\|\boldsymbol{Y}-\design \estMC\|^2 \sim
\sigma^2\chi^2(n-k)$ {\color{red} loi du Chi 2 à $n-k$ degrés de
liberté}.
\end{frame}

\begin{frame}
\frametitle{Pr\'evision: mod\`ele lin\'eaire gaussienne}
\begin{itemize}
\item D'apr\`es la Proposition,
$$
\eta:=\frac{\widehat Y -{\bf x}_0^T\vartheta} {\sqrt{\sigma^2 {\bf
x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}}\sim {\mathcal
N}(0,1).
$$
\item On replace $\sigma^2$ inconnu par $\widehat \sigma_n^2 =
{\|\boldsymbol{Y}-\design \estMC\|^2}/({n-k}).$
\item {\color{red}$t$-statistique:}
$$
t:= \frac{\widehat Y -{\bf x}_0^T\vartheta} {\sqrt{\widehat
\sigma_n^2 {\bf x}_0^T\big(\design^T\design\big)^{-1}{\bf
x}_0}}=\frac{\eta}{\sqrt{\chi/(n-k)}}\sim t_{n-k},
$$
{\color{red}loi de Student à $n-k$ degrés de liberté}, car $\eta\sim
{\mathcal N}(0,1)$, $\chi:=\|\boldsymbol{Y}-\design
\estMC\|^2/\sigma^2\sim \chi^2(n-k)$ et $\eta\ind\chi$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Pr\'evision: intervalle de confiance}
\begin{eqnarray*}
&&\PP \Big(-q_{1-\frac{\alpha}{2}}(t_{n-k}) \le \frac{\widehat Y
-{\bf x}_0^T\vartheta} {\sqrt{\widehat \sigma_n^2 {\bf
x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}}\le
q_{1-\frac{\alpha}{2}}(t_{n-k})\Big) \\\hspace{4mm} &&= \PP(-
q_{1-\frac{\alpha}{2}}(t_{n-k}) \le t\le
q_{1-\frac{\alpha}{2}}(t_{n-k})) = 1-\alpha.
\end{eqnarray*}
$\Longrightarrow$ {\color{red}intervalle de confiance} de niveau
$1-\alpha$ pour $r(\vartheta,{\bf x}_0)={\bf x}_0^T\vartheta$ est
{\color{red}$[r_L, r_U]$}, o\`u:
\begin{eqnarray*}
{\color{red}r_L}&=&\widehat Y -
q_{1-\frac{\alpha}{2}}(t_{n-k})\sqrt{\widehat \sigma_n^2
{\bf x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0},\\
{\color{red}r_U}&=& \widehat Y +
q_{1-\frac{\alpha}{2}}(t_{n-k})\sqrt{\widehat \sigma_n^2 {\bf
x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}.
\end{eqnarray*}
\end{frame}



\begin{frame}
\frametitle{Limites des moindres carrés et du cadre gaussien}
\begin{itemize}
\item Calcul {\color{red}explicite} (et efficace) de l'EMC  limité à
une fonction de régression {\color{red}linéaire}.
\item Mod\`ele lin\'eaire donne un cadre assez g\'en\'eral:
\begin{itemize}
\item Mod\`ele
polynomial, \item {\color{red}Mod\`eles avec interactions...}
\end{itemize}
\item {\color{red} Hypothèse de gaussianité} = cadre asymptotique implicite.
\item Besoin d'outils pour les modèles  à réponse {\color{red}$Y$ discrète}.
\end{itemize}
\end{frame}

%\section{R\'egression lin\'eaire non-gaussienne}

\begin{frame}
\frametitle{R\'egression lin\'eaire non-gaussienne} Mod\`ele de
r\'egression lin\'eaire \vspace{3mm} \centerline{$ Y_i= \vartheta^T
{\bf x}_i+\xi_i, \quad i=1,\dots,n.$}

\vspace{-2mm}

\begin{itemize}
\item \underline{Hyp. 1'} : {\color{red}$\xi_i$ i.i.d., $\E[\xi_i]
=0$, $\E[\xi_i^2] = \sigma^2>0$.}
\item \underline{Hyp. 2'} : $\design^T \design>0$, {\color{red}$\lim_n\max_{1\le i \le n}{\bf x}_i^T
\big(\design^T \design\big)^{-1}{\bf x}_i =0$.}
\end{itemize}
\begin{prop}[Normalit\'e asymptotique de l'EMC]
$$
\sigma^{-1}\big(\design^T
\design\big)^{1/2}(\estMC-\vartheta)\stackrel{d}{\longrightarrow}
{\mathcal N}\big(0, \mathrm{Id}_k), \quad n\to\infty.
$$
\end{prop}
\begin{itemize}
\item A comparer avec le cadre gaussien:\vspace{2mm}
\centerline{$\sigma^{-1}\big(\design^T
\design\big)^{1/2}(\estMC-\vartheta)\sim {\mathcal N}\big(0,
\mathrm{Id}_k)$ \text{pour tout $n$.}}
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Vitesses de convergence}
%\end{frame}

%\subsection{Propriété de l'EMC: cadre g\'en\'eral (non-gaussien) }
%
%\begin{frame}
%\begin{itemize}
%\item \underline{Hyp. 1} : $\design^T \design$ inversible
%\item  \underline{Hyp. 2} :
%{\color{red}$\E\big[\boldsymbol{\xi}\big]=0$,
%$\E\big[\boldsymbol{\xi}\boldsymbol{\xi}^T\big] = \sigma^2
%\mathrm{Id}_n$}.
%\end{itemize}
%\begin{prop}
%%Sous les hypothèses précédentes
%\begin{itemize}
%\item $\E_\vartheta\big[\estMC\big]=\vartheta$ et
%$$\E_\vartheta\big[\big(\estMC-\vartheta\big)\big(\estMC-\vartheta\big)^T\big]=\sigma^2 \big(\design^T\design\big)^{-1}$$
%\item Si l'on pose
%$$\boxed{\widehat \sigma_n^2 = \frac{\|\boldsymbol{Y}-\design \estMC\|^2}{n-{\color{red}k}} = \frac{1}{n-{\color{red}k}}\sum_{i = 1}^n\big(Y_i-(\estMC)^T\bx_i\big)^2}$$
%alors $\E_\vartheta\big[\widehat \sigma_n^2\big]=\sigma^2.$
%\end{itemize}
%\end{prop}
%\end{frame}





\section{Régression non-linéaire}


\begin{frame}
\frametitle{Régression non-linéaire}
\begin{itemize}
\item On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),$$
où
$$\boxed{Y_i = r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;i=1,\ldots,n}$$
avec
$$\bx_i\in \R^k,\;\;\text{et}\;\; {\color{red}\vartheta \in \Theta \subset \R^d}.$$
\item Si $\xi_i \sim_{\text{i.i.d.}} {\mathcal N}(0,\sigma^2)$,
$${\mathcal L}_n(\vartheta, Y_1,\ldots, Y_n) \propto \exp\Big(-\frac{1}{2\sigma^2}\sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2\Big)$$
et l'estimateur du {\color{red}maximum de vraisemblance} est obtenu en minimisant la fonction
$$\vartheta \leadsto \sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Moindre carrés non-linéaires}
\begin{df}
\begin{itemize}
\item $M$-estimateur associé à la {\color{red}fonction de contraste} $\psi:\Theta \times {\color{red}\R^k}\times \R\rightarrow \R$ : tout estimateur $\est$ satisfaisant
$$\sum_{i = 1}^n \psi(\est, \bx_i, Y_i) = \max_{a \in \Theta} \sum_{i = 1}^n \psi(a,\bx_i,Y_i).$$
\item Estimateur des {\color{red}moindres carrés non-linéaires} : associé au contraste $\psi(a,\bx,y) = -\big(y-r(a,\bx)\big)^2$.
\end{itemize}
\end{df}
\begin{itemize}
\item {\color{red}Extension} des résultats en densité
$\rightarrow$ théorèmes limites pour des sommes de v.a.
indépendantes {\color{red} non-équidistribuées}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle à réponse binaire}
\begin{itemize}
\item On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),\;\;{\color{red}Y_i \in \{0,1\}},\;\bx_i \in \R^k.$$
\item Modélisation {\color{red}via la fonction de régression}
$$\bx \leadsto p_{\bx}(\vartheta) = \E_\vartheta\big[Y|\bX = \bx\big] = \PP_\vartheta\big[Y = 1|\bX=\bx\big]$$
%$$Y_i = p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big)$$
\item {\color{red}Représentation}
\begin{align*}
Y_i & =  p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big) \\
& = r(\vartheta,\bx_i)+\xi_i
\end{align*}
avec
$r(\vartheta, \bx_i) = p_{\bx_i}(\vartheta)$ et $\xi_i = Y_i-p_{\bx_i}(\vartheta).$
\item $\E_\vartheta\big[\xi_i\big]=0$ mais structure des $\xi_i$ {\color{red}compliquée} (dépendance en $\vartheta$).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle à réponse discrète}
\begin{itemize}
\item $Y_i $ v.a. de Bernoulli de paramètre $p_{\bx_i}({\color{red}\vartheta})$.

{\color{red} Vraisemblance}
$${\mathcal L}_n(\vartheta,Y_1,\ldots, Y_n) = \prod_{i = 1}^n p_{\bx_i}({\color{red}\vartheta})^{Y_i}(1-p_{\bx_i}\big({\color{red}\vartheta})\big)^{1-Y_i}$$
$\rightarrow$ méthodes de résolution numérique.
\item {\color{red} Régression logistique} (très utile dans les applications)
$$p_{\bx}(\vartheta) = \psi(\bx^T\vartheta),$$
$$\psi(t)=\frac{e^t}{1+e^t},\;t \in \R\;\;{\color{red}{\text{fonction logistique}}}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Régression logistique et modèles latents}
\begin{itemize}
\item {\color{red}Représentation équivalente de la régression logistique} : on observe
$$\boxed{Y_i = 1_{\big\{Y_i^\star >0\big\}},\;\;i=1,\ldots,n}$$
(les $\bx_i$ sont donnés), et $Y_i^\star$ est une  {\color{red}variable latente} ou cachée,
$$\boxed{Y^\star_i ={\color{red}\vartheta}^T \bx_i + U_i,\;\;i=1,\ldots, n}$$
avec {\color{red}$U_i\sim_{\text{i.i.d.}} F$}, où
$$F(t) = \frac{1}{1+e^{-t}},\;t \in \R.$$
\item
\begin{align*}
\PP_\vartheta\big[Y_i^\star>0] & = \PP_\vartheta\big[\bx_i^T\vartheta + U_i >0\big] \\
& = 1-\PP_\vartheta\big[U_i \leq -\bx_i^T\vartheta\big] \\
& = 1-\big(1+\exp(-\bx_i^T\vartheta)\big)^{-1} =  \psi(\bx_i^T\vartheta).
\end{align*}
\end{itemize}
\end{frame}

\section{Bilan provisoire : modèles paramétriques dominés}

\begin{frame}
\frametitle{Bilan provisoire : modèles paramétriques dominés}
%, construction d'estimateurs dans les situations suivantes :
\begin{itemize}
\item \underline{{\color{red}Modèle de densité :}} on observe
$$X_1,\ldots,X_n \sim_{\text{i.i.d?}} \PP_\vartheta,\;\;\vartheta \in \Theta \subset \R^d.$$
{\color{blue}Estimateurs :} moments, $Z$- et $M$-estimateurs, {\color{red}EMV}.
\item\underline{{\color{red}Modèle de régression :}} on observe
$$Y_i = r(\vartheta, \bx_i)+\xi_i,\;\;i=1,\ldots, n,\;\;\xi_i\;\text{i.i.d.},\;\vartheta \in \Theta \subset \R^d.$$
{\color{blue} Estimateurs :}
\begin{itemize}
\item Si $r(\vartheta, \bx) =  \vartheta^T\bx$, EMC (coïncide avec l'{\color{red}EMV} si les $\xi_i$ gaussiens)
\item Sinon, $M$-estimateurs, {\color{red}EMV}...
\item Autres méthodes selon des {\color{red}hypothèses} sur le \og design \fg{}...
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bilan provisoire (cont.) : précision d'estimation}
$\est$ estimateur de $\vartheta$ : {\color{red}précision, qualité} de  $\est$ ?
Approche par {\color{red}région-intervalle de confiance}
\begin{itemize}
\item Pour $\alpha \in (0,1)$, on construit ${\mathcal C}_{n,\alpha}(\est)$ {\color{red} ne dépendant pas de $\vartheta$} (observable)
tel que
$$\PP_\vartheta \big[\vartheta \in {\mathcal C}_{n,\alpha}(\est)\big] \geq 1-\alpha$$
asymptotiquement lorsque $n\rightarrow \infty$, uniformément en $\vartheta$...
La {\color{red}précision} de l'estimateur est le {\color{red} diamètre} (moyen) de ${\mathcal C}_{n,\alpha}(\est)$.
\item Par exemple : ${\mathcal C}_{n,\alpha}(\est) =$ boule de centre $\est$ et de rayon {\color{red}à déterminer}.
\end{itemize}
\end{frame}

\begin{frame}
En pratique, une information {\color{red} non-asymptotique} de type
$$\E\big[\|\est-\vartheta\|^2\big] \leq c_n(\vartheta)^2,$$
ou bien {\color{red} asymptotique} de type
$$v_n(\est-\vartheta) \stackrel{d}{\longrightarrow} Z_\vartheta,\;\;n\rightarrow \infty$$
(avec $v_n\rightarrow \infty$) permet \og souvent\fg{} de construire
un(e) région-intervalle de confiance.
\end{frame}








\end{document}
