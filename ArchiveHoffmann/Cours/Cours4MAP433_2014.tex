
\documentclass{beamer}
\usetheme[right,hideothersubsections]{Berkeley}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext}
\usepackage{amsfonts}

%\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[applemac]{inputenc}


\newenvironment{disarray}{\everymath{\displaystyle\everymath{}}\array} {\endarray}
\newtheorem{theo}{Théorème}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{cor}{Corollary}[theo]
\newtheorem{lm}{Lemma}
\newtheorem{nota}{Notation}
\newtheorem{rk}{Remark}
\newtheorem{remarque}{Remarque}
\newtheorem{exa}{Example}
\newtheorem{df}{Définition}
\newtheorem{hypothese}{Hypothèse}
\newtheorem{terminologie}{Terminologie}
\newenvironment{dem}{\textbf{Proof}}{\flushright$\blacksquare$\\}

\DeclareMathOperator{\E}{{\mathbb E}}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\IF}{{\mathbb I}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Pour le modèle linéaire

\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bx}{\boldsymbol{x}}
\DeclareMathOperator{\vp}{\boldsymbol{p}}
\DeclareMathOperator{\vq}{\boldsymbol{q}}
\DeclareMathOperator{\estMC}{\widehat \vartheta_n^{\,\,{\tt mc}}}
\DeclareMathOperator{\estMCNL}{\widehat \vartheta_n^{\,\,{\tt mcnl}}}
\DeclareMathOperator{\estMV}{\widehat \vartheta_n^{\,\,{\tt mv}}}
\DeclareMathOperator{\design}{\mathbb{M}}
\DeclareMathOperator{\est}{\widehat \vartheta_{\mathnormal{n}}}
\DeclareMathOperator{\var}{\mathrm{Var}}
\DeclareMathOperator{\estMVc}{\widehat \vartheta_{n,0}^{\,{\tt mv}}}
\DeclareMathOperator{\Xbar}{\overline{\mathnormal{X}}_\mathnormal{n}}




\title{MAP 433 : Introduction aux méthodes statistiques. Cours 4}
%\author{M. Hoffmann}
%\institute{Université Paris-Est and ETG}
\begin{document}
\date{21 février 2014}
\maketitle



\begin{frame}
\frametitle{Aujourd'hui}
\tableofcontents
\end{frame}


\section{$M$-estimation, rappel du Cours 3}

\begin{frame}
\frametitle{$M$-estimation}
\begin{itemize}
\item \underline{Situation} : on observe $X_1,\ldots, X_n$ de loi $\PP_{\vartheta}$ sur $\R$ et $\vartheta \in \Theta$.
\item \underline{Principe} : Se donner une application $\psi : \Theta \times \R \rightarrow \R_+$ telle que, pour tout $\vartheta \in \Theta \subset {\color{red}\R^d}$,
$$a \leadsto \E_\vartheta\big[\psi(a,X)\big] = \int \psi(a,x)\PP_\vartheta(dx)$$
admet {\color{red}un maximum en $a=\vartheta$}.
\end{itemize}
\begin{df}
On appelle $M$-estimateur associé à $\psi$ tout estimateur ${\color{red}\est}$ satisfaisant
$$\sum_{i = 1}^n \psi({\color{red}\est}, X_i) = \max_{a \in \Theta}\sum_{i = 1}^n\psi(a, X_i).$$
\end{df}
%{itemize}
%\item Il n'y a pas unicité de $\est$ (à ce niveau).
%\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Un exemple classique : paramètre de localisation}
\begin{itemize}
\item $\Theta = \R$, $\PP_{\color{red}\vartheta}(dx) = f(x-{\color{red}\vartheta})dx$, et $\int_{\R}xf(x)dx=0$, $\int_{\R}x^2\PP_\vartheta(dx)<+\infty$ pour tout $\vartheta \in \R$. On pose
$$\boxed{\psi(a,x)=-(a-x)^2}$$
\item La fonction
$$a \leadsto \E_\vartheta\big[\psi(a,X)\big] =
-\int_{\R}(a-x)^2f(x-\vartheta)dx$$
admet un {\color{red}maximum} en $a=\E_\vartheta\big[X\big] = \int_{\R}xf(x-\vartheta)dx=\vartheta.$
\item {\color{red}$M$-estimateur associé :}
$$\sum_{i = 1}^n(X_i-\est)^2 = \min_{a \in \R}\sum_{i = 1}^n (X_i-a)^2.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Paramètre de localisation}
\begin{itemize}
\item C'est {\color{red}aussi} un $Z$-estimateur associé à $\phi(a,x)=2(x-a)$: on résout
$$\sum_{i = 1}^n (a-X_i)=0\;\;\text{d'où}\;\;\est = \overline{X}_n.$$
\item Dans cet {\color{red}exemple très simple},
tous les points de vue coïncident.
%
\item Si, dans le même contexte,
$\int_{\R}x^2\PP_\vartheta(dx)=+\infty$ et $f(x)=f(-x)$, on peut
utiliser $Z$-estimateur avec $\phi(a,x)={\rm Arctg}(x-a)$. M\'ethode
robuste, mais est-elle optimale? Peut-on faire mieux {\color{red}si
$f$ est connue? A suivre...}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lien entre $Z$- et $M$- estimateurs}
\begin{itemize}
\item {\color{red}Pas d'inclusion} entre ces deux classes d'estimateurs {\color{red}en général} :
\begin{itemize}
\item Si $\psi$ non-régulière, $M$-estimateur $\nRightarrow$ $Z$-estimateur
\item Si une équation d'estimation admet plusieurs solutions distinctes, $Z$-estimateur $\nRightarrow$ $M$-estimateur (cas d'un extremum local).
\end{itemize}
\item Toutefois, si $\psi$ {\color{red}est régulière}, les $M$-estimateurs {\color{red}sont} des $Z$-estimateurs : si $\Theta \subset \R$ ($d=1$), en posant
$$\phi(a,x) = \partial_a\psi(a,x),$$
on a
$$\boxed{\sum_{i = 1}^n \partial_{a} \psi(\vartheta, X_i)\big|_{a = \est}
= \sum_{i = 1}^n \phi(\est, X_i)=0}.$$
%\item On travaillera (presque) toujours dans ce cadre.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{$M$-estimateur : exemple}
%\end{frame}

\subsection{Principe de maximum de vraisemblance}

\begin{frame}
\frametitle{Maximum de vraisemblance}
\begin{itemize}
\item Principe {\color{red} fondamental} et
{\color{red}incontournable} en statistique. Cas particuliers connus
depuis le XVIII\`eme si\`ecle. D\'efinition g\'en\'erale:
Fisher~(1922).
\item Fournit une première {\color{red}méthode systématique} de construction d'un $M$-estimateur
(souvent un $Z$-estimateur, souvent aussi {\it a posteriori} un
estimateur par substitution simple).
\item Procédure {\color{red}optimale} (dans quel sens ?)
sous des hypothèses de {\color{red} régularité} de la famille
$\{\PP_\vartheta, \vartheta \in \Theta\}$ (Cours 6).
\item Parfois difficile à mettre en oeuvre en pratique
$\rightarrow$ {\color{red}méthodes numériques}, statistique
computationnelle.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Fonction de vraisemblance}
\begin{itemize}
\item La famille $\{\PP_\vartheta,\vartheta \in \Theta\}$ est dominée par une mesure $\sigma$-finie $\mu$. On se donne, pour $\vartheta \in \Theta$
$$f(\vartheta,x) = \frac{d\PP_\vartheta}{d\mu}(x),\;x \in \R.$$
\end{itemize}
\begin{df}
{\color{red}Fonction de vraisemblance} du $n$-échantillon associée à la famille $\{f(\vartheta,\cdot),\vartheta \in \Theta\}$ :
$$\boxed{\vartheta \leadsto {\mathcal L}_n(\vartheta, X_1,\ldots, X_n) = \prod_{i = 1}^n f(\vartheta, X_i)}$$
\end{df}
%\end{frame}
\begin{itemize}
\item C'est une fonction aléatoire (définie $\mu$-presque partout).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemples}
\begin{itemize}
\item \underline{Exemple 1}: {\color{red}Modèle de Poisson}. On observe
$$X_1,\ldots, X_n \sim_{\text{i.i.d.}}\text{Poisson}({\color{red}\vartheta}),$$
${\color{red}\vartheta} \in \Theta = \R_+\setminus \{0\}$ et prenons
$\mu(dx) = \sum_{k \in \N}\delta_k(dx)$.
\item La densit\'e de $\PP_\vartheta$ par rapport \`a $\mu$ est
$$f({\color{red}\vartheta}, x) = e^{-{\color{red}\vartheta}}
\frac{{\color{red}\vartheta}^x}{x!}, \quad x=0,1,2,\dots.$$
\item La {\color{red} fonction de vraisemblance} associée s'écrit
\begin{align*}
\vartheta \leadsto {\mathcal L}_n(\vartheta, X_1,\ldots, X_n)
&= \prod_{i = 1}^n e^{-\vartheta}\frac{\vartheta^{X_i}}{X_i!} \\
&= \frac{1}{\prod_{i = 1}^nX_i!} e^{-n\vartheta} \vartheta^{\sum_{i = 1}^n X_i}.
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemples}
\begin{itemize}
\item \underline{Exemple 2} {\color{red}Mod\`ele de Cauchy}. On observe
$$X_1,\ldots, X_n \sim_{\text{i.i.d.}}\text{Cauchy},$$
${\color{red}\vartheta} \in \Theta = \R$ et $\mu(dx)=dx$ ({\color{red} par exemple}).
\item On a alors
$$\PP_{\color{red}\vartheta}(dx)=f({\color{red}\vartheta},x)dx=\frac{1}{\pi\big(1+(x-{\color{red}\vartheta})^2\big)}dx.$$
\item La {\color{red} fonction de vraisemblance} associée s'écrit
$$\vartheta \leadsto {\mathcal L}_n(\vartheta, X_1,\ldots, X_n) = \frac{1}{\pi^n}\prod_{i = 1}^n \big(1+(X_i-\vartheta)^2\big)^{-1}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principe de maximum de vraisemblance}
\begin{itemize}
\item Cas d'une famille de lois {\color{red} restreinte à deux points}
$$\Theta  = \{\vartheta_1,\vartheta_2\} \subset \R,$$
avec $\PP_{\vartheta_i}$ discrète et $\mu(dx)$ la mesure de comptage.
\item {\color{red}A priori}, pour tout $(x_1,\ldots, x_n)$, et pour $\vartheta \in \{\vartheta_1,\vartheta_2\}$,
\begin{align*}
\PP_\vartheta\big[X_1=x_1,\ldots, X_n=x_n\big] & = \prod_{i=1}^n \PP_\vartheta\big[X_i=x_i\big] \\
&=\prod_{i = 1}^nf(\vartheta, x_i).
\end{align*}
La probabilit\'e d'avoir la r\'ealisation fix\'ee $(x_1,\ldots,
x_n)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principe de maximum de vraisemblance}
\begin{itemize}
\item {\color{red}A posteriori, on observe $(X_1,\ldots, X_n)$.} L'événement
$$\Big\{\prod_{i = 1}^n f({\color{red}\vartheta_1},X_i) > \prod_{i = 1}^n f({\color{blue}\vartheta_2},X_i)\Big\}\;\;\;\text{(Cas 1)}$$
{\color{red}ou bien} l'événement
$$\Big\{\prod_{i = 1}^n f({\color{blue}\vartheta_2},X_i) > \prod_{i = 1}^n f({\color{red}\vartheta_1},X_i)\Big\}\;\;\;\text{(Cas 2)}$$
est réalisé. (On ignore le cas d'égalité.)
\item {\color{red} Principe de maximum de vraisemblance:}
$$\estMV = {\color{red}\vartheta_1} 1_{\{\text{Cas 1}\}}+ {\color{blue}\vartheta_2} 1_{\{\text{Cas 2}\}}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimateur du maximum de vraisemblance}
\begin{itemize}
\item On généralise le principe précédent pour une famille de lois et un ensemble de paramètres {\color{red}quelconques}.
\item \underline{Situation} : $X_1,\ldots, X_n\sim_{\text{i.i.d.}}\PP_\vartheta$, $\{\PP_\vartheta,\vartheta \in \Theta\}$ dominée, $\Theta \subset \R^d$, $\vartheta \leadsto {\mathcal L}_n(\vartheta, X_1,\ldots, X_n)$ vraisemblance associée.
\end{itemize}
\begin{df}
On appelle {\color{red} estimateur du maximum de vraisemblance} tout estimateur $\estMV$ satisfaisant
$${\mathcal L}_n(\estMV,X_1,\ldots, X_n) = \max_{\vartheta \in \Theta} {\mathcal L}_n(\vartheta, X_1,\ldots, X_n).$$
\end{df}
\begin{itemize}
\item {\color{red}Existence, unicité...}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Remarques}
\begin{itemize}
\item \underline{Log-vraisemblance}:
\begin{align*}\vartheta \leadsto \ell_n(\vartheta, X_1,\ldots, X_n)& = \log {\mathcal L}_n(\vartheta, X_1,\ldots, X_n)\\
& = \sum_{i = 1}^n \log f(\vartheta, X_i).
\end{align*}
{\color{red}Bien défini} si $f(\vartheta, \cdot) >0$ $\mu$-pp.
$$\text{Max. vraisemblance = max. log-vraisemblance.}$$
\item L'estimateur du maximum de vraisemblance {\color{red} ne dépend pas} du choix de la mesure dominante $\mu$.
\item Notion de {\color{red} racine de l'équation de vraisemblance} : tout estimateur $\widehat \vartheta_n^{\,{\tt rv}}$ vérifiant
$$\nabla_\vartheta \ell_n(\widehat \vartheta_n^{\,{\tt rv}}, X_1,\ldots, X_n) = 0.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple : modèle normal } L'expérience statistique est
engendrée par un $n$-échantillon de loi ${\mathcal
N}(\mu,\sigma^2)$, le paramètre est $\vartheta = (\mu,\sigma^2)\in
\Theta = \R\times \R_+\setminus\{0\}$.
\begin{itemize}
\item
{\color{red}Vraisemblance} $${\mathcal L}_n((\mu,\sigma^2),
X_1,\ldots, X_n) =
\frac1{(2\pi\sigma^2)^{n/2}}\exp\big(-\tfrac{1}{2\sigma^2}
\sum_{i=1}^n(X_i-\mu)^2\big).$$
\item {\color{red}Log-vraisemblance}
$$\ell_n\big((\mu,\sigma^2),X_1,\ldots, X_n\big) = -\frac{n}{2}
\log(2\pi \sigma^2)-\frac{1}{2\sigma^2}\sum_{i = 1}^n (X_i-\mu)^2.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple : modèle normal }
%\begin{itemize}
%\item
{\color{red}Equation(s) de vraisemblance}
$$
\left\{
\begin{array}{lll}
\partial_\mu\ell_n \big((\mu,\sigma^2),X_1,\ldots, X_n\big) & = &\displaystyle\frac{1}{\sigma^2}\sum_{i = 1}^n (X_i-\mu) \\ \\
\partial_{\sigma^2}\ell_n \big((\mu,\sigma^2),X_1,\ldots, X_n\big)&
 = &\displaystyle -\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}
 \sum_{i = 1}^n (X_i-\mu)^2.
\end{array}
\right.
$$
Solution de ces \'equations (pour $n \geq 2$):
$$\boxed{\widehat
\vartheta_n^{\,{\tt rv}} = \big(\overline{X}_n,\frac{1}{n} \sum_{i =
1}^n(X_i-\overline{X}_n)^2\big)}$$ et on vérifie que $\widehat
\vartheta_n^{\,{\tt rv}} =\estMV$.
%\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Exemple : modèle de Poisson}
\begin{itemize}
\item
{\color{red}Vraisemblance}
$${\mathcal L}_n(\vartheta, X_1,\ldots, X_n) =
\frac{1}{\prod_{i = 1}^n X_i!}e^{-n\vartheta}\vartheta^{\sum_{i = 1}^n X_i}.$$
\item {\color{red}Log-vraisemblance}
$$\ell_n(\vartheta, X_1,\ldots, X_n) = c(X_1,\ldots, X_n)-n\vartheta +\sum_{i =1}^n X_i \log \vartheta.$$
\item {\color{red}Equation de vraisemblance}
$$-n+\sum_{i = 1}^n X_i \frac{1}{\vartheta} = 0,\;\;
\text{soit}\;\;
\boxed{\widehat \vartheta_n^{\,{\tt rv}} = \frac{1}{n}\sum_{i = 1}^n X_i=\overline{X}_n}$$
et on vérifie que $\widehat \vartheta_n^{\,{\tt rv}} =\estMV$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple : mod\`ele de Laplace} L'expérience statistique
est engendrée par un $n$-échantillon de loi de Laplace de paramètre
$\vartheta \in \Theta = \R$. La densité par rapport à la mesure de
Lebesgue :
$$f(\vartheta,x) = \frac{1}{2\sigma}\exp\big(-\frac{|x-\vartheta|}{\sigma}\big),$$
où $\sigma >0$ est {\color{red}connu}.
\begin{itemize}
\item {\color{red}Vraisemblance}
$${\mathcal L}_n(\vartheta, X_1,\ldots, X_n) = (2\sigma)^{-n}
\exp\big(-\frac{1}{\sigma}\sum_{i = 1}^n \big|X_i-\vartheta\big|\big)$$
\item {\color{red}Log-vraisemblance}
$$\ell_n(\vartheta,X_1,\ldots, X_n) = - n \log(2\sigma)-
\frac{1}{\sigma}\sum_{i = 1}^n \big|X_i-\vartheta\big|.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple : mod\`ele de Laplace} Maximiser ${\mathcal
L}_n(\vartheta, X_1,\ldots, X_n)$ revient à minimiser la fonction
$\vartheta \leadsto \sum_{i = 1}^n \big|X_i-\vartheta\big|$,
dérivable presque partout de dérivée constante par morceaux.
{\color{red}Equation de vraisemblance:}
$$\sum_{i = 1}^n \text{sign}(X_i-\vartheta)=0.$$
Soit $X_{(1)}\leq \ldots \leq X_{(n)}$ la statistique d'ordre.
\begin{itemize}
\item
$n$ pair: $\estMV$ {\color{red}n'est pas unique}; tout point de
l'intervalle
$\big[X_{\big(\tfrac{n}{2}\big)},X_{\big(\tfrac{n}{2}+1\big)} \big]$
est un EMV.
\item $n$ impair: $\estMV=X_{\big(\tfrac{n+1}{2}\big)}$,
l'EMV est unique. Mais $\widehat \vartheta_n^{\,{\tt rv}}$ n'existe
pas.
\item {\color{red}pour tout} $n$, la
médiane empirique est un EMV.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple : mod\`ele de Cauchy}
\begin{itemize}
\item {\color{red}Vraisemblance}
$${\mathcal L}_n(\vartheta, X_1,\ldots, X_n) = \pi^{-n} \prod_{i =1}^n \frac{1}{1+(X_i-\vartheta)^2}.$$
\item {\color{red}Log-vraisemblance}
$$\ell_n(\vartheta,X_1,\ldots, X_n) = -n\log \pi -\sum_{i = 1}^n \log\big(1+(X_i-\vartheta)^2\big)$$
\item {\color{red}Equation de vraisemblance}
$$\boxed{\sum_{i = 1}^n \frac{X_i-\vartheta}{1+(X_i-\vartheta)^2}=0}$$
pas de solution explicite et admet en général plusieurs solutions.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Maximum de vraisemblance = $M$-estimateur}
\begin{itemize}
\item \underline{Une inégalité de convexité} : $\mu$ mesure $\sigma$-finie sur $\R$ ; $f,g$ deux {\color{red}densités de probabilités} par rapport à $\mu$. Alors
$$\boxed{\int_{\R}f(x)\log f(x) \mu(dx) \geq \int_{\R} f(x) \log g(x) \mu(dx)}$$
(si les intégrales sont finies) avec égalité {\color{red}ssi} $f=g$ $\mu$-pp.
\item \underline{Preuve}: à montrer
$$\int_{\R} f(x) \log \frac{g(x)}{f(x)}\mu(dx) \leq 0.$$
(avec une convention de notation appropriée)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Une inégalité de convexité}
\begin{itemize}
\item On a $\log(1+x)\leq x$ pour $x \geq -1$ avec égalité ssi $x=0$.
\item Donc
$$\log \frac{g(x)}{f(x)} = \log\Big(1+\big(\frac{g(x)}{f(x)}-1\big)\Big) \leq \frac{g(x)}{f(x)}-1$$
(avec égalité ssi $f(x)=g(x)$).
\item Finalement
\begin{align*}
\int_{\R}f(x)\log \frac{g(x)}{f(x)}\mu(dx)& \leq \int_{\R} f(x)\Big(\frac{g(x)}{f(x)}-1\Big)\mu(dx) \\
& = \int_{\R} g(x)\mu(dx)- \int_{\R}f(x) \mu(dx)\\
&=0.
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conséquence pour l'EMV}
\begin{itemize}
\item On pose
$$\boxed{\psi(a,x):=\log f(a,x),\;\;a \in \Theta,\;x\in\R}$$
(avec une convention pour le cas où on n'a pas $f(a,\cdot) >0$.)
\item La fonction
$$a \leadsto \E_\vartheta \big[\psi(a,X)\big]=\int_{\R}\log f(a,x) f(\vartheta,x) \mu(dx)$$
a un maximum en $a=\vartheta$ d'après {\color{red}l'inégalité de convexité}.
\end{itemize}
\end{frame}

\begin{frame}
%\frametitle{}
\begin{itemize}
\item Le $M$-estimateur associé à $\psi$ maximise la fonction
$$a \leadsto \sum_{i = 1}^n \log f(a, X_i) = \ell_n(a, X_1,\ldots, X_n)$$
c'est-à-dire la {\color{red} log-vraisemblance}. C'est {\color{red}l'estimateur du maximum de vraisemblance}.

\item C'est aussi un $Z$-estimateur si la fonction $\vartheta \leadsto \log f(\vartheta, \cdot)$ est régulière, associé à la fonction
$$\phi(\vartheta, x) = \partial_\vartheta \log f(\vartheta, x) = \frac{\partial_\vartheta f(\vartheta, x)}{f(\vartheta, x)},\;\vartheta \in \Theta, x\in \R$$
lorsque $\Theta \subset \R$, \`a condition que le maximum de
log-vraisemblance n'est pas atteint sur la fronti\`ere de $\Theta$.
(Se généralise en dimension $d$.)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Choix de mod\`ele statistique}
\begin{itemize}
\item  Le statisticien a le choix de la famille
$\{\PP_{\vartheta}, \vartheta\in\Theta\}$. L'EMV d\'epend de ce
choix.
\item \underline{Exemple:} on a l'\'echantillon ($n=10$):
$$ \underbrace{0.92, -0.20, -1.80, 0.02,  0.49, 1.41, -1.59, -1.29,
0.34}_{\footnotesize{tirage \ de \ {\mathcal N}(0,1)}},
{\color{red}100}.$$
\item On prend $\PP_{\vartheta}(dx) = f(x-\vartheta)dx$ pour deux
$f$ diff\'erents:
\item $f$ densit\'e de la loi normale $\Rightarrow$
$\estMV={\overline X}_n= {\color{red}9.83}$.
\item $f$ densit\'e de loi de Laplace $\Rightarrow$
 tout point de l'intervalle $[0.02, 0.34]$ est un $\estMV$, en
particulier, la m\'ediane: $$\estMV = M_n=
(0.02+0.34)/2={\color{red}0.18}.$$
\item {\color{red}Autre choix de mod\`ele...}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Un $M$-estimateur qui n'est pas un $Z$-estimateur}
\begin{itemize}
\item On observe $X_1,\ldots, X_n\sim_{\text{i.i.d}.}$ uniformes sur $[0,\vartheta]$, $\vartheta \in \Theta = \R_+\setminus \{0\}$.
\item On a $$\PP_{\color{red}\vartheta}(dx) = {\color{red}\vartheta}^{-1}1_{[0,{\color{red}\vartheta}]}(x)dx$$
et
\begin{align*}
{\mathcal L}_n(\vartheta, X_1,\ldots, X_n)& = \vartheta^{-n}\prod_{i = 1}^n 1_{[0,\vartheta]}(X_i) \\
& = \vartheta^{-n}1_{\{\max_{1 \leq i \leq n} X_i \leq \vartheta\}}
\end{align*}
\item La fonction de vraisemblance {\color{red}n'est pas régulière}.
\item {\color{red}L'estimateur du maximum de vraisemblance est}
$\estMV = \max_{1 \leq i \leq n}X_i$. %{\color{red} A suivre...}
\end{itemize}
\end{frame}


\section{EMV, asymptotique des $Z$- et $M$- estimateurs}

\begin{frame}
\frametitle{Asymptotique des $Z$- et $M$-estimateurs}
\begin{itemize}
\item Problème général {\color{red}délicat}. Dans ce cours : conditions suffisantes.
\item {\color{red}Convergence} : critère simple pour les $M$-estimateurs.
\item {\color{red}Vitesse de convergence} : technique simple pour les $Z$-estimateurs, à condition de savoir que l'estimateur est convergent.
\item Sous des hypothèses de régularité, un $M$-estimateur est un $Z$-estimateur.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence des $M$-estimateurs}
\begin{itemize}
\item \underline{Situation}: on observe $X_1,\ldots, X_n$ i.i.d. de loi dans la famille $\{\PP_\vartheta, \vartheta \in \Theta\}$.
\item $\psi:\Theta \times \R \rightarrow \R$ {\color{red}fonction de contraste}.
\item {\color{red}Loi des grands nombres :}
$$
M_n(a)=\frac{1}{n}\sum_{i = 1}^n \psi(a,X_i)$$
converge en $\PP_\vartheta$-probabilité vers
$$M(a,\vartheta)\;=\E_\vartheta\big[\psi(a,X)\big]$$
{\color{red} qui atteint son maximum en $a=\vartheta$}
\item \og à montrer\fg{} :
$${\color{red}\est}= {\color{red}
\arg \max_{a \in \Theta}}\, M_n(a)
\stackrel{\PP_\vartheta}{\longrightarrow} {\color{red}\arg \max_{a
\in \Theta}}
\E_\vartheta\big[\psi(a,X)\big]={\color{red}\vartheta}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence des $M$-estimateurs}
\begin{prop}
Si le $M$-estimateur ${\color{red}\est}$ associé à la fonction de contraste est bien défini et si
\begin{itemize}
\item $\sup_{a \in \Theta}|M_n(a)-M(a,\vartheta)| \stackrel{\PP_\vartheta}{\longrightarrow} 0$,
\item $\forall \varepsilon >0, \;\sup_{|a-\vartheta| \geq \varepsilon}M(a,\vartheta)<M(\vartheta,\vartheta)$ {\color{red}(condition de maximum)}
%\item $M_n(\est) \geq M_n(\vartheta)-\varepsilon_n$, avec $\varepsilon_n \stackrel{\PP_\vartheta}{\longrightarrow} 0$,
\end{itemize}
alors ${\color{red}\est} \stackrel{\PP_\vartheta}{\longrightarrow} \vartheta$.
\end{prop}
\begin{itemize}
\item La condition 1 (convergence uniforme) peut être délicate à montrer...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des $Z$-estimateurs}
\begin{itemize}
\item \underline{Situation}: on observe $X_1,\ldots, X_n$ i.i.d. de loi dans la famille $\{\PP_\vartheta, \vartheta \in \Theta\}$, ${\color{red}\Theta \subset \R}$.
\item {\color{red} $\est$} : $Z$-estimateur {\color{red}associé à} $\phi:\Theta \times \R \rightarrow \R$ vérifie
$$\boxed{\sum_{i = 1}^n \phi({\color{red}\est},X_i)=0}$$
\item Si $\est$ est un $M$-estimateur associé à la fonction de contraste $\psi$ {\color{red}régulière}, alors c'est un $Z$-estimateur associé à la fonction $\phi(a,x) = \partial_a\psi(a,x)$.
\item On suppose $\est$ convergent. {\color{red} Que dire de sa loi limite} ?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des $Z$-estimateurs : principe}
\begin{itemize}
\item {\color{red} Loi des grands nombres}$$Z_n(a)=\frac{1}{n}\sum_{i = 1}^n \phi(a,X_i) \stackrel{\PP_\vartheta}{\longrightarrow} Z(a,\vartheta) = \E_\vartheta\big[\phi(a,X)\big]$$
\item \underline{Principe}. Développement de Taylor autour de $\vartheta$ :
$$0 = Z_n(\est) = Z_n(\vartheta)+({\color{red}\est}-\vartheta)Z_n'(\vartheta)+{\color{red}\tfrac{1}{2}(\est-\vartheta)^2Z''(\widetilde \vartheta_n)}.$$
\item On {\color{red} néglige} le reste :
$$\sqrt{n}({\color{red}\est}-\vartheta) \approx \frac{-\sqrt{n}Z_n(\vartheta)}{Z_n'(\vartheta)}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des $Z$-estimateurs : principe}
\begin{itemize}
\item Convergence du {\color{red} numérateur}
$$\sqrt{n}Z_n(\vartheta) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n\phi(\vartheta,X_i) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0,\E_{\color{red}\vartheta}\big[\phi({\color{red}\vartheta},X)^2\big]\big)$$
si ${\color{red}\E_\vartheta\big[\phi(\vartheta,X)\big]=0}$ et ${\color{red}\E_\vartheta\big[\phi(\vartheta,X)^2\big]<+\infty}$.
\item Convergence du {\color{red} dénominateur} $$Z'_n(\vartheta) = \frac{1}{n}\sum_{i = 1}^n\partial_\vartheta \phi(\vartheta,X_i) \stackrel{\PP_\vartheta}{\longrightarrow}
\E_{{\color{red}\vartheta}}\big[\partial_\vartheta \phi({\color{red}\vartheta},X)\big]$$
{\color{red} $\neq 0$ (à supposer)}.
\item + hypothèses techniques pour {\color{red} contrôler le reste} (besoin de la convergence de $\est$).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des $Z$-estimateurs}
\begin{prop}[Convergence des $Z$-estimateurs]
\begin{itemize}
%\item $\forall \vartheta \in \Theta, \PP_\vartheta \ll \mu$
\item Soit $\Theta$ un ouvert de $\R$. Pour tout $\vartheta\in \Theta$, $\textcolor{red}{\est \stackrel{\PP_\vartheta}{\rightarrow} \vartheta}$, $\E_\vartheta\big[\phi(\vartheta,X)^2\big]<+\infty$
et
$$\E_\vartheta\big[\phi(\vartheta,X)\big]=0,\;\E_\vartheta\big[\partial_\vartheta\phi(\vartheta, X)\big]\neq 0.$$
\item {\color{red}(Contrôle reste)} pour tout $\vartheta\in\Theta$, pour tout $a$ {\color{red}dans un voisinage de $\vartheta$},
$$|\partial^2_a\phi(a,x)|\leq g(x),\;\;\E_\vartheta\big[g(X)\big]<+\infty.$$
\end{itemize}
Alors
$$\sqrt{n}({\color{red}\est}-\vartheta) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0,\frac{\E_{\color{red}\vartheta}[\phi({\color{red}\vartheta},X)^2]}{\big(\E_{\color{red}\vartheta}[\partial_{\vartheta}\phi({\color{red}\vartheta}, X)]\big)^2}\Big).$$
\end{prop}
\end{frame}


\section{Méthode d'estimation dans le modèle de régression}

\subsection{Modèle de régression, notion de \og design\fg{}}

\begin{frame}
\frametitle{Influence d'une variable sur une autre}
\begin{itemize}
\item \underline{Principe} : on part de {\color{red}l'observation} d'un $n$-échantillon
$$Y_1,\ldots, Y_n\;\;\;(Y_i \in \R)$$
\item A chaque observation $Y_i$ est associée une {\color{red} observation auxiliaire} $\bX_i {\color{red} \in \R^k}$.
\item On {\color{red}suspecte} l'échantillon
$$\bX_1,\ldots, \bX_n\;\;\;{\color{red} (\bX_i \in \R^k)}$$
de contenir la \og majeure partie de la variabilité des $Y_i$ \fg{}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modélisation de l'influence}
\begin{itemize}
\item Si $\bX_i$ contient {\color{red}toute la variabilité} de $Y_i$, alors $Y_i$ est mesurable par rapport à $\bX_i$ : il existe $r:\R^k\rightarrow \R$ telle que
$$Y_i = r\big(\bX_i\big),$$
mais peu réaliste (ou alors {\color{red}problème d'interpolation
numérique}).
\item \underline{Alternative} : représentation précédente avec {\color{red} erreur additive} : on {\color{red} postule}
$$Y_i = r\big(\bX_i\big)+\xi_i,$$
$\xi_i$ erreur al\'eatoire centrée (pour des raisons
d'identifiabilité).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Motivation: meilleure approximation $L^2$}
\begin{itemize}
\item \underline{Meilleure approximation $L^2$}. Si
$\E\big[Y^2\big]<+\infty$, la meilleure approximation de $Y$ par une
variable aléatoire $\bX$-mesurable est donnée par
{\color{red}l'espérance conditionnelle} $\E\big[Y|\bX\big]$ :
$$\E\big[\big(Y-r(\bX)\big)^2\big] = \min_h \E\big[\big(Y-h(\bX)\big)^2\big]$$
\item o\`u
$$r(\bx) = \E\big[Y|\bX=\bx\big],\;\;\bx \in \R^k.$$
\item On appelle $r(\cdot)$ {\bf fonction de r\'egression de $Y$ sur
$\bX$}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Régression}
\begin{itemize}
\item On d\'efinit:
$$\xi = Y-\E\big[Y|\bX \big]\;\;\Longrightarrow\; \E\big[\xi\big]=0.$$
\item On a alors naturellement la repr\'esentation d\'esir\'ee
$$Y=r(\bX)+\xi, \quad \E\big[\xi\big]=0$$
si l'on pose
$$\boxed{r(\bx) = \E\big[Y|\bX=\bx\big],\;\;\bx \in \R^k}$$

\item On observe alors un $n$-échantillon
$$(\bX_1,Y_1),\ldots, (\bX_n,Y_n)$$
où
$$Y_i = r(\bX_i)+\xi_i,\;\;\E\big[\xi_i\big]=0$$
avec comme {\color{red}paramètre la fonction $r(\cdot)$}+ un {\color{red}jeu d'hypothèses} sur la loi des $\xi_i$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle de régression à design aléatoire}
\begin{df}
Modèle de régression {\bf à design aléatoire} = donnée de
l'observation
$$(\bX_1,Y_1),\ldots, (\bX_n,Y_n)$$
avec $(Y_i,\bX_i)\in \R\times \R^k$ {\color{red}i.i.d.},
et\\\vspace{3mm} \centerline{$Y_i =
r({\color{red}\vartheta},\bX_i)+\xi_i,\;\;
\E\big[\xi_i|\bX_i\big]=0,\;\;{\color{red}
\vartheta \in \Theta \subset \R^d}.$}
\begin{itemize}
\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
$\vartheta$ près.
\item $\bX_i$ = variables explicatives, co-variables, pr\'edicteurs;
$(\bX_1,\ldots,\bX_n)$ = {\bf design}.
\end{itemize}
\end{df}
\end{frame}

%\begin{frame}
%\frametitle{Remarques}
%\begin{itemize}
%\item Si $(X_i,Y_i )\in \R \times \R$ {\color{red}estimer} $\vartheta$ revient à {\color{red} rechercher} la fonction dans la famille $\{r(\vartheta,\cdot),\vartheta \in \Theta\}$ qui approche le mieux les points $(X_i,Y_i)$ pour un certain {\color{red}critère à définir}\vspace{2mm}.
%(Pb d'approximation, {\color{red}pas} d'interpolation).
%\item Si $Y_i \in \{0,1\}$ modèle à {\color{red} réponse binaire}.
%
%\underline{Exemple} : $Y_i$ = présence/absence d'une maladie chez l'individu $i$, et $\bX_i$ = vecteur de marqueurs biologiques.
%\item Dans cette acceptation du modèle, {\color{red} le statisticien ne choisit pas la valeur des covariables}.
%\end{itemize}
%\end{frame}

\subsection{R\'egression à design déterministe}


\begin{frame}
\frametitle{Modèle alternatif : signal+bruit}
\begin{itemize}
\item \underline{Principe} : {\color{red}sur un exemple}. On observe
$$Y_i = r(\vartheta, i/n)+\xi_i,\;\;i=1,\ldots,n$$
où $r({\color{red}\vartheta},\cdot):[0,1]\rightarrow \R$ est une
fonction connue au paramètre ${\color{red}\vartheta \in \Theta
\subset \R^d}$ près, et les $\xi_i$ sont i.i.d.,
$\E\big[\xi_i\big]=0$.
\item {\color{red} But} : reconstruire $r({\color{red}\vartheta},\cdot)$ c'est-à-dire {\color{red} estimer $\vartheta$}.
\item Plus généralement, on observe
$$Y_i = r(\vartheta, \bx_i)+\xi_i,\;i=1,\ldots, n$$
où $\bx_1,\ldots, \bx_n$ sont des points de $\R^k$ {\color{red}
d\'eterministes}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle de régression à design déterministe}
\begin{df}
Modèle de régression {\bf à design déterministe} = donnée de
l'observation
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
avec $Y_i \in \R, \bx_i\in \R^k$, et \\\vspace{3mm} \centerline{$Y_i
=
r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;\E\big[\xi_i\big]=0,\;\;{\color{red}
\vartheta \in \Theta \subset \R^d}.$}
\begin{itemize}
%\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
%$\vartheta$ près.
\item $\bx_i$ déterministes, donnés (ou choisis) : plan d'expérience, points du \og design\fg{}.
\item Hypothèses sur les $\xi_i$ : à débattre. {\color{red}Pour simplifier}, les $\xi_i$ sont i.i.d. {\color{red} (hypothèse restrictive)}.
\item $\Longrightarrow$ les $Y_i$ ne sont {\color{red}pas i.i.d.}
\end{itemize}
\end{df}

 \underline{Question}: Comment estimer $\theta$ dans ce mod\`ele?

\end{frame}

\begin{frame}
\frametitle{Régression gaussienne}


\begin{itemize}
\item  Modèle de régression à design déterministe :
$$Y_i =
r({\vartheta},\bx_i)+\xi_i,\;\;\vartheta \in \Theta\subset  \R^d.$$
\item  Supposons: $\xi_i \sim {\mathcal N}(0,\sigma^2)$, i.i.d.
\item On a alors le mod\`ele de {\color{red}régression gaussienne}.
Comment estimer $\vartheta$?  {\color{red}On sait explicier la loi
de l'observation} $Z=(Y_1,\dots,Y_n)$ $\Longrightarrow$ appliquer le
principe du maximum de vraisemblance.

\item La loi de $Y_i$:
\begin{align*}
\PP^{Y_i}(dy) & = \frac{1}{\sqrt{2\pi \sigma^2}}\exp\big
(-\frac{1}{2\sigma^2}(y-r({\vartheta},\bx_i))^2\big)dy \\
& \ll dy.
\end{align*}

%d'où
%$$\PP^{(Y_1,\ldots, Y_n)}(dy_1\ldots dy_n) = \Big(\prod_{i=1}^n \frac{1}{\sqrt{2\pi \sigma^2}}\exp\big(-\frac{1}{2\sigma^2}(y-\vartheta^T\bx_i)\big)\Big) dy_1\ldots dy_n$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{EMV pour régression gaussienne}
\begin{itemize}
\item  Le modèle $\{\PP_\vartheta^n {\color{red} = \text{loi de }\;(Y_1,\ldots, Y_n)},\vartheta \in \R^k\}$ est {\color{red}dominé} par
$\mu^n(dy_1\ldots dy_n) = dy_1\ldots dy_n.$
\item D'où
\begin{align*}
 \frac{d\PP_\vartheta^n}{d\mu^n}(y_1,\ldots, y_n)
  =\; &\prod_{i=1}^n \tfrac{1}{\sqrt{2\pi \sigma^2}}\exp
  \big(-\tfrac{1}{2\sigma^2}(y_i-r({\vartheta},\bx_i))^2\big) \\
\;= & \tfrac{1}{(\sqrt{2\pi \sigma^2})^{n}}
\exp\big(-\tfrac{1}{2\sigma^2}\sum_{i =
1}^n\big(y_i-r({\vartheta},\bx_i)\big)^2\big).
\end{align*}
\item La fonction de vraisemblance
$$\boxed{{\mathcal L}_n(\vartheta, Y_1,\ldots, Y_n)
\propto \exp\Big(-\frac{1}{2\sigma^2}\sum_{i = 1}^n\big(Y_i-
r({\vartheta},\bx_i)\big)^2\Big)}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Estimateur des moindres carrés} Maximiser la
{\color{red} vraisemblance} en r\'egression gaussienne = minimiser
la somme des carr\'es: $$ \sum_{i = 1}^n
\big(Y_i-r({\vartheta},\bx_i)\big)^2 \to \min_{\vartheta \in\Theta}.
$$
\begin{df}
Estimateur des {\color{red}moindres carrés} : tout estimateur
$\estMC$ t.q. \centerline{$\estMC \in \arg \min_{\vartheta \in
\Theta}\sum_{i = 1}^n \big(Y_i-r({\vartheta},\bx_i)\big)^2.$}
\end{df}
\begin{itemize}
\item  L'EMC est un M-estimateur. Pour le
mod\`ele de r\'egression gaussienne: $\boxed{\rm EMV = EMC}$.
\item {\color{red} Existence, unicit\'e.}
\item Propri\'et\'es remarquables si la r\'egression est lin\'eaire:
$r({\vartheta},\bx_i) = \vartheta^T\bx_i$.
\end{itemize}
\end{frame}

\subsection{La droite des moindres carrés}

\begin{frame}
\frametitle{Droite de régression}
\begin{itemize}
\item \underline{Modèle le plus simple}
$\boxed{r(\vartheta,x)=a +bx}$
$$\boxed{Y_i = a\,+ b\, x_i+\xi_i,\;\;i=1,\ldots,n}$$
avec ${\color{red}\vartheta = (a,b)^T \in \Theta = \R^2}$ et les
$(x_1,\ldots, x_n)$ donnés.
%\item Erreurs centr\'ees: $\E\big[\xi_i\big]=0$, de variances $\E\big[\xi_i^2\big]$ finies.
\item L'estimateur des moindres carrés:
$${\color{red}\estMC}=(\hat a, \hat b) =
\arg \min_{(a,b)\in \R^2}\sum_{i = 1}^n \big(Y_i-a - b x_i\big)^2.$$
%\item \underline{Résidu} : si $\est$ est un estimateur de $\vartheta$
%$$\widehat \xi_i = Y_i - r(\est, x_i)\;\;\text{{\color{red}résidu} au point}\;i.$$
%\item \underline{RSS} : (Residual Sum of Squares)
%$$\|\widehat \xi\|^2 =\|\widehat \xi(\est)\|^2 = \sum_{i = 1}^n\big(Y_i - r(\est,x_i)\big)^2.$$
\item {\color{red}Solution explicite} existe toujours, sauf cas pathologique quand tous les $x_i$ sont les
m\^emes (Poly, page 112).
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Estimateur des moindres carrés}
%\begin{df}
%\end{df}
%\begin{itemize}
%\item Solution {\color{red}explicite}
%$$\boxed{\estMC=(\widehat \vartheta_{n,0}^{\,{\tt mc}},\widehat \vartheta_{n,1}^{\,{\tt mc}})=\Big(\overline{Y}_n-\widehat \vartheta_{n,1}^{\,{\tt mc}}\overline{x}_n, \frac{\langle x_\cdot-\overline{x}_n, Y_\cdot -\overline{Y}_n\rangle_n}{\|x_\cdot -\overline{x}_n\|_n^2}\Big)}$$
%$\langle u_\cdot,v\cdot\rangle_n = \sum_{i = 1}^n u_iv_i$. (on
%retrouvera directement ces formules...)
%\end{itemize}
%\end{frame}


\frame{ \frametitle{R\'egression lin\'eaire simple}
  \begin{figure}[h]
\begin{center}
\includegraphics[height=\textheight]{x-y1.eps}
\end{center}
\end{figure}
}

\frame{

\frametitle{R\'egression lin\'eaire simple}
  \begin{figure}[h]
\begin{center}
\includegraphics[height=\textheight]{x-y2.eps}
\end{center}
\end{figure}
}

\subsection{R\'egression lin\'eaire multiple}

\begin{frame}
\frametitle{R\'egression lin\'eaire multiple (=Modèle linéaire)}
\begin{itemize}
\item La fonction de r\'egression est $r(\vartheta,\bx_i) = \vartheta^T\bx_i$.
On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
avec
$$\boxed{Y_i = \vartheta^T\bx_i+\xi_i,\;\;i=1,\ldots, n}$$
où $\vartheta \in \Theta = \R^k,\;\;\bx_i \in \R^k$.
\item {\color{red}Matriciellement}
$$\boxed{\boldsymbol{Y} = \mathbb{M}\vartheta + \boldsymbol{\xi}}$$
avec $\boldsymbol{Y} = (Y_1,\ldots, Y_n)^T$, $\boldsymbol{\xi} =
(\xi_1,\ldots, \xi_n)^T$ et $\mathbb{M}$ la matrice $(n\times k)$
dont les {\color{red} lignes} sont les $\bx_i$.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Réduction \og design\fg{} aléatoire $\longrightarrow$ déterministe}
%\begin{itemize}
%\item Les modèles de régression à \og design\fg{} déterministe ou aléatoire se traitent  {\color{red} essentiellement de la même manière} :
%\end{itemize}
%\begin{hypothese}[Ancillarité des covariables]
%On suppose que la loi $\PP^{\bX}$ des $\bX_i$ ne dépend pas du paramètre inconnu ${\color{red}\vartheta}$.
%\end{hypothese}
%\begin{itemize}
%\item Sous l'hypothèse d'ancillarité, le caractère aléatoire des $\bX_i$ -- observés -- ne joue aucun rôle : on peut faire l'étude mathématique du modèle {\color{red}conditionnellement aux $\bX_i$}.
%\item {\color{red} Désormais} : on se place  dans le modèle de régression à \og design \fg{} {\color{red}déterministe}.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{\og design\fg{} aléatoire vs. déterministe}
%\begin{remarque}
%{\it A posteriori} pourquoi considérer le modèle de régression à \og design \fg{} aléatoire et ne pas se placer d'emblée en signal + bruit ?
%\begin{itemize}
%\item {\color{red}Le design aléatoire} fournit une interprétation en terme de fonction de régression obtenue via l'espérance conditionnelle.
%\item {\color{red}Essentiel} pour le traitement des {\color{red}modèles à réponse binaire} (ou multiple), voir plus loin.
%\end{itemize}
%\end{remarque}
%\end{frame}





%\begin{frame}
%\frametitle{Estimation de $\sigma^2$}
%\begin{itemize}
%\item {\color{red}Estimation de $\sigma$} (ou $\sigma^2$) à partir des observations
%$$\boxed{Y_i = \vartheta_0\,+\vartheta_1\,x_i+{\color{red}\sigma}\, \varepsilon_i,\;\;i=1,\ldots,n}$$
%{\color{red}avec}
%$$\boxed{\E_\vartheta\big[\varepsilon_i\big]=0,\;\E_\vartheta\big[\varepsilon_i^2\big]=1}$$
%\item \underline{Estimateur naturel} de $\sigma^2$ :
%$$\widehat \sigma_n^2 = \frac{1}{n}\sum_{i = 1}^n\big(Y_i-r(\estMC, x_i)\big)^2$$
%\item Somme de variables aléatoires {\color{red}non indépendantes}.
%\item Difficile de progresser {\color{red}sans hypothèse supplémentaire}. Si les $\varepsilon_i$ sont i.i.d. ${\mathcal N}(0,1)$, alors {\color{red}on sait} \og résoudre \fg{} le problème... plus loin.
%\end{itemize}
%\end{frame}





%\subsection{EMV et EMC}

\begin{frame}
\frametitle{EMC en régression linéaire multiple}
\begin{itemize}
\item Estimateur des {\color{red}moindres carrés} en régression
linéaire multiple : tout estimateur $\estMC$ satisfaisant
$$\sum_{i = 1}^n
\big(Y_i-(\estMC)^T\bx_i\big)^2 = \min_{\vartheta \in \R^k}\sum_{i =
1}^n \big(Y_i-{\vartheta}^T\bx_i\big)^2.$$
\item En notations matricielles :
\begin{eqnarray*} \|\boldsymbol{Y}-\design\estMC\|^2 &=& \min_{\vartheta \in
\R^k}\|\boldsymbol{Y}-\design\vartheta\|^2\\
&=& \min_{v \in V}\|\boldsymbol{Y}-v\|^2
\end{eqnarray*}
o\`u $V=\text{Im}(\design) = \{v\in \R^n: v=\design\vartheta, \
\vartheta\in \R^k\}$.
 Projection orthogonale sur $V$.
 \end{itemize}
 \end{frame}
 
 
 \end{document}
 

 \subsection{G\'eometrie de l'EMC}

 \begin{frame}
\frametitle{G\'eometrie de l'EMC}
 \begin{itemize}
 \item L'EMC vérifie
$$\boxed{\design {\estMC} = P_V \boldsymbol{Y}}$$
o\`u $P_V$ est le projecteur orthogonal sur $V$.
\item Mais $\design^T  P_V= \design^T  P_V^T = ( P_V\design)^T =
\design^T$. On en d\'eduit {\color{red}les \'equations normales des
moindres carr\'es}:
$$\boxed{\design^T\design {\estMC} =
\design^T\boldsymbol{Y}.}$$
\item \underline{Remarques.}
  \begin{itemize}
  \item L'EMC est un $Z$-estimateur.
  \item Pas d'{\color{red}unicit\'e} de $\estMC$ si la matrice
  $\design^T\design$ n'est pas inversible.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame} \frametitle{G\'eometrie de l'EMC}
\begin{prop}
Si $\design^T\design$ (matrice $k \times k$) inversible, alors
$\estMC$ {\color{red}est unique} et
$$\boxed{\estMC = \big(\design^T\design\big)^{-1}\design^T \boldsymbol{Y}}$$
\end{prop}
\begin{itemize}
\item Contient le cas précédent de la droite de régression simple.
\item Résultat g\'eometrique, {\color{red}non stochastique}.
\item $\design^T\design\ge0$; \ \ $\design^T\design$
inversible $\Longleftrightarrow$ $\design^T\design>0$;
$$\design^T\design>0 \ \Longleftrightarrow \ {\rm rang}(\design)=k
\ \Longleftrightarrow \ {\rm dim}(V)=k.$$
$$\design^T\design>0 \quad \Longrightarrow \quad {\color{red} n \geq k}.$$
\end{itemize}
\end{frame}


\begin{frame} \frametitle{G\'eom\'etrie de l'EMC}
Soit $\design^T\design>0$. Alors, la matrice $n\times n$
$$
A = \design\big(\design^T\design\big)^{-1}\design^T
$$
est dite {\bf matrice chapeau} (\texttt{hat matrix}).
%
\begin{prop}
Si $\design^T\design>0$, alors $A$ est le projecteur sur
$V$:\\\vspace{2mm} \centerline{$A=P_V$} et ${\rm rang}(A)=k$.
\end{prop}
\underline{Preuve} :
\begin{itemize}
\item  $A=A^T$, $A=A^2$, donc $A$ est un
projecteur.
\item ${\rm Im}(A) = V$, donc $A=P_V$; \
${\rm rang}(P_V)={\rm dim}(V)=k$.
\end{itemize}
{\color{red}\og Chapeau \fg{}}, car $A$ g\'en\`ere la pr\'evison de
$\design\vartheta$ not\'ee $\widehat{\boldsymbol{Y}}$ :
$$\widehat{\boldsymbol{Y}}= \design\estMC= A\boldsymbol{Y}.$$
\end{frame}

\subsection{Propriétés statistiques de l'EMC : cas gaussien}


\begin{frame}
\frametitle{Cadre gaussien : loi des estimateurs}
\begin{itemize}
\item \underline{Hyp. 1} : $\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2\mathrm{Id}_n)$.
\item \underline{Hyp. 2} : $\design^T \design>0$.
\end{itemize}
\begin{prop}
\begin{itemize}
\item[(i)] $\estMC \sim {\mathcal N}\big(\vartheta, \sigma^2 \big(\design^T\design\big)^{-1}\big)$
\item[(ii)] $\|\boldsymbol{Y}-\design \estMC\|^2 \sim \sigma^2\chi^2(n-k)$ {\color{red} loi du Chi 2 à $n-k$ degrés de liberté}
\item[(iii)] $\estMC$ et $\boldsymbol{Y}-\design \estMC$ sont indépendants.
\end{itemize}
\end{prop}
\begin{itemize}
\item \underline{Preuve} : {\color{red}Thm. de Cochran} (Poly, page 18). Si
$\boldsymbol{\xi}\sim {\mathcal N}(0,\mathrm{Id}_n)$ et $A_j$
matrices $n \times n$ projecteurs t.q. $A_jA_i=0$ pour $i\neq j$,
alors : $A_j\,\boldsymbol{\xi} \sim {\mathcal N}\big(0,A_j\big)$,
{\color{red}indépendants}, $\|A_j\boldsymbol{\xi}\|^2\sim
\chi^2(\mathrm{Rang}(A_j))$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de la proposition}
\begin{itemize}
\item (i)
$\estMC = \vartheta + \big(\design^T\design\big)^{-1}\design^T
\boldsymbol{\xi}$.

On vérifie: $\ \ \E[\estMC]=\vartheta$,
\begin{align*}
&\E\big[ \big(\design^T\design\big)^{-1}\design^T \boldsymbol{\xi} \big(\big(\design^T\design\big)^{-1}\design^T \boldsymbol{\xi}\big)^T\big] \\
=\; &\sigma^2\big(\design^T\design\big)^{-1}.
\end{align*}
\item (ii)
\begin{align*}
\boldsymbol{Y}-\design \estMC & = \design\big(\vartheta - \estMC\big)+\boldsymbol{\xi} \\
& = -\design\big(\design^T\design\big)^{-1}\design^T\boldsymbol{\xi}+\boldsymbol{\xi} \\
& =
\sigma(\text{Id}_n-A)\boldsymbol{\xi}',\;\boldsymbol{\xi}'\sim{\mathcal
N}(0,\mathrm{Id}_n).
\end{align*}
\item (iii) le vecteur $(\estMC,\boldsymbol{Y}-\design \estMC)$ est gaussien. On calcule explicitement sa matrice de variance-covariance.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Propriétés de l'EMC}
%\begin{itemize}
%\item \underline{Hyp. 1} : $\design^T \design>0$.
%\item  \underline{Hyp. 2} : {\color{red}$\E\big[\boldsymbol{\xi}\big]=0$, $\E\big[\boldsymbol{\xi}\boldsymbol{\xi}^T\big] = \sigma^2 \mathrm{Id}_n$}.
%\end{itemize}
%\begin{prop}
%%Sous les hypothèses précédentes
%\begin{itemize}
%\item $\E_\vartheta\big[\estMC\big]=\vartheta$ et
%$$\E_\vartheta\big[\big(\estMC-\vartheta\big)\big(\estMC-\vartheta\big)^T\big]=\sigma^2 \big(\design^T\design\big)^{-1}$$
%\item Si l'on pose
\begin{itemize}
\item Estimateur de la variance $\sigma^2$:
$$\boxed{\widehat \sigma_n^2 = \frac{\|\boldsymbol{Y}-\design \estMC\|^2}{n-{\color{red}k}} = \frac{1}{n-{\color{red}k}}\sum_{i = 1}^n\big(Y_i-(\estMC)^T\bx_i\big)^2}$$
C'est un estimateur sans biais: $\E_\vartheta\big[\widehat
\sigma_n^2\big]=\sigma^2.$
\item Lois des coordonn\'ees de $\estMC$:
$$
(\estMC)_j -\vartheta_j \sim {\mathcal N}\big(0, \sigma^2 b_j)
$$
o\`u $b_j$ est le $j$\`eme \'el\'ement diagonal de $\big(\design^T
\design\big)^{-1}$. $$ \frac{(\estMC)_j -\vartheta_j}{\widehat
\sigma_n \sqrt{b_j}} \sim t_{n-k}$$ {\color{red}loi de Student \`a
$n-k$ degr\'es de libert\'e}.
\end{itemize}
%\end{prop}
\end{frame}

\begin{frame}
    \frametitle{Exemple de donn\'ees de r\'egression}
\begin{center}
\vspace{-2.5cm}
\includegraphics[height=2\textheight]{cours4_data1.pdf}
\end{center}
\end{frame}

\begin{frame}
\frametitle{R\'esultats de traitement statistique initial}
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline
(Intercept) &$152.133$&$2.576$&$59.061$&$< 2e-16***$\\
age&$-10.012$&$59.749$&$ -0.168$&$0.867000$\\\hline
sex &$-239.819$&$61.222$&$-3.917$&$0.000104***$\\
bmi&$519.840$&$66.534$&$7.813$&$4.30e-14***$\\\hline
map&$324.390$&$65.422$&$4.958$&$1.02e-06***$\\
tc&$-792.184$&$416.684$&$-1.901$&$0.057947$\\\hline
ldl&$476.746$&$339.035$&$1.406$&$0.160389$\\
hdl&$101.045$&$212.533 $&$0.475$&$0.634721$\\\hline
tch&$177.064$&$161.476$&$ 1.097$&$0.273456$\\
ltg&$751.279$&$ 171.902$&$4.370$&$ 1.56e-05***$\\\hline
glu&$67.625$&$ 65.984$&$1.025$&$0.305998$\\\hline
\end{tabular}
\end{frame}



%\begin{frame}
%\frametitle{Exemple de donn\'ees de r\'egression}
%\begin{center}
%\begin{footnotesize}
%\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c||c|}
%\hline
%Patient&age&sex&bmi&map&tc&ldl&hdl&tch&ltg&glu&Response\\\hline
%1&59&2&32.1&101&157&93.2&38&4&4.9&87&151\\
%2&48&1&21.6&87&183&103.2&70&3&3.9&69&75\\
%3&72&2&30.5&93&156&93.6&41&4&4.7&85&141\\
%4&24&1&25.3&84&198&131.4&40&5&4.9&89&206\\
%5&50&1&23.0&101&192&125.4&52&4&4.3&80&135\\
%6&23&1&22.6&89&139&64.8&61&2&4.2&68&97\\
%\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots&\vdots\\
%441&36&1&30.0&95&201&125.2&42&5&5.1&82&220\\
%442&36&1&19.6&71&250&132.2&97&3&4.6&92&57\\\hline
%\end{tabular}
%%\end{table}
%\end{footnotesize}
%\end{center}
%\end{frame}
%%\begin{frame}
%%\frametitle{Limites des moindres carrés et du cadre gaussien}
%%\begin{itemize}
%%\item {\color{red}EMC} : limité à une fonction de régression {\color{red}linéaire}.
%%\item {\color{red} hypothèse de gaussianité} : cadre asymptotique implicite.
%%\item Besoin d'outils pour les modèles {\color{red} à réponse $Y$ discrète}.
%%\item Hors cadre gaussien, le choix du critère quadratique est discutable {\color{red} régression robuste... attendre}.
%%\end{itemize}
%%\end{frame}

%\subsection{Régression non-linéaire}
%
%\begin{frame}
%\frametitle{Régression non-linéaire}
%\begin{itemize}
%\item On observe
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),$$
%où
%$$\boxed{Y_i = r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;i=1,\ldots,n}$$
%avec
%$$\bx_i\in \R^k,\;\;\text{et}\;\; {\color{red}\vartheta \in \Theta \subset \R^d}.$$
%\item Si $\xi_i \sim_{\text{i.i.d.}} {\mathcal N}(0,\sigma^2)$,
%$${\mathcal L}_n(\vartheta, Y_1,\ldots, Y_n) \propto \exp\Big(-\frac{1}{2\sigma^2}\sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2\Big)$$
%et l'estimateur du {\color{red}maximum de vraisemblance} est obtenu en minimisant la fonction
%$$\vartheta \leadsto \sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2.$$
%\end{itemize}
%\end{frame}

%\begin{frame}
%\frametitle{Moindre carrés non-linéaires}
%\begin{df}
%\begin{itemize}
%\item $M$-estimateur associé à la {\color{red}fonction de contraste} $\psi:\Theta \times {\color{red}\R^k}\times \R\rightarrow \R$ : tout estimateur $\est$ satisfaisant
%$$\sum_{i = 1}^n \psi(\est, \bx_i, Y_i) = \max_{a \in \Theta} \sum_{i = 1}^n \psi(a,\bx_i,Y_i).$$
%\item Estimateur des {\color{red}moindres carrés non-linéaires} : associé au contraste $\psi(a,\bx,y) = -\big(y-r(a,\bx)\big)^2$.
%\end{itemize}
%\end{df}
%\begin{itemize}
%\item {\color{red}Extension} des résultats en densité $\rightarrow$ théorèmes limites pour des sommes de v.a.  indépendantes {\color{red} non-équidistribuées} (à suivre...)
%\end{itemize}
%\end{frame}

%\begin{frame}
%\frametitle{Modèle à réponse binaire}
%\begin{itemize}
%\item On observe
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),\;\;{\color{red}Y_i \in \{0,1\}},\;\bx_i \in \R^k.$$
%\item Modélisation {\color{red}via la fonction de régression}
%$$\bx \leadsto p_{\bx}(\vartheta) = \E_\vartheta\big[Y|\bX = \bx\big] = \PP_\vartheta\big[Y = 1|\bX=\bx\big]$$
%%$$Y_i = p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big)$$
%\item {\color{red}Représentation}
%\begin{align*}
%Y_i & =  p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big) \\
%& = r(\vartheta,\bx_i)+\xi_i
%\end{align*}
%avec
%$r(\vartheta, \bx_i) = p_{\bx_i}(\vartheta)$ et $\xi_i = Y_i-p_{\bx_i}(\vartheta).$
%\item $\E_\vartheta\big[\xi_i\big]=0$ mais structure des $\xi_i$ {\color{red}compliquée} (dépendance en $\vartheta$).
%\end{itemize}
%\end{frame}

%\begin{frame}
%\frametitle{Modèle à réponse discrète}
%\begin{itemize}
%\item $Y_i $ v.a. de Bernoulli de paramètre $p_{\bx_i}({\color{red}\vartheta})$.
%
%{\color{red} Vraisemblance}
%$${\mathcal L}_n(\vartheta,Y_1,\ldots, Y_n) = \prod_{i = 1}^n p_{\bx_i}({\color{red}\vartheta})^{Y_i}(1-p_{\bx_i}\big({\color{red}\vartheta})\big)^{1-Y_i}$$
%$\rightarrow$ méthodes de résolution numérique.
%\item {\color{red} Régression logistique} (très utile dans les applications)
%$$p_{\bx}(\vartheta) = \psi(\bx_i^T\vartheta),$$
%$$\psi(x)=\frac{e^x}{1+e^x}\;\;{\color{red}{\text{fonction logistique}}}.$$
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Régression logistique et modèles latents}
%\begin{itemize}
%\item {\color{red}Représentation équivalente de la régression logistique} : on observe
%$$\boxed{Y_i = 1_{\big\{Y_i^\star >0\big\}},\;\;i=1,\ldots,n}$$
%(les $\bx_i$ sont donnés), et $Y_i^\star$ est une  {\color{red}variable latente} ou cachée,
%$$\boxed{Y^\star_i ={\color{red}\vartheta}^T \bx_i + U_i,\;\;i=1,\ldots, n}$$
%avec {\color{red}$U_i\sim_{\text{i.i.d.}} F$}, où
%$$F(x) = \frac{1}{1+e^{-x}}.$$
%\item
%\begin{align*}
%\PP_\vartheta\big[Y_i^\star>0] & = \PP_\vartheta\big[\bx_i^T\vartheta + U_i >0\big] \\
%& = 1-\PP_\vartheta\big[U_i \leq -\bx_i^T\vartheta\big] \\
%& = 1-\big(1+\exp(-\bx_i^T\vartheta)\big)^{-1} =  \psi(\bx_i^T\vartheta).
%\end{align*}
%\end{itemize}
%\end{frame}
%
%










\end{document}
