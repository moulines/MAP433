
\documentclass{beamer}
\usetheme[right,hideothersubsections]{Berkeley}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext}
\usepackage{amsfonts}

%\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[applemac]{inputenc}


\newenvironment{disarray}{\everymath{\displaystyle\everymath{}}\array} {\endarray}
\newtheorem{theo}{Théorème}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{cor}{Corollary}[theo]
\newtheorem{lm}{Lemma}
\newtheorem{lemme}{Lemme}
\newtheorem{nota}{Notation}
\newtheorem{rk}{Remark}
\newtheorem{remarque}{Remarque}
\newtheorem{exa}{Example}
\newtheorem{df}{Définition}
\newtheorem{hypothese}{Hypothèse}
\newtheorem{terminologie}{Terminologie}
\newenvironment{dem}{\textbf{Proof}}{\flushright$\blacksquare$\\}

\DeclareMathOperator{\E}{{\mathbb E}}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\IF}{{\mathbb I}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Pour le modèle linéaire

\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bx}{\boldsymbol{x}}
\DeclareMathOperator{\vp}{\boldsymbol{p}}
\DeclareMathOperator{\vq}{\boldsymbol{q}}
\DeclareMathOperator{\estMC}{\widehat \vartheta_n^{\,\,{\tt mc}}}
\DeclareMathOperator{\estMCNL}{\widehat \vartheta_n^{\,\,{\tt mcnl}}}
\DeclareMathOperator{\estMV}{\widehat \vartheta_n^{\,\,{\tt mv}}}
\DeclareMathOperator{\design}{\mathbb{M}}
\DeclareMathOperator{\est}{\widehat \vartheta_{\mathnormal{n}}}
\DeclareMathOperator{\var}{\mathrm{Var}}
\DeclareMathOperator{\estMVc}{\widehat \vartheta_{n,0}^{\,{\tt mv}}}
\DeclareMathOperator{\Xbar}{\overline{\mathnormal{X}}_\mathnormal{n}}




\title{MAP 433 : Introduction aux méthodes statistiques. Cours 7}
\author{M. Hoffmann}
%\institute{Université Paris Dauphine}
\begin{document}
\date{28 mars 2014}
\maketitle



\begin{frame} 
\frametitle{Aujourd'hui} 
\tableofcontents
\end{frame}

\section{Estimation optimale : approche asymptotique}

\begin{frame}
\frametitle{Approche asymptotique}
\begin{itemize}
\item Hypothèse simplificatrice : $\vartheta \in \Theta\; {\color{red}\subset \R}$. On se restreint aux {\color{red} estimateurs asymptotiquement normaux} c'est-à-dire vérifiant
$$\sqrt{n}\big(\est - \vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0,v(\vartheta)\big)$$
cf. théorèmes limites obtenus pour les $Z$-,$M$-estimateurs.
\item Si $\widehat \vartheta_{n,1}$ et $\widehat \vartheta_{n,2}$ as. normaux de variance asymptotique
 $v_1(\vartheta) \leq v_2(\vartheta),$ alors la précision de $\widehat \vartheta_{n,1}$ est {\color{red}asymptotiquement meilleure} que celle de $\widehat \vartheta_{n,2}$ au point $\vartheta$ : 
\begin{align*}
\widehat \vartheta_{n,1} & = \vartheta+\sqrt{\frac{{\color{red}v_1(\vartheta)}}{n}}\xi^{(n)}\\
\widehat \vartheta_{n,2} & = \vartheta+\sqrt{\frac{{\color{red}v_2(\vartheta)}}{n}}\zeta^{(n)}
\end{align*}
où $\xi^{(n)}$ et $\zeta^{(n)} \stackrel{d}{\rightarrow} {\mathcal N}(0,1)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Comparaison d'estimateurs : cas asymptotique}
\begin{itemize}
\item Si $v_1(\vartheta) < v_2(\vartheta)$, et si $\vartheta \leadsto v_i(\vartheta)$ est continue, on pose
$${\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,i}) = \left[\widehat \vartheta_{n,i}\pm\sqrt{\frac{v_i(\widehat \vartheta_{n,i})}{n}}\Phi^{-1}(1-\alpha/2)\right],\;\;i=1,2$$
où $\alpha \in (0,1)$ et $\Phi(\cdot)$ est la fonction de répartition de la loi normale standard.
\item ${\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,i})$, $i=1,2$ sont deux {\color{red}intervalles de confiance asymptotiquement de niveau $1-\alpha$} et on a
$$\frac{|{\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,1})|}{|{\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,2})|} \stackrel{\PP_\vartheta^n}{\longrightarrow} \sqrt{\frac{v_1(\vartheta)}{v_2(\vartheta)}}<1.$$
\item La notion de {\color{red} longueur minimale possible d'un intervalle de confiance} est en général difficile à manipuler.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion provisoire}
\begin{itemize}
\item Il est {\color{red}difficile en général} de comparer des estimateurs.
\item Cadre asymptotique + normalité asymptotique $\rightarrow$ comparaison de la {\color{red}variance asymptotique} $\vartheta \leadsto v(\vartheta)$.
\item Sous des hypothèses de
régularité du modèle $\{\PP_\vartheta^n, \vartheta \in \Theta\}$
% + restriction de la classe des estimateurs 
%\end{itemize}
alors 
%(cours 6)
\begin{itemize}
\item Il {\color{red}existe} une variance asymptotique $v^\star(\vartheta)$ {\color{red}minimale} parmi les variances de la classe des $M$-estimateurs as. normaux. 
\item Cette fonction est associée à une {\color{red}quantité d'information intrinsèque} au modèle.
\item La variance asymptotique de l'{\color{red}EMV} est $v^\star(\vartheta)$.
\end{itemize}
\item Ceci règle {\color{red}partiellement} le problème de l'optimalité.
% $\rightarrow$ notion de super-efficacité. 
\end{itemize}
\end{frame}

\section{Modèles réguliers et information de Fisher}

\begin{frame}
\frametitle{Régularité d'un modèle statistique et information}
\begin{itemize}
\item \underline{Cadre simplificateur} : modèle de densité
$$X_1,\ldots, X_n\;\;\text{i.i.d. de loi}\;\; \PP_\vartheta$$
dans la famille $\big\{\PP_\vartheta, \vartheta \in \Theta\big\}$ avec $\Theta {\color{red}\; \subset \R}$ pour simplifier.
\item \underline{Notation} : 
$$f(\vartheta, x) = \frac{d\PP_\vartheta}{d\mu}(x),\;\;x\in \R,\vartheta \in \Theta.$$
\item \underline{{\color{red}Hypothèse}} : la quantité
$${\color{red}\boxed{{\mathbb I}(\vartheta) = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]}}$$
est bien définie.
\end{itemize}
\end{frame}

\subsection{Construction de l'information de Fisher}

\begin{frame}
\frametitle{Information de Fisher}
\begin{df}
\begin{itemize}
\item $\mathbb{I}(\vartheta) = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]$ s'appelle {\color{red}l'information de Fisher} de la famille $\{\PP_\vartheta,\vartheta \in \Theta\}$ au point $\vartheta$. Elle ne dépend pas de la mesure dominante $\mu$. 
\item Le cadre d'intérêt est celui où
$${\color{red}0 < \mathbb{I}(\vartheta)< +\infty.}$$
\item $\mathbb{I}(\vartheta)$ quantifie \og l'information \fg{} qu'apporte chaque observation $X_i$ sur le paramètre $\vartheta$.
\end{itemize}
\end{df}
\underline{Remarque} : on a $\PP_\vartheta\big[f(\vartheta, X)>0\big]=1$, donc la quantité $\log f(\vartheta, X)$ est bien définie. 
\end{frame}

\begin{frame}
\frametitle{Information dans quel sens ? Origine de la notion}
\begin{itemize}
\item Supposons l'EMV $\estMV$ bien défini et {\color{red}convergent}.
\item Supposons l'application $(\vartheta,x) \leadsto f(\vartheta, x)$ possédant {\color{red}toutes les propriétés de régularité et d'intégrabilité} voulues.
\item Alors
$$\boxed{\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0,{\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big)}$$
en loi sous $\PP_\vartheta$, où encore
$$\estMV \stackrel{d}{\approx} \vartheta +\frac{1}{{\color{red}\sqrt{n\mathbb{I}(\vartheta)}}} \,{\mathcal N}(0,1)$$
en loi sous $\PP_\vartheta$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Construction de l'information + jeu d'hypothèses attenant}
\begin{itemize}
\item Heuristique : on établira un jeu d'hypothèses justifiant {\color{red}a posteriori} le raisonnement.
\item \underline{Etape 1} : l'EMV $\estMV$ {\color{red}converge} :
$$\estMV \stackrel{\PP_\vartheta}{\longrightarrow} \vartheta$$
via le théorème de convergence des $M$-estimateurs.
\item \underline{Etape 2} : l'EMV $\estMV$ est un {\color{red}$Z$-estimateur} :
$$0 = \partial_\vartheta \Big(\sum_{i = 1}^n \log f(\vartheta,X_i)\Big)_{\vartheta = {\color{red}\estMV}}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Construction de $\mathbb{I}(\vartheta)$ cont.}
\begin{itemize}
\item \underline{Etape 3} : développement asymptotique {\color{red}autour de $\vartheta$} :
$${\color{red}0}  \approx \sum_{i = 1}^n \partial_\vartheta \log f(\vartheta, X_i) + {\color{red}(\estMV-\vartheta)} \sum_{i = 1}^n \partial^2_\vartheta \log f(\vartheta, X_i),$$
soit
$${\color{red}\estMV-\vartheta} \approx -\frac{\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)}{ \sum_{i = 1}^n {\color{red}\partial^2_\vartheta \log f}({\color{red}\vartheta}, X_i)}$$
%Puis : étude du numérateur et du dénominateur 
\item \underline{Etape 4} : le numérateur. Normalisation et convergence de 
$\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)\;\;?$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Numérateur}
\begin{lemme}
On a $$\E_\vartheta\big[{\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X)\big]=0.$$
\end{lemme}
\begin{proof}[Preuve]
\begin{align*}
\E_\vartheta\big[{\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X)\big] & = \int_{\R} \partial_\vartheta \log f(\vartheta, x) f(\vartheta, x)\mu(dx) \\
& = \int_{\R} \frac{\partial_\vartheta f(\vartheta, x)}{f(\vartheta, x)} f(\vartheta, x) \mu(dx) \\
& = \int_{\R} \partial_\vartheta f(\vartheta, x) \mu(dx) \\
& = \partial_\vartheta \int_{\R} f(\vartheta, x) \mu(dx) = \partial_\vartheta 1 = 0.
\end{align*}
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Dénominateur}
De même $\int_{\R} \partial_\vartheta^2 f(\vartheta, x) \mu(dx)= 0.$
{\color{red}Conséquence} :
 $$\boxed{{\color{red}\mathbb{I}(\vartheta)} = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big] = {\color{red}-\E_\vartheta\big[\partial_\vartheta^2 \log f(\vartheta, X)\big]}}$$
En effet
\begin{align*}
&{\color{red} \E_\vartheta\big[\partial_\vartheta^2 \log f(\vartheta, X)\big]} & =
%& \int_{\R} \partial_\vartheta^2 \log f(\vartheta, x)f(\vartheta, x)\mu(dx) 
\\
& = \int_{\R} \frac{\partial_\vartheta^2 f(\vartheta,x) f(\vartheta, x)-\big(\partial_\vartheta f(\vartheta, x)\big)^2}{f(\vartheta, x)^2}f(\vartheta, x)\mu(dx)\\
&= \int_{\R}\partial_\vartheta^2 f(\vartheta, x)\mu(dx)-\int_{\R} \frac{\big(\partial_\vartheta f(\vartheta, x)\big)^2}{f(\vartheta, x)}\mu(dx) \\
& = 0-\int_{\R} \Big(\frac{\partial_\vartheta f(\vartheta,x)}{f(\vartheta, x)}\Big)^2f(\vartheta, x) \mu(dx) ={\color{red} -\E\big[\big(\partial_\vartheta\log f(\vartheta, X)\big)^2\big]}.
\end{align*}
\end{frame}




\begin{frame}
\frametitle{Conséquences}
\begin{itemize}
\item Les $\partial_\vartheta \log f(\vartheta, X_i)$ sont i.i.d. et $\E_\vartheta \big[\partial_\vartheta \log f(\vartheta, X)\big]=0$. TCL :
\begin{align*}\frac{1}{\sqrt{n}} \sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i) & \stackrel{d}{\longrightarrow} \mathcal N\big(0, {\color{red}\E_\vartheta \big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]}\big)\\
& = {\mathcal N}\big(0,{\color{red}\mathbb{I}(\vartheta)}\big).
\end{align*}
\item Les $\partial_\vartheta^2 \log f(\vartheta, X_i)$ sont i.i.d. LGN :
\begin{align*}
\frac{1}{n}\sum_{i = 1}^n {\color{red}\partial_\vartheta^2 \log f}({\color{red}\vartheta}, X_i) & \stackrel{\PP_\vartheta}{\longrightarrow}
\E_\vartheta\big[{\color{red}\partial_\vartheta^2\log f}({\color{red}\vartheta}, X)\big] \\
&\stackrel{\text{conséquence}}{=}-\mathbb{I}(\vartheta).
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item En combinant les deux estimations + lemme de Slutsky :
\begin{align*}
{\color{red}\sqrt{n}}(\estMV-\vartheta) & \approx -\frac{{\color{red}\frac{1}{\sqrt{n}}}\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)}{{\color{red}\frac{1}{n}} \sum_{i = 1}^n {\color{red}\partial^2_\vartheta \log f}({\color{red}\vartheta}, X_i)}\\
& \stackrel{d}{\longrightarrow} \frac{{\mathcal N}\big(0,{\color{red}\mathbb{I}(\vartheta)}\big)}{{\color{red}\mathbb{I}(\vartheta)}} \\
& \stackrel{\text{loi}}{=} {\mathcal N}\Big(0,{\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big).
\end{align*}
\item Le raisonnement est {\color{red} rigoureux dès lors que} : i) on a la convergence de $\estMV$, ii) on peut justifier le lemme et sa conséquence, iii) $\mathbb{I}(\vartheta)$ est bien définie et non dégénérée et iv) on sait contrôler le terme de reste dans le développement asymptotique, {\color{red}partie la plus difficile}. 
\end{itemize}
\end{frame}

\subsection{Modèle régulier}

\begin{frame}
\frametitle{Modèle régulier}
\begin{df} La famille de densités $\{f(\vartheta,\cdot),\vartheta \in \Theta\}$,  par rapport à la mesure dominante $\mu$, $\Theta \subset \R$, est {\color{red}régulière} si
\begin{itemize}
\item $\Theta$ ouvert et $\{f(\vartheta, \cdot)>0\}=\{f(\vartheta', \cdot)>0\}$, $\forall \vartheta, \vartheta' \in \Theta$.
\item $\mu$-p.p. $\vartheta \leadsto f(\vartheta,\cdot)$, $\vartheta \leadsto \log f(\vartheta,\cdot)$ sont ${\mathcal C}^2$.
 \item $\forall \vartheta \in \Theta, \exists {\mathcal V}_\vartheta \subset \Theta$ t.q. pour $a \in {\mathcal V}_\vartheta$
$$|\partial_a^{2}\log f(a,x)|+|\partial_a \log f(a,x)|+\big(\partial_a\log f(a,x)\big)^2\leq g(x)$$
où
$$\int_{\mathbb{R}}g(x)\sup_{a \in {\mathcal V}(\vartheta)}f(a,x)\mu(dx)<+\infty.$$
\item L'information de Fisher est non-dégénérée :
$$\forall \vartheta \in \Theta,\;\;\mathbb{I}(\vartheta) >0.$$
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Résultat principal}
\begin{prop}
\begin{itemize}
\item Si l'expérience engendrée par l'observation $X_1,\ldots, X_n\sim_{\text{i.i.d.}}\PP_\vartheta$ est associée à une famille de probabilités $\{\PP_\vartheta, \vartheta \in \Theta\}$ sur $\R$ {\color{red} régulière} au sens de la définition précédente, alors
$$\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0,\frac{1}{\mathbb{I}(\vartheta)}\Big).$$
\item Si $\est$ est un $Z$-estimateur {\color{red}régulier} asymptotiquement normal de variance $v(\vartheta)$, alors
$$\forall \vartheta \in \Theta,\;\;v(\vartheta) \geq \frac{1}{\mathbb{I}(\vartheta)}.$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve de la proposition}
\begin{itemize}
\item Le premier point consiste à {\color{red}rendre rigoureux} le raisonnement précédent. {\color{red}Point délicat : } le contrôle du terme de reste.
\item {\color{red}Optimalité de la variance de l'EMV parmi celle des $Z$-estimateurs} : on a vu que si $\est$ est un $Z$-estimateur régulier associé à la fonction $\phi$, alors, sa variance asymptotique $v(\vartheta) = v_\phi(\vartheta)$ vaut
$$v_\phi(\vartheta) = \frac{\E_\vartheta\big[\phi(\vartheta,X)^2\big]}{\big(\E_\vartheta\big[\partial_\vartheta \phi(\vartheta, X)\big]\big)^2}.$$
\item {\color{red}A montrer} : pour toute fonction $\phi$ :
$$\boxed{\frac{\E_\vartheta\big[\phi(\vartheta,X)^2\big]}{\big(\E_\vartheta\big[\partial_\vartheta \phi(\vartheta, X)\big]\big)^2} \geq \frac{1}{\mathbb{I}(\vartheta)}}.$$ 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de l'inégalité}
\begin{itemize}
\item Par construction 
$$\partial_a\E_\vartheta\big[\phi(a,X)\big]_{\big|a=\vartheta}=0.$$
\item (avec $\dot\phi(\vartheta, x)=\partial_{\vartheta}\phi(\vartheta,x)$)
\begin{align*}
0&=\int_{\R}\big[\dot \phi(\vartheta,x)f(\vartheta,x)+\phi(\vartheta,x)\partial_\vartheta f(\vartheta,x)\big]\mu(dx)\\
& = \int_{\R}\big[\dot \phi(\vartheta,x)f(\vartheta,x)+\phi(\vartheta, x){\color{red}\partial_\vartheta \log f(\vartheta, x) f(\vartheta,x)}\big]\mu(dx).
\end{align*}
\item \underline{Conclusion}
$$\boxed{{\color{red}\E_\vartheta\big[\dot \phi(\vartheta,X)\big] = -\E_\vartheta\big[\phi(\vartheta, X)\partial_\vartheta \log f(\vartheta,X)\big]}}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de l'inégalité (fin)}
\begin{itemize}
\item On a 
$$\E_\vartheta\big[\dot \phi(\vartheta,X)\big] = -\E_\vartheta\big[\phi(\vartheta, X)\partial_\vartheta \log f(\vartheta,X)\big]$$
\item \underline{Cauchy-Schwarz} :
$$\big(\E_\vartheta\big[\dot \phi(\vartheta,X)\big] \big)^2 \leq \E_\vartheta\big[\phi(\vartheta, X)^2\big] \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta,X)\big)^2\big],$$
c'est-à-dire
$$\boxed{v_\phi(\vartheta)^{-1} = \frac{\big(\E_\vartheta\big[\dot \phi(\vartheta,X)\big] \big)^2}{\E_\vartheta\big[\phi(\vartheta, X)^2\big]} \leq \mathbb{I}(\vartheta).}$$
\end{itemize}
\end{frame}

\subsection{Cadre général et interprétation géométrique}

\begin{frame}
\frametitle{Information de Fisher dans un modèle général}
\begin{df}
\begin{itemize}
\item {\color{red}Situation} : suite d'expériences statistiques
$${\mathcal E}^n=\big(\mathfrak{Z}^n, {\mathcal Z}^n, \{\PP_\vartheta^n,\vartheta \in \Theta\}\big)$$
dominées par $\mu_n$, associées à l'observation $Z^{(n)}$,
$$f_n(\vartheta,z)=\frac{d\PP_\vartheta^n}{d\mu^n}(z),\;\;z\in\mathfrak{Z}^n,{\color{red}\vartheta \in \Theta \subset \R}.$$
\item {\color{red} Information de Fisher} (si elle existe) de l'expérience au point $\vartheta$ :
$$\mathbb{I}(\vartheta\,|\,{\mathcal E}_n)=\E_\vartheta^n\big[\big(\partial_\vartheta \log  f_n(\vartheta, Z^{(n)})\big)^2\big]$$
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Le cas multidimensionnel}
\begin{itemize}
\item {\color{red}Même contexte} que précédemment, avec $\Theta \subset \R^d$, et ${\color{red} d \geq 1}$.
\item {\color{red} Matrice d'information de Fisher}
$$\mathbb{I}(\vartheta)= \E_\vartheta \big[\nabla_{\vartheta}\log f(\vartheta, Z^{n}) \nabla_{\vartheta}\log f(\vartheta, Z^{n})^T\big]$$
{\color{red}matrice symétrique positive}. 
\item Si $\mathbb{I}(\vartheta)$ définie et si ${\mathcal E}^n$ {\color{red}modèle de densité}, en généralisant à la dimension $d$ les conditions de régularité, on a {\color{red}}
$$\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0, {\color{red}\mathbb{I}(\vartheta)^{-1}}\Big).$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Interprétation géométrique}
\begin{itemize}
\item On pose $\mathbb{D}(a,\vartheta)=\E_\vartheta\big[\log f(a,X)\big]$. On a vu (inégalité d'entropie) que
\begin{align*}
\mathbb{D}(a,\vartheta) & = \int_{\R}\log f(a,x) f(\vartheta,x)\mu(dx) \\
&  \leq \int_{\R}\log f({\color{red}\vartheta}, x) f(\vartheta,x)\mu(dx) = \mathbb{D}({\color{red}\vartheta},\vartheta).
\end{align*}
\item On a
$$\boxed{\mathbb{I}(\vartheta)=\partial_a^2 \mathbb{D}(a,\vartheta)_{\big|a=\vartheta}.
}$$
\begin{itemize}
\item Si $\mathbb{I}(\vartheta)$ est \og petite \fg{}, le {\color{red}rayon de courbure de $a \leadsto \mathbb{D}(a,\vartheta)$ est grand} dans un voisinage de $\vartheta$ : la stabilisation d'un maximum empirique (l'EMV) est plus difficile, rendant moins précis l'estimation.
\item Si $\mathbb{I}(\vartheta)$ est \og grande \fg{}, le {\color{red} rayon de courbure est petit} et le maximum de l'EMV est mieux localisé.
 \end{itemize}
%\item {\color{red}Faiblesse de cette approche} : nécessite de la régularité en $\vartheta$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Information de Fisher et régression}
\begin{itemize}
\item ${\mathcal E}^n$ expérience engendrée par $(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$ avec 
$$Y_i = r(\vartheta,\bx_i)+\xi_i,$$
$\xi_i$ : densité {\color{red}$g$ par rapport à la mesure de Lebesgue} + \og design \fg{} déterministe.
\item Observation : $Z^n = (Y_1,\ldots, Y_n)$, $\mu^n=dy_1\ldots dy_n$, $z=(y_1,\ldots, y_n)$
et
$$f_n(\vartheta, Z^n) = \prod_{i = 1}^n g\big(Y_i-r(\vartheta, \bx_i)\big)$$
\item {\color{red} Information de Fisher}
$$\mathbb{I}(\vartheta|{\mathcal E}^n) = \E_\vartheta\big[\big(\partial_\vartheta \log f_n(\vartheta, Z^n)\big)^2\big]$$
\end{itemize}
\end{frame}

\subsection{Exemples, applications}


\begin{frame}
\frametitle{Information de Fisher et régression}
\begin{itemize}
\item {\color{red}Formule explicite} pour la log-vraisemblance
$$\partial_\vartheta \log f_n(\vartheta, Z^n)  = \sum_{i = 1}^n \partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)$$
\item {\color{red}Propriété analogue avec le modèle de densité} : $\E_\vartheta\big[\partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)\big]=0$. 
\item {\color{red}Information de Fisher} par indépendance + centrage :
\begin{align*}
\mathbb{I}(\vartheta|{\mathcal E}^n) & = \sum_{i = 1}^n\E_\vartheta^n\big[\big(\partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)\big)^2\big] \\
& = \ldots
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemples et applications}
A {\color{red}titre d'exercice}, savoir calculer l'information de Fisher pour :
\begin{itemize}
\item L'estimation du paramètre d'une loi de Poisson dans le modèle de densité.
\item L'estimation de la moyenne-variance pour un échantillon gaussien.
\item {\color{red}La régression logistique}
\item L'estimation du paramètre d'une loi exponentielle {\color{red} avec ou sans} censure.
\end{itemize} 
\end{frame}

\begin{frame}
\frametitle{Efficacité à un pas}
\begin{itemize}
%\item {\color{red}Situation : modèle de densité} $Z^n = (X_1,\ldots, X_n)$.
\item Dans un modèle régulier, le {\color{red}calcul numérique} de l'EMV peut être difficile à réaliser.
\item Si l'on dispose d'un estimateur $\est$ {\color{red}asymptotiquement normal} et si les évaluations 
$$\ell'_n(\vartheta) = \tfrac{1}{n}\sum_{i = 1}^n \partial_\vartheta \log f(\vartheta, X_i),\;\;\ell''_n(\vartheta)=\tfrac{1}{n}\sum_{i = 1}^n\partial_\vartheta^2 \log f(\vartheta, X_i)$$
sont {\color{red}faciles}, alors on peut {\color{red} corriger} $\est$ de sorte d'avoir le même comportement asymptotique que l'EMV :
$$\widetilde \vartheta_n = \est - \frac{\ell'_n(\est)}{\ell''_n(\est)}\;\;\;\text{(algorithme de Newton)}$$
satisfait
$$\boxed{\sqrt{n}\big(\widetilde \vartheta_n-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0, {\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big)}$$
\end{itemize}
\end{frame}







\section{Tests statistiques}

\subsection{Notion de test et d'erreur de test}



\begin{frame}
\frametitle{Exemple introductif}
\begin{itemize}
\item On observe 10 lancers d'une pièce de monnaie et on obtient le résultat suivant :
$$(P, P, F, F, P, F, P, P, F, P).$$
{\color{red}La pièce est-elle équilibrée} ?
\item {\color{red}Répondre} à cette question revient à {\color{red}construire une procédure de décision} :
$$\varphi = \varphi(P, P, F, F, P, F, P, P, F, P)$$
$$ = 
\left\{
\begin{array}{ll}
0 & \text{{\color{red}on accepte} l'hypothèse \og la pièce est équilibrée\fg{}} \\
1 & \text{{\color{red}on rejette} l'hypothèse \og la pièce est équilibrée\fg{}}
\end{array}
\right.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Résolution}
\begin{itemize}
\item On associe l'expérience statistique (par exemple)
$${\mathcal E}^{10} = \big(\{0,1\}^{10}, \text{{\small parties de}}(\{0,1\}^{10}), \{\PP_\vartheta^{10}, \vartheta \in [0,1]\}\big),$$
avec ($P=0$, $F=1$)
$$\PP^{10}_\vartheta=\big(\vartheta\delta_{0}(dx)+(1-\vartheta)\delta_{1}(dx)\big)^{\otimes 10}.$$
\item\underline{Hypothèse nulle} :  {\color{red} \og la pièce est équilibrée \fg{}} 
$$\boxed{H_0: \vartheta = \frac{1}{2}}$$
\item \underline{Hypothèse alternative} :  {\color{red}\og la pièce est truquée\fg{}} 
$$\boxed{H_1 : \vartheta \neq \frac{1}{2}}$$ 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Résolution (cont.)}
\begin{itemize}
\item On note $Z$ l'observation.
\item On {\color{red}construit} une {\color{red}règle de décision simple} :
$$\varphi = 1_{\big\{Z \in {\mathcal R}\}} = 
\left\{
\begin{array}{ll}
0 & \text{on accepte l'hypothèse} \\
1 & \text{on rejette l'hypothèse.}
\end{array}
\right.$$
\item ${\mathcal R} \subset {\mathfrak Z}$ (espace des observables) : {\color{red}zone de rejet} ou {\color{red}région critique}.
\item \underline{Exemple}\footnote{léger abus de notation...}
$${\mathcal R} = \big\{\big|\widehat \vartheta(Z)-\tfrac{1}{2}\big| > t_0\big\},\;\;\widehat \vartheta(Z) = \estMV \big(\stackrel{exemple}{=} 0,6\big)$$
où $t_0$ est un seuil à choisir... {\color{red}Comment ?}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Erreur de décision}
\begin{itemize}
\item 
Lorsque l'on prend la décision $\varphi$, on peut se {\color{red}tromper de deux manières} :
$$\text{{\color{red}Rejeter}}\; H_0\;\;(\varphi=1)\;\;\text{{\color{red}alors que}}\;\;\vartheta = \frac{1}{2}$$
ou encore
$$\text{{\color{red}Accepter}}\;H_0\;\;(\varphi=0)\;\;\text{{\color{red}alors que}}\;\;\vartheta  \neq \frac{1}{2}.$$
\item \underline{Erreur de première espèce}  {\color{red}(=rejeter à tort)}
$$\PP_{\tfrac{1}{2}}^{10}\big[\varphi=1\big]$$
\item \underline{Erreur de seconde espèce}  {\color{red}(=accepter à tort)}
$$\big(\PP_\vartheta^{10}\big[\varphi=0\big],\;\;\vartheta \neq \frac{1}{2}\big).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion provisoire}
\begin{itemize}
\item Un \og bon test \fg{} $\varphi$ doit garantir {\color{red} simultanément} des erreurs de première et seconde espèce {\color{red}petites}.
\item {\color{red}Un test optimal existe-t-il ?}
\item Si {\color{red}non}, comment aborder la notion d'optimalité et comment construire un test optimal ?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Définition formelle}
\begin{itemize}
\item \underline{Situation} : ${\mathcal E} = \big({\mathcal Z}, \mathfrak{Z}, \{\PP_\vartheta, \vartheta \in \Theta\}\big)$ engendrée par l'observation $Z$.
\item {\color{red}Hypothèse nulle et alternative} : $\Theta_0 \subset \Theta$ et $\Theta_1 \subset \Theta$  t.q.
$$\Theta_0 \cap \Theta_1 = \emptyset.$$ 
\begin{df}[Test simple] Un test (simple) de l'hypothèse nulle $H_0: \vartheta \in \Theta_0$ contre l'alternative $H_1:\vartheta \in \Theta_1$ est une statistique $\varphi  = \varphi(Z) \in \{0,1\}$.
 (Fonction d') {\color{red}erreur de première espèce} :
$$\vartheta \in \Theta_0 \leadsto \PP_\vartheta\big[\varphi = 1\big]$$
 (Fonction d') {\color{red}erreur de seconde espèce}
$$\vartheta \in \Theta_1 \leadsto \PP_\vartheta \big[\varphi = 0\big] = 1- \text{{\color{red}puissance}}_\varphi(\vartheta).$$
\end{df}
\end{itemize}
\end{frame}

\subsection{Hypothèse simple contre alternative simple}

\begin{frame}
\frametitle{Hypothèse simple contre alternative simple}
\begin{itemize}
\item Cas où $\Theta = \{\vartheta_0, \vartheta_1\}$ avec $\vartheta_0 \neq \vartheta_1$.
\item Existe-t-il un test $\varphi^\star$ {\color{red}optimal}, au sens où :
$\forall \varphi$ test simple, on a {\color{red}simultanément}
$$\PP_{\vartheta_0}\big[\varphi^\star=1\big]\leq \PP_{\vartheta_0}\big[\varphi=1\big]$$
{\color{red}et}
$$\PP_{\vartheta_1}\big[\varphi^\star=0\big] \leq \PP_{\vartheta_1}\big[\varphi=0\big]\;\;\;{\color{red}?}$$
\item Si $\PP_{\vartheta_0}$ et $\PP_{\vartheta_1}$ ne sont pas {\color{red}étrangères} (cf. Cours 6) un tel test $\varphi^\star$ {\color{red}ne peut pas exister}.
%\item \underline{Esquisse de démonstration}
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Absence d'optimalité stricte}
\begin{itemize}
%\item \underline{Esquisse de démonstration}
\item {\color{red}Equivalence} tests simples $\leftrightsquigarrow$ estimateurs $\widehat \vartheta$ de $\vartheta$ {\color{red}via la représentation}:
$$\widehat \vartheta = \vartheta_0 1_{{\mathcal R}^c}+\vartheta_1 1_{{\mathcal R}} \leftrightsquigarrow \varphi = 1_{\mathcal R}.$$ 
\item {\color{red}Fonction de risque}
$${\mathcal P}(\varphi, \vartheta) = \E_\vartheta\big[1_{\widehat \vartheta \neq \vartheta}\big],\;\;\vartheta = \vartheta_0,\vartheta_1.$$
\item La fonction de perte $\ell(\widehat \vartheta, \vartheta) = 1_{\widehat \vartheta \neq \vartheta}$ joue le même rôle que la perte quadratique $(\widehat \vartheta - \vartheta)^2$ dans le Cours 6.
\item Test optimal $\varphi^\star$ $\leftrightsquigarrow$ estimateur optimal $\vartheta^\star$ pour ${\mathcal P}$.
\item {\color{red}Comme pour le cas du risque quadratique}, dès que $\PP_{\vartheta_0}$ et $\PP_{\vartheta_1}$ ne sont pas étrangères, un estimateur optimal {\color{red}n'existe pas} (cf. Cours 6). 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Riposte : principe de Neyman}
\begin{itemize}
\item 
On \og {\color{red}disymétrise} \fg{} les hypothèses $H_0$ et $H_1$ 
: $H_0$ est \og plus importante \fg{} que $H_1$ dans le sens suivant : on {\color{red} impose} une {\color{red}erreur de première espèce prescrite}.
\end{itemize}
\begin{df}
Pour $\alpha \in [0,1]$, un test $\varphi = \varphi_\alpha$ de l'hypothèse nulle $H_0:\vartheta \in \Theta_0$ contre une alternative $H_1$ est de niveau $\alpha$ si 
$$\sup_{\vartheta \in \Theta_0}\PP_\vartheta\big[\varphi_\alpha = 1\big] \leq \alpha.$$
\end{df}
\begin{itemize}
\item Un test de niveau $\alpha$ ne dit {\color{red}rien} sur l'erreur de seconde espèce (comportement sur l'alternative).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principe de Neyman (cont.)}
\begin{itemize}
\item Choix de la \og disymétrisation \fg{} = choix de modélisation. 
%Cas évident (dépistage d'une maladie), cas moins évident (détection de missile, efficacité d'un médicament).
\item \underline{{\color{red}Principe de Neyman}}  : $\alpha \in (0,1)$, parmi les test de niveau $\alpha$, chercher celui (ou ceux) ayant {\color{red}une erreur de seconde espèce minimale}.
\end{itemize}
\begin{df}
Un test de niveau $\alpha$ est dit {\color{red}Uniformément Plus Puissant} (UPP) si son erreur de seconde espèce est minimale parmi celles des tests de niveau $\alpha$.
\end{df}
\begin{itemize}
\item Pour le cas d'une {\color{red}hypothèse simple} contre une {\color{red}alternative simple}, un test UPP existe.
\end{itemize}
\end{frame}

\subsection{Lemme de Neyman-Pearson}

\begin{frame}
\frametitle{Principe de construction}
% \underline{{\color{red}Principe de construction}}
\begin{itemize}
\item $f(\vartheta, z) = \frac{d\PP_{\vartheta}}{d\mu}(z),\;\;z \in \mathfrak{Z},\;\;\vartheta =\vartheta_0,\vartheta_1$, $\mu$ mesure dominante. L'{\color{red}EMV} --si bien défini-- s'écrit
$$\estMV = \vartheta_0 1_{\{f(\vartheta_1,Z)<f(\vartheta_0, Z)\}}+\vartheta_1 1_{\{f(\vartheta_0,Z)< f(\vartheta_1, Z)\}}.$$
\item On choisit une {\color{red} région critique} de la forme
$$\boxed{{\mathcal R}(c) = \big\{f(\vartheta_1, Z) > c f(\vartheta_0,Z)\big\},\;\;c>0}$$
et on {\color{red}calibre} $c = c_\alpha$ de sorte que
$$\boxed{\PP_{\vartheta_0}\big[Z \in {\mathcal R}(c_\alpha)\big]=\alpha.}$$
\item Le test ainsi construit (si cette équation admet une solution) {\color{red}est de niveau $\alpha$}. On {\color{red}montre} qu'il est UPP.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lemme de Neyman-Pearson}
\begin{prop} Soit $\alpha \in [0,1]$. S'il existe $c_\alpha $ solution de 
$$\boxed{\PP_{\vartheta_0}\big[f(\vartheta_1, Z) > c_\alpha f(\vartheta_0, Z)\big] = \alpha}$$
alors le test de région critique ${\mathcal R}_\alpha = \big\{f(\vartheta_1,Z) > c_\alpha f(\vartheta_0, Z)\big\}$ {\color{red}est de niveau $\alpha$} et {\color{red}UPP} pour tester $H_0:\vartheta=\vartheta_0$ contre $H_1:\vartheta=\vartheta_1$.
\end{prop}
\begin{itemize}
\item Si $U = f(\vartheta_1,Z)/f(\vartheta_0,Z)$ bien définie et ${\mathcal L}(U)\ll dx$ (sous $\PP_{\vartheta_0}$), alors
$\PP_{\vartheta_0}\big[U > c_\alpha\big]=\alpha$ admet une solution.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple de mise en oeuvre}
\begin{itemize}
\item On observe
$$Z=(X_1,\ldots, X_n) \sim_{\text{i.i.d.}} {\mathcal N}(\vartheta, 1).$$
\item {\color{red}Construction du test de N-P.} de $H_0:\vartheta = \vartheta_0$ contre $H_1:\vartheta = \vartheta_1$, avec $\vartheta_0 < \vartheta_1$.
\item {\color{red}Mesure dominante} $\mu^n=\;$mesure de Lebesgue sur $\R^n$ et
$$f(\vartheta, Z)=\tfrac{1}{(2\pi)^{n/2}}\exp\big(-\frac{1}{2}\sum_{i = 1}^n X_i^2+n\vartheta\overline{X}_n-\frac{n\vartheta^2}{2}\big).$$
\item {\color{red}Rapport de vraisemblance}
$$\frac{f(\vartheta_1,Z)}{f(\vartheta_0,Z)} = \exp\big(n(\vartheta_1-\vartheta_0)\overline{X}_n-\frac{n}{2}(\vartheta_1^2-\vartheta_0^2)\big).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple (cont.)}
\begin{itemize}
\item {\color{red}Zone de rejet} du test de N-P. :
\begin{align*}
& \big\{f(\vartheta_1,Z) >  cf(\vartheta_0,Z)\big\}\\
  =& \big\{n(\vartheta_1-\vartheta_0)\overline{X}_n-\frac{n}{2}(\vartheta_1^2-\vartheta_0^2) > \log c\big\} \\
 =& \big\{\overline{X}_n > \frac{\vartheta_0+\vartheta_1}{2}+\tfrac{\log c}{n(\vartheta_0-\vartheta_1)}\big\}.
\end{align*}
\item {\color{red}Choix de $c$}. On résout
$$\PP_{\vartheta_0}\big[\overline{X}_n > \tfrac{1}{2}(\vartheta_0+\vartheta_1)+\tfrac{\log c}{n(\vartheta_0-\vartheta_1)}\big]=\alpha.$$
\item {\color{red}Approche standard} : on raisonne sous $\PP_{\vartheta_0}$. On a
$$\overline{X}_n = \vartheta_0 + \frac{1}{\sqrt{n}}\xi^{n,\vartheta_0},$$
où $\xi^{n,\vartheta_0}$ est une gaussienne standard ${\mathcal N}(0,1)$ sous $\PP_{\vartheta_0}$ {\color{red}mais pas sous une autre probabilité $\PP_\vartheta$ si $\vartheta \neq \vartheta_0$!} 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple (fin)}
\begin{itemize}
\item {\color{red}Résolution de}
$$\PP_{\vartheta_0}\big[\vartheta_0 + \frac{1}{\sqrt{n}}\xi^{n,\vartheta_0} > \frac{1}{2}(\vartheta_0+\vartheta_1)+\frac{\log c}{n(\vartheta_0-\vartheta_1)}\big]=\alpha.$$
\item {\color{red}Equivalent à}
$\PP_{\vartheta_0}\big[\xi^{n\vartheta_0} > \frac{\sqrt{n}}{2}(\vartheta_1-\vartheta_0)+\frac{1}{\sqrt{n}}\frac{\log c}{\vartheta_0-\vartheta_1}\big]=\alpha,$
soit
$$\frac{\sqrt{n}}{2}(\vartheta_1-\vartheta_0)+\frac{1}{\sqrt{n}}\frac{\log c}{\vartheta_0-\vartheta_1}=\Phi^{-1}(1-\alpha),$$
où $\Phi(x) = \int_{-\infty}^x e^{-u^2/2}\tfrac{du}{\sqrt{2\pi}}$.
\item {\color{red}Conclusion}
$$\boxed{c_\alpha = \exp\big(n\frac{(\vartheta_1-\vartheta_0)^2}{2}+\sqrt{n}(\vartheta_0-\vartheta_1)\Phi^{-1}(1-\alpha)\big)}$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bilan provisoire}
\begin{itemize}
\item Si l'on accepte {\color{red}le principe de Neyman}, on sait résoudre le problème à deux points.
\item Que faire si l'hypothèse nulle $H_0$ ou l'alternative $H_1$ sont {\color{red}composites} ? 
\begin{itemize}
\item On peut proposer des extensions si l'on dispose de structures particulières sur la vraisemblance du modèle (Poly. Ch. 7.3, hors programme).
\item On sait dire {\color{red} beaucoup de choses} dans le cas gaussien.
\end{itemize}
\item {\color{red} Critique méthodologique de l'approche de Neyman} $\leadsto$ notion de $p$-valeur.
\item On ne sait {\color{red}toujours pas} répondre à la question de l'exemple introductif... {\color{red}cadre asymptotique} Cours 8.
\end{itemize}
\end{frame}


\end{document}



\section{Tests gaussiens}

\subsection{Tests sur la moyenne}

\begin{frame}
\frametitle{Tests gaussiens incontournables}
\begin{itemize}
\item On observe
$$\bY=(Y_1,\ldots, Y_n) \sim {\mathcal N}(\mu, \sigma^2 \text{Id}_n).$$
\item \underline{{\color{red}Test sur la moyenne, variance connue}}
$$H_0:\mu \leq \mu_0\;\;\;\text{contre}\;\;\;H_1:\mu > \mu_0$$
\item {\color{red} Principe} on estime $\mu$ et on rejette $H_0$ si l'estimateur est \og plus grand \fg{} que $\mu_0$.
$${\mathcal R}(c_\alpha) = \big\{\overline{Y}_n - \mu_0 \geq c_\alpha\big\},\;\;\;c_\alpha\;\;\text{à déterminer}.$$
\item On choisit $c_\alpha$ de sorte que 
$$\sup_{\mu \leq \mu_0}\PP_\mu\big[{\mathcal R}(c_\alpha)\big] \leq \alpha.$$
\item Il y a {\color{red}plusieurs choix possibles}. On fait le choix rendant  ${\mathcal R}(c_\alpha)$ {\color{red}  maximale}.
%\item On a {\color{red}seulement} optimalité parmi la classe de test de zone de rejet de la forme ${\mathcal R}(c), c >0$ sans outil supplémentaire.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul de $c_\alpha$}
\begin{itemize}
\item {\color{red}Majoration de l'erreur de première espèce}. Si $\mu \leq \mu_0$, on a
\begin{align*}
\PP_{\mu}\big[\overline{Y}_n - \mu_0 \geq c_\alpha\big] & = \PP_\mu\big[(\mu+\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu})-\mu_0 \geq c_\alpha\big] \\
& =   \PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha + (\mu_0-{\color{red}\mu})\big] \\
& \leq \PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha\big].
\end{align*}
où $\xi^{n,\mu}$ est, en loi sous $\PP_\mu$, une gaussienne standard. 
\item {\color{red} Petit miracle} : la loi de $\xi^{n,\mu}$ sous $\PP_\mu$ ne {\color{red}dépend pas de} $\mu$
% (on parle de statistique libre). 
Donc
\begin{align*}
\PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha\big] & =1-\Phi\big(\tfrac{\sqrt{n}}{\sigma}c_\alpha\big) \\
& \stackrel{\text{on veut}}{\leq} \alpha.
\end{align*}
\item Le choix $c_{\alpha,n} = \tfrac{\sigma}{\sqrt{n}}\Phi^{-1}(1-\alpha)$ conduit à la zone de rejet ${\mathcal R}(c_\alpha)$ {\color{red}maximale}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Contrôle de l'erreur de seconde espèce}
\begin{itemize}
\item On a construit un test de niveau $\alpha$ parmi une classe {\color{red}donnée a priori} de tests basés sur un estimateur \og raisonnable\fg{}, de sorte que l'on ait une zone de rejet maximale. Désormais, $c_{\alpha, n}$ est {\color{red}fixé}.
\item On {\color{red}évalue à la main} l'erreur de seconde espèce ou la {\color{red}fonction de puissance}
\begin{align*}
\mu \in (\mu_0,+\infty) & \leadsto \PP_\mu\big[\overline{Y}_n - \mu_0 < c_{\alpha,n}\big] \\
&= 1 - {\text{puissance du test au point }}\mu
\end{align*}
\item {\color{red}Montrer que} pour tout $\mu > \mu_0$, on a $\PP_\mu\big[\overline{Y}_n - \mu_0 < c_{\alpha,n}\big] \rightarrow 0$ lorsque $n\rightarrow \infty$. 
\item Pour l'optimalité dans un sens plus fort, il faut {\color{red} d'autres outils}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Autres tests classiques gaussiens}
\begin{itemize}
%\item \underline{{\color{red}Test sur la moyenne et la  variance}}. 
\item {\color{red}Ingrédient principal} :
$$s_n^2:=\frac{1}{n-1}\sum_{i = 1}^n (Y_i-\overline{Y}_n)^2 = \frac{n}{n-1}\big(\widehat \sigma^2_n\big)^{{\tt mv}}$$
alors
$$(n-1)\frac{s_n^2}{\sigma^2} \sim \chi^2(n-1)$$
et
$$\frac{\sqrt{n}(\overline{Y}_n-\mu)}{s_n} \sim \text{Student}(n-1)$$
et ces variables sont {\color{red}pivotales} : leur loi ne dépend pas de $\mu,\sigma^2$ sous $\PP_{\mu,\sigma^2}$.
\item Les lois du {\color{red}$\chi^2$} et de {\color{red}Student} (à $k$ degrés de liberté) sont classiques et s'étudient indépendamment.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Tests sur la moyenne}
%\begin{itemize}
%\item On teste {\color{red}$H_0 : \mu \leq \mu_0$ contre $H_1: \mu > \mu_0$}. Un test de niveau $\alpha$ : donné par
%$${\mathcal R}_\alpha = \big\{T(\bY)>q_{1-\alpha, n-1}^{\mathfrak T}\big\}$$
%où
%$$T(Y)  = \frac{\sqrt{n}(\overline{Y}_n-\mu_0)}{s_n}$$
%%et $q_{1-\alpha, n-1}^{\mathfrak{T}}$ = quantile d'ordre $1-\alpha$ de la loi de Student à $n-1$ degrés de liberté : 
%où 
%$$\PP\big[\text{Student}_{n-1} > q_{1-\alpha, n-1}^{\mathfrak{T}}\big]=\alpha$$
%\item On teste {\color{red} $H_0 : \mu = \mu_0$ contre $H_1:\mu \neq \mu_0$}. Un test de niveau $\alpha$: donné par
%${\mathcal R}_\alpha = \big\{\big|T(\bY)\big| > q_{1-{\color{red}\alpha/2}, n-1}^{\mathfrak{T}}\big\}$.
%\end{itemize}
%\end{frame}
%\subsection{Tests sur la variance}
%\begin{frame}
%\frametitle{Test sur la variance}
%\begin{itemize}
%\item On teste {\color{red} $H_0:\sigma^2 \leq \sigma_0^2$ contre $H_1:\sigma^2 > \sigma_0^2$}. Un test de niveau $\alpha$ : donné par 
%$${\mathcal R}_\alpha = \big\{V(\bY)>q_{1-\alpha,n-1}^{\chi^2}\big\},$$
%où
%$$V(\bY) = \frac{1}{\sigma_0^2}\sum_{i = 1}^n (Y_i - \overline{Y})^2$$
%et
%$$\PP\big[\text{Chi-deux}_{n-1} > q_{1-\alpha,n-1}^{\chi^2}\big] = \alpha.$$
%\item {\color{red}Mêmes remarques méthodologiques} sur l'optimalité de ces tests que précédemment.
%\end{itemize}
%\end{frame}


\section{Compléments : $p$-valeur et liens entre tests et régions de confiance}

\begin{frame}
\frametitle{$p$-valeurs}
\begin{itemize}
\item {\color{red}Exemple} : on observe
$$X_1,\ldots, X_n\sim_{\text{i.i.d.}}{\mathcal N}(\mu,\sigma^2),\;\;\;\sigma^2\;\;\text{connu}.$$
\item {\color{red}Objectif}: tester $H_0:\mu=0$ contre $H_1:\mu\neq 0$.
\item Au niveau $\alpha=5\%$, on rejette si
$$\big| \overline{X}_n \big| > \frac{\phi^{-1}(1-\alpha/2)}{\sqrt{n}}$$ 
\item {\color{red}Application numérique} : $n=100$, $\overline{X}_{100}=0.307$. On a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.196$. {\color{red}on rejette l'hypothèse...}.
%\item  
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{$p$-valeur (cont.)}
\begin{itemize}
\item {\color{red}Et pour un autre choix de $\alpha$ ?}. Pour $\alpha=0.01$, on a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.256$. On rejette toujours... Pour $\alpha=0.001$, on a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.329$. {\color{red}On accepte $H_0$ !}
\item Que penser de cette petite expérience ?
\begin{itemize}
\item En pratique, on a une observation une bonne fois pour toute (ici $0.307$) et on \og choisit \fg{} $\alpha$... {\color{red}comment ?}
\item On ne veut pas $\alpha$ trop grand (trop de risque), mais en prenant $\alpha$ de plus en plus petit... on va {\color{red} fatalement} finir par accepter $H_0$ ! 
\end{itemize}
\item Défaut de méthodologie inhérent au principe de Neyman (contrôle de l'erreur de première espèce).
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{p-valeur}
\begin{itemize}
\item Quantité {\color{red}significative} : non par le niveau $\alpha$, mais le {\color{red}seuil de basculement de décision} : c'est la $p$-valeur ($p$-value) du test. 
\end{itemize}
\begin{df}
Soit ${\mathcal R}_\alpha$ une famille de zones de rejet d'un test de niveau $\alpha$ pour une hypothèse $H_0$ contre une alternative $H_1$. Soit $Z$ l'observation associée à l'expérience. On a $Z \in \mathfrak{Z}$ et ${\mathcal R}_0 = \mathfrak{Z}$.
On appelle {\color{red}$p$-valeur du test} la quantité
$$p-\text{valeur}(Z) = \inf\{\alpha, Z \in {\mathcal R}_\alpha\}.$$
\end{df}
\end{frame}

\begin{frame}
\frametitle{Interprétation de la $p$-valeur}
\begin{itemize}
\item Une grande valeur de la $p$-valeur s'interprète en faveur de {\color{red}ne pas vouloir rejeter l'hypothèse}.
\item \og Ne pas vouloir rejeter l'hypothèse \fg{} peut signifier deux choses :
\begin{itemize}
\item L'hypothèse est vraie
\item L'hypothèse est fausse {\color{red} mais} le test n'est pas {\color{red}puissant} (erreur de seconde espèce {\color{red}grande}).
\end{itemize}
\item {\color{red}Souvent :} la $p$-valeur est la probabilité (sous $H_0$) que la statistique de test d'une expérience \og copie \fg{} soit $\geq$ à la statistique de test observée.
\item {\color{red}Exemple du test du $\chi^2$ et de l'expérience de Mendel} (à suivre) %
\end{itemize}
\end{frame}


%\begin{frame}
%\frametitle{Poly. Errata Ch. 6}
%{\small
%\begin{itemize}
%\item p. 142. Dans l'énonce de la proposition 6.3, lire
%$$\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}  = + \E_{\vartheta}\big[\partial_\vartheta^2 \ell(\vartheta, X)\big] = - \mathbb{I}(\vartheta)$$
%\item p 143. Dans la preuve de la proposition 6.3, on doit écrire pour la première formule
%$$\partial_a \mathcal{F}(a,\vartheta) = + \int \partial_\vartheta \ell(\vartheta, x)f(\vartheta, x) \mu(dx)$$
%puis remplacer \og{} minimum \fg{} par \og{} maximum \fg{} dans la phrase en-dessous...
%\item Après le lemme 6.3.3., on obtient alors $\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   = -\mathbb{I}(\vartheta)$
%\item Finalement, p 144, début du paragraphe 6.3.4, on se retrouve avec
%$$\mathbb{I}(\vartheta) = - \partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta} \geq 0$$
%et tout (re)devient cohérent...
%\end{itemize}
%}
%\end{frame}
%

\end{document}















\subsection{Test d'appartenance à un sous-espace linéaire}

\begin{frame}
\frametitle{Test d'appartenance à un sous-espace linéaire}
\begin{itemize}
\item {\color{red}Modèle linéaire gaussien} $\vartheta \in \R^d$ et
$$\bY = \design \vartheta + \boldsymbol{\xi},\;\;\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2 \text{Id}_n),$$
avec $\det\design^T\design >0$.
\item $a \in \R$, $j \in \{1,\ldots, d\}$ donné. {\color{red}Test de $H_0: \vartheta_j=a$ contre $H_1:\vartheta_j \neq a$}, $\vartheta = (\vartheta_1,\ldots, \vartheta_d)^T$.
\item {\color{red}On a} (exercice !)
$$\frac{\big(\estMC\big)_j-\vartheta_j}{s_n \sqrt{(\design^T\design)_{jj}^{-1}}} \stackrel{d}{=} \text{Student}(n-d).$$
\item {\color{red}Test  de niveau $\alpha$} défini par la zone de rejet
$${\mathcal R}_{\alpha} = \Big\{\Big|\frac{\big(\estMC\big)_j-a}{s_n \sqrt{(\design^T\design)_{jj}^{-1}}}\Big| > q_{1-\alpha/2, n-d}^{\mathfrak{T}}\Big\}.$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Sélection de variables}
\begin{itemize}
\item On écrit le modèle linéaire (gaussien) avec $d \geq 2$:
$$Y_i = \vartheta^T\bx_i+\xi_i = \sum_{i = 1}^d \vartheta_i x_i + \xi_i,\;\;i=1,\ldots, n$$
\item $1 \leq k <d$ fixé. {\color{red}Test d'influence des $k$ premières variables seulement}. On teste
$$\boxed{H_0:\vartheta_{k+\ell}=0,\;\;\ell = 1,\ldots, d-k}$$
contre 
$$\boxed{H_1: \text{il existe} \;1 \leq \ell \leq d-k,\;\text{t.q.}\;\vartheta_{k+\ell} \neq 0}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Formulation du problème : F-tests.}
\begin{itemize}
\item Poly. pp. 186--188.
\item $\mathbb{G}$ matrice d'une application linéaire de $\R^d \rightarrow \R^m$ de la forme
$$
\mathbb{G} = 
\left(
\begin{array}{llllll}
0 & \ldots & 0 & \;\;1 & \ldots & 0 \\
\vdots & \ddots &\vdots &\;\; \vdots & \ddots & \vdots \\
0 & \ldots & 0 &\;\; 0 & \ldots & 1
\end{array}
\right),
$$
bloc de $0$ : $m$ lignes et $d-m$ colonnes.
% alors que le second bloc est la matrice identité à $m$ lignes et $m$ colonnes. 
et $\boldsymbol{b} = (a_1,\ldots, a_m)^T$ donné. 
\item {\color{red}On teste}
$$\boxed{H_0:\mathbb{G}\vartheta = \boldsymbol{b}}$$
{\color{red}contre}
$$\boxed{H_1:\mathbb{G}\vartheta \neq \boldsymbol{b}}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{F-tests (fin)}
\begin{itemize}
\item Sous l'hypothèse (sous $\PP_\vartheta$ tel que $\mathbb{G}\vartheta=\boldsymbol{b}$) on a (Cochran)
$$\mathbb{G}\estMC \sim {\mathcal N}\big(\boldsymbol{b},\sigma^2 \mathbb{G}( \design^T \design )^{-1}\mathbb{G}^T\big)$$ 
\item En posant ${\bf U} = \sigma^2 \mathbb{G}(\design^T\design)^{-1}\mathbb{G}^T$, on {\color{red}montre}
$$(\mathbb{G}\estMC-{\bf b})^T{\bf U}^{-1}(\mathbb{G}\estMC-{\bf b}) \sim \chi^2(m)\;\;{\color{red}\text{sous}\; H_0}.$$
\item Si $\sigma^2$ inconnu, on l'estime par $\widehat \sigma_n^2 = \frac{\|{\bf Y}-\design \,\estMC\|^2}{n-d}$. Alors la loi de
$$\frac{(\mathbb{G}\estMC-{\bf b})^T\widehat {\bf U}^{-1}(\mathbb{G}\estMC-{\bf b})}{m}$$
ne {\color{red}dépend pas de $\vartheta$ ni de $\sigma^2$} sous $H_0$ et suit la loi de Fisher-Snedecor à $(m,n-d)$ degrés de liberté.
\end{itemize}
\end{frame}



\section{Notion de $p$-valeur}

\begin{frame}
\frametitle{Poly. Errata Ch. 6}
{\small
\begin{itemize}
\item p. 142. Dans l'énonce de la proposition 6.3, lire
$$\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}  = + \E_{\vartheta}\big[\partial_\vartheta^2 \ell(\vartheta, X)\big] = - \mathbb{I}(\vartheta)$$
\item p 143. Dans la preuve de la proposition 6.3, on doit écrire pour la première formule
$$\partial_a \mathcal{F}(a,\vartheta) = + \int \partial_\vartheta \ell(\vartheta, x)f(\vartheta, x) \mu(dx)$$
puis remplacer \og{} minimum \fg{} par \og{} maximum \fg{} dans la phrase en-dessous...
\item Après le lemme 6.3.3., on obtient alors $\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   = -\mathbb{I}(\vartheta)$
\item Finalement, p 144, début du paragraphe 6.3.4, on se retrouve avec
$$\mathbb{I}(\vartheta) = - \partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   0$
et tout (re)devient cohérent...
\end{itemize}
}
\end{frame}

\end{document}









