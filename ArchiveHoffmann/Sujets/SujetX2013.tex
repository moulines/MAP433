 \documentclass[12pt]{article}
 \usepackage[applemac]{inputenc}

\usepackage{amsmath,amssymb,amsthm,amsfonts,amstext,amsbsy,amscd}
%\usepackage[notref,notcite]{showkeys}
\usepackage{a4wide}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{picins}
%\usepackage{tikz}
\setlength{\parskip}{0.3cm}
%\setlength{\textwidth}{13.7cm}
\setlength{\textwidth}{14cm}
\setlength{\textheight}{19.3cm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Albert's definitions
\def\vp{\varphi}
\def\<{\langle}
\def\>{\rangle}
\def\t{\tilde}
\def\i{\infty}
\def\e{\eps}
\def\sm{\setminus}
\def\nl{\newline}
\def\o{\overline}
\def\wt{\widetilde}
\def\wh{\widehat}
\def\cK{\cal K}
\def\co{\cal O}
\def\Chi{\raise .3ex
\hbox{\large $\chi$}} \def\vp{\varphi}
\newcommand{\norme}[1]{ {\left\lVert  #1\right\rVert}}
\newcommand{\dd}{\text{d}}
\newcommand{\ve}{\varepsilon}
\def\({\Bigl (}
\def\){\Bigr )}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{$$ \begin{array}{lll}}
\newcommand{\eea}{\end{array} $$}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\iref}[1]{(\ref{#1})}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%This is Markus' definition
\newcommand{\MR}{{($\spadesuit$)}}
\frenchspacing \sloppy
\numberwithin{equation}{section}
%\swapnumbers
\newtheorem{satz}{Satz}[section]
\newtheorem{theo}[satz]{Theorem}
\newtheorem{prop}[satz]{Proposition}
\newtheorem{theorem}[satz]{Theorem}
\newtheorem{proposition}[satz]{Proposition}
\newtheorem{corollary}[satz]{Corollary}
\newtheorem{lemma}[satz]{Lemma}
\newtheorem{assumption}[satz]{Assumption}
\newtheorem{korollar}[satz]{Korollar}
\newtheorem{definition}[satz]{Definition}
\newtheorem{bemerkung}[satz]{Bemerkung}
\newtheorem{remark}[satz]{Remark}
\newtheorem{remarks}[satz]{Remarks}
\newtheorem{beispiel}[satz]{Beispiel}
\newtheorem{example}[satz]{Example}
\DeclareMathOperator{\E}{{\mathbb E}}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\spann}{span}
\DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\im}{im}
\DeclareMathOperator{\rg}{rg} \DeclareMathOperator{\Eig}{Eig}
\DeclareMathOperator{\sgn}{sgn} \DeclareMathOperator{\ran}{ran}
\DeclareMathOperator{\kerr}{ker} \DeclareMathOperator{\dimm}{dim}
\DeclareMathOperator{\codim}{codim}
\DeclareMathOperator{\esssup}{esssup}
\DeclareMathOperator{\diag}{diag} \DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\grad}{grad} \DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\argmin}{argmin}
\DeclareMathOperator{\Var}{Var} \DeclareMathOperator{\Cov}{Cov}
\renewcommand{\d}{\ensuremath {\,\text{d}}}
\providecommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}
\renewcommand{\cdot}{{\scriptstyle \bullet} }
\providecommand{\abs}[1]{\lvert #1 \rvert}
\providecommand{\norm}[1]{\lVert #1 \rVert}
%\providecommand{\bnorm}[1]{{\Bigl\lVert #1 \Bigr\rVert}}s
\providecommand{\babs}[1]{{\Bigl\lvert #1 \Bigr\rvert}}
\providecommand{\scapro}[2]{\langle #1,#2 \rangle}
\providecommand{\floor}[1]{\lfloor #1 \rfloor}
\providecommand{\dfloor}[1]{{\lfloor #1 \rfloor_\Delta}}
\providecommand{\ceil}[1]{\lceil #1 \rceil}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\renewcommand{\le}{\leqslant}
\renewcommand{\ge}{\geqslant}
\newcommand{\cit}[1]{\citeasnoun{#1}}
%\newcommand{\CORRECTION}[1]{{\it \textsc{Correction succinte~:} \\ #1}}
\newcommand{\CORRECTION}[1]{}



\begin{document}
\title{\'Ecole Polytechnique -- Année 2013-2014\\ MAP\-433\;\; Statistique}
\date{}
\maketitle



{\small {\bf Attention :}
%On tiendra grand compte de la qualité de la rédaction et de la présentation. 
Les Sections 1 et 2 sont indépendantes\footnote{La Section 1 est construite d'après un exemple de Le Cam (1990) remontant à une étude de Neyman et Scott.}. Il n'est pas nécessaire de traiter toutes les questions pour obtenir une excellente note !

{\small Pour $x \in \R$, on note $\Phi(x) = \int_{-\infty}^x e^{-t^2/2}\tfrac{dt}{\sqrt{2\pi}}$ la fonction de répartition de la loi gaussienne standard et pour $n \geq 1$ et $\alpha \in (0,1)$, on note $q_{n,1-\alpha}^{\chi^2}$ le quantile d'ordre $1-\alpha$ de la loi du Chi-deux à $n$ degrés de liberté, défini par
$$\PP(\chi^2_n > q_{n,1-\alpha}^{\chi^2})=\alpha$$
si la variable aléatoire $\chi^2_n$ suit la loi du Chi-deux à $n$ degrés de liberté.}

\section{Effets dimensionnels}
Soit $n \geq 1$ un entier et $\mathcal E^n$ l'expérience engendrée par 
$$Z^n= \big(X_1, Y_1, X_2, Y_2, \ldots, X_n,Y_n\big)$$
où les $X_i$ sont des variables aléatoires gaussiennes indépendantes, de moyenne $\mu_i \in \R$ et de variance $\sigma^2>0$. Les $Y_i$ sont indépendantes, indépendantes des $X_i$, et $Y_i$ a la même loi que $X_i$. Le paramètre est
$$\vartheta = (\mu_1,\mu_2,\ldots, \mu_n,\sigma^2) \in \Theta = \R^n\times (0,\infty) \subset \R^{n+1}$$
et on note $\PP_\vartheta^n$ la loi de $Z^n$.
\begin{enumerate}
\item Montrer que l'expérience $\mathcal E^n = \{\PP_\vartheta^n,\vartheta \in \Theta\}$ est dominée et expliciter sa fonction de vraisemblance
$$\vartheta \leadsto \mathcal L(\vartheta, X_1,Y_1,X_2,Y_2,\ldots,X_n,Y_n).$$

\CORRECTION{
On a 
$$\PP_\vartheta(dx_1dy_1\cdots dx_ndy_n) = \prod_{i = 1}^nf_{\mu_i,\sigma}(x_i)f_{\mu_i,\sigma}(y_i)dx_idy_i,$$
où
$$f_{\mu_i,\sigma}(x) = \frac{1}{\sqrt{2\pi}}\exp\big(-\tfrac{1}{2\sigma^2}(x-\mu_i)^2\big)$$
est la densité de la loi normale de moyenne $\mu_i$ et de variance $\sigma^2$.
Donc la mesure de Lebesgue sur $\R^{2n}$ domine $\PP_{\vartheta}$ pour tout $\vartheta \in \Theta$ et
\begin{align*}
\mathcal L(\vartheta,X_1,Y_1,\ldots, X_n,Y_n) & = \prod_{i = 1}^nf_{\mu_i,\sigma}(X_i)f_{\mu_i,\sigma}(Y_i) \\
& = \Big(\frac{1}{\sqrt{2\pi \sigma^2}}\Big)^{2n}\prod_{i = 1}^n\exp\Big(-\frac{1}{2\sigma^2}\big((X_i-\mu_i)^2+(Y_i-\mu_i)^2\big)\Big).
\end{align*}
}
\end{enumerate}
\subsection{Estimation de la variance}
On s'intéresse dans un premier temps à l'estimation de $\sigma^2$, c'est-à-dire la $(n+1)$-i$^{\text{ème}}$ composante du paramètre $\vartheta$.
On pose
$$T_n = n^{-1}\sum_{i = 1}^n \Big(\frac{X_i-Y_i}{\sqrt{2}}\Big)^2.$$
\begin{itemize}
\item[2.] Quelle est la loi de $T_n$ sous $\PP_{\vartheta}^n$ ?

\CORRECTION{
Les variables aléatoires $U_i = \frac{X_i-Y_i}{\sqrt{2}}$ sont indépendantes et de loi $\mathcal N(0,\sigma^2)$ sous $\PP_\vartheta$. Donc 
$$T_n \stackrel{\mathcal L(\PP_\vartheta)}{=} \sigma^2\frac{\chi_n}{n}$$
où $\chi_n$ suit la loi du $\chi^2$ à $n$ degés de libertés.  
}
\item[3.] Montrer que $T_n$ est asymptotiquement normal (lorsque $n\rightarrow \infty$) et calculer sa variance limite.

\CORRECTION{Par la loi des grands nombres, $T_n$ est convergent et par le théorème central limite, $T_n$ est asymptotiquement normal de variance limite $2\sigma^4$.
}
\item[4.] Construire un intervalle de confiance asymptotique pour $\sigma^2$.

\CORRECTION{
On invoque le lemme de Slutsky et le cours. Un intervalle de confiance asymptotique de niveau de confiance $1-\alpha$ est
$$\big[T_n \pm \frac{\sqrt{2}T_n}{\sqrt{n}}\,\Phi^{-1}(1-\alpha/2)\big].$$
}
\item[5.] Montrer que $\vartheta \leadsto \mathcal L(\vartheta,  X_1,Y_1,X_2,Y_2,\ldots,X_n,Y_n)$ admet un maximum unique sur $\Theta$ et
calculer l'estimateur du maximum de vraisemblance $(\widehat \sigma^2)^{{\tt MV}}_n$ pour le paramètre $\sigma^2$.

\CORRECTION{
Pour $i=1,\ldots, n$, l'application
$$\mu_i \leadsto (X_i-\mu_i)^2+(Y_i-\mu_i)^2$$
admet un minimum unique en $\widehat \mu_i^{{\tt mv}} = \frac{X_i+Y_i}{2}$. D'autre part, en injectant $\widehat \mu_i^{{\tt mv}}$ dans la vraisemblance et en passant au logarithme, il reste à considérer
$$\sigma^2 \leadsto n \log \frac{1}{\sigma^2}-\sum_{i = 1}^n \frac{1}{2\sigma^2}\Big(2\Big(\frac{X_i-Y_i}{2}\Big)^2\Big)$$
qui a un maximum unique en 
$$\big(\widehat \sigma^2\big)^{{\tt mv}} = \frac{1}{n}\sum_{i = 1}^n \Big(\frac{X_i-Y_i}{2}\Big)^2 = \frac{1}{2}T_n.$$
}
\item[6.] L'estimateur du maximum de vraisemblance $(\widehat \sigma^2)^{{\tt MV}}_n$ est-il convergent ?

\CORRECTION{
On a $T_n \stackrel{\PP_\vartheta)}{\rightarrow} \frac{\sigma^2}{2} \neq \sigma^2$ pour tout $\sigma \in (0,\infty)$. Donc le maximum de vraisemblance pour $\sigma^2$ n'est pas convergent.
}
\item[7.] Pourquoi ce résultat semble-t-il contredire la théorie asymptotique étudiée en cours ? Proposer une explication pour lever ce paradoxe.

\CORRECTION{
Dans un modèle régulier, le maximum de vraisemblance est asymptotiquement normal, à condition que la dimension du paramètre $d$ soit fixée. Mais ici, le paramètre $\vartheta$ est de dimension $n+1$ et sa dimension augmente lorsque $n$ augmente. Ceci a un effet sur l'estimation de la $n$-ième composante. Ici le maximum de vraisemblance pour la variance se concentre bien autour d'une valeur fixe, mais autour de $\sigma^2/2$ et non $\sigma^2$! 
}
\end{itemize}
\subsection{Estimation du vecteur moyenne}

On suppose désormais $\sigma^2$ connu. On cherche à estimer 
$(\mu_1,\ldots, \mu_n)$.
\begin{itemize}
\item[8.] Montrer que
$$\widehat \mu^{{\tt mv}} = \big(\widehat \mu_1^{{\tt mv}}, \ldots, \widehat \mu_n^{{\tt mv}}\big) = \frac{1}{2}\big(X_1+Y_1,X_2+Y_2,\ldots, X_n+Y_n\big).$$

\CORRECTION{
Le résultat a été établi à la Question 5.
}
\end{itemize}
On pose 
$$
\mathcal R_n(\widehat \mu^{{\tt mv}} ) =\sup_{(\mu_1,\ldots, \mu_n) \in \R^n}\E_{\vartheta}^n\Big[\sum_{i = 1}^n \big(\widehat \mu_i^{{\tt mv}}-\mu_i\big)^2\Big]$$
et on suppose désormais $\sigma^2 = \sigma^2_n \rightarrow 0$ lorsque $n\rightarrow \infty$. 
\begin{itemize}
\item[9.]  A quelle condition sur $\sigma^2_n$ a-t-on $\mathcal R_n(\widehat \mu^{{\tt mv}}) \rightarrow 0$ lorsque $n \rightarrow \infty$ ?

\CORRECTION{
On a $\tfrac{X_i+Y_i}{2} = \mu_i+\tfrac{\sigma_n}{\sqrt{2}}\xi_i$, où les $\xi_i$ sont indépendantes, de loi normale standard sous $\PP_\vartheta$. Il vient
$$\E_{\vartheta}\Big[\sum_{i = 1}^n \big(\widehat \mu_i^{{\tt mv}}-\mu_i\big)^2\Big] = \E_{\vartheta}\Big[\sum_{i = 1}^n \big(\tfrac{\sigma_n}{\sqrt{2}}\xi_i\big)^2\Big] = \frac{n\sigma_n^2}{2}.$$
Il faut et il suffit $\sigma_n^2=o(n^{-1})$.
}
\end{itemize}
Soit $(a_i)_{i \geq 1}$ une suite croissante de nombres strictement positifs vérifiant $\lim_{n\rightarrow \infty} a_n=\infty$. 
On définit 
$$\mathcal M_n = \big\{(\mu_1,\ldots, \mu_n) \in \R^n,\;\;\sum_{i=1}^na_i\mu_i^2\leq 1\big\}.$$
Pour $1 \leq N \leq n$ entier, on considère la suite d'estimateurs
$$\widehat \mu^N = \big(\widehat \mu_1^N,\ldots, \widehat \mu_n^N\big)=\frac{1}{2}\big(X_1+Y_1,X_2+Y_2,\ldots, X_N+Y_N,0,\ldots, 0\big).$$
\begin{itemize}
\item[10.] Montrer que 
$${\widetilde {\mathcal R}_n}(\widehat \mu^N) = \sup_{(\mu_1,\ldots, \mu_n)\in \mathcal M_n}\E_{\vartheta}^n\Big[\sum_{i = 1}^n(\widehat \mu_i^N-\mu_i)^2\Big] \leq \frac{N\sigma_n^2}{2}+v_N,$$
où $\lim_{N\rightarrow \infty} v_N=0$.

\CORRECTION{
On a 
\begin{align*}
\E_{\vartheta}\Big[\sum_{i = 1}^n(\widehat \mu_i^N-\mu_i)^2\Big]  & = \E_{\vartheta}\Big[\sum_{i = 1}^N(\tfrac{\sigma}{\sqrt{2}}\xi_i)^2\Big] + \sum_{i = N+1}^n\mu_i^2 \\
& = \frac{\sigma_n^2N}{2} + \sum_{i = N+1}^n \frac{a_i}{a_i}\mu_i^2 \\
& \leq \frac{\sigma_n^2N}{2} + a_{N+1}^{-1} \sum_{i = 1}^na_i\mu_i^2 \leq \frac{\sigma_n^2N}{2} + a_{N+1}^{-1} 
\end{align*}
car la suite $a_i^{-1}$ est décroissante. Puisque $a_N \rightarrow \infty$ lorsque $N \rightarrow \infty$, on obtient le résultat anoncé avec $v_N = a_{N+1}^{-1} =o(1)$.
}
\item[11.]  En déduire qu'il existe toujours un choix $N=N_n$ de sorte que l'on ait
$\lim_{n \rightarrow \infty} {\widetilde {\mathcal R}_n}(\widehat \mu^{N_n}) = 0.$
A-t-on toujours  
$$\liminf_{n \rightarrow \infty} \sup_{(\mu_1,\ldots, \mu_n)\in \mathcal M_n}\E_{\vartheta}^n\Big[\sum_{i = 1}^n\big(\widehat \mu_i^{{\tt mv}}-\mu_i\big)^2\Big] = 0\;?$$ 

\CORRECTION{
Il suffit de prendre $N=N_n \rightarrow \infty$ de sorte que $\sigma^2_nN_n\rightarrow 0$ et $N_n\leq n-1$, un choix qui est toujours possible puisque  $\sigma_n^2 \rightarrow 0$. Mais si $\sigma_n^2n$ ne tend pas vers $0$ (c'est la cas par exemple si $\sigma_n = n^{-\alpha}$ avec $\alpha \leq 1/2$), alors $\hat \mu^{{\tt mv}}$ ne converge pas pour le critère ${\widetilde {\mathcal R}}_n$ alors que $\widehat \mu^{N_n}$ convergera toujours.
}
\end{itemize}
\section{Un test de variance}

\subsection{Etude à deux points}
Pour $\rho >0$, on pose
$$f_{\rho}(y) = \frac{1}{2\sqrt{2\pi}}y^{-3/4}\rho^{-1/2}\exp\Big(-\frac{1}{2}\frac{\sqrt{y}}{\rho}\Big){\bf 1}_{\{y>0\}}.$$
\begin{enumerate}
\item Vérifier que $f_{\rho}$ est une densité de probabilité. 

\CORRECTION{C'est un calcul standard. On peut remarquer que si $Y_i$ suit la loi de densité $f_\rho$, alors $Y_i= \rho^2 X_i^4$, où $X_i$ suit une loi gaussienne standard. En effet, pour toute fonction test $\varphi$, on a
\begin{align*}
\E[\varphi(\rho^2X_i^4)] 
% \int_{\R} \varphi(\rho^2x^4)\exp(-\tfrac{1}{2}x^2)\tfrac{dx}{\sqrt{2\pi}} \\
& = 2\int_0^\infty \varphi(\rho^2x^4)\exp(-\tfrac{1}{2}x^2)\tfrac{dx}{\sqrt{2\pi}} \\
& = 2\int_0^\infty \varphi(y)\exp(-\tfrac{1}{2}\rho^{-1}y^{1/2})\rho^{-1/2}\frac{y^{-3/4}}{4\sqrt{2\pi}}dy
\end{align*}
en effectuant le changement de variable $\rho^2x^4=y$ qui conduit à $x=\rho^{-1/2}y^{1/4}$ et $dx=\rho^{-1/2}\frac{y^{-3/4}}{4}dy$.
}

\end{enumerate}
On observe un $n$-échantillon $(Y_1,\ldots, Y_n)$ de densité $f_\rho(y)$ et on pose 
$$U_n = \sum_{i = 1}^n\sqrt{Y_i}.$$ 
\begin{itemize}
\item[2.] Soit $\rho_1>\rho_0$. Montrer que la zone de rejet du test de Neyman-Pearson de 
$$H_0:\rho = \rho_0\;\;\text{ contre}\;\;H_1:\rho=\rho_1$$
s'écrit
$${\mathcal R}_n(\kappa) = \big\{U_n > \kappa\big\}$$
et calculer $\kappa = \kappa_n(\alpha,\rho_0)$ pour que le test soit exactement de niveau $\alpha \in (0,1)$. Montrer en particulier que 
$\kappa_n(\alpha,\rho_0)$ ne dépend pas de $\rho_1$.

\CORRECTION{
Le test de Neyman-Pearson a pour zone de rejet
$$\mathcal R = \big\{\prod_{i = 1}^nf_{\rho_1}(Y_i)>c\prod_{i = 1}^nf_{\rho_0}(Y_i)\big\}$$
où $c$ est choisi de sorte que $\PP_{\rho_0}\big(\mathcal R\big)=\alpha$. On a
$$\prod_{i = 1}^nf_{\rho}(Y_i) = C(Y_1,\ldots,Y_n)\rho^{-n/2}\exp\big(-\tfrac{1}{2}\sum_{i = 1}^n\tfrac{\sqrt{Y_i}}{\rho}\big)$$
d'où
\begin{align*}
\mathcal R & = \Big\{\Big(\tfrac{\rho_0}{\rho_1}\big)^{n/2}\exp\Big(-\tfrac{1}{2}\Big(\tfrac{1}{\rho_1}-\tfrac{1}{\rho_0}\Big)\sum_{i = 1}^n\sqrt{Y_i}\Big)>c\Big\} \\
& = \Big\{ -\tfrac{1}{2} \Big(\tfrac{1}{\rho_1}-\tfrac{1}{\rho_0}\Big) U_n >  \log(c \Big(\tfrac{\rho_1}{\rho_0}\big)^{n/2})\Big\} \\
& = \Big\{U_n >   2\Big(\tfrac{1}{\rho_0}-\tfrac{1}{\rho_1}\Big)^{-1} \log(c \Big(\tfrac{\rho_1}{\rho_0}\big)^{n/2})\Big\} \\
\end{align*}
(en notant au passage que $ -\tfrac{1}{2} \Big(\tfrac{1}{\rho_1}-\tfrac{1}{\rho_0}\Big)>0$), d'où le résultat avec
$$\kappa = 2\frac{\rho_0\rho_1}{\rho_1-\rho_0}\log\big(c \Big(\tfrac{\rho_1}{\rho_0}\big)^{n/2}\big).$$
Il reste à expliciter $c$ ou de manière équivalente $\kappa$. Sous $\PP_{\rho_0}$, on a $Y_i=\rho_0^2X_i^4$ où les $X_i$ sont des gaussiennes standard, d'où
$$U_n=\sum_{i = 1}^n\sqrt{Y_i} \stackrel{\mathcal L(\PP_{\rho_0})}{=} \rho_0\sum_{i = 1}^n X_i^2=\rho_1\chi_n^2,$$
où $\chi_n^2$ suit la loi du Chi-deux à $n$ degrés de libertés sous $\PP_{\rho_0}$.
Donc
\begin{align*}
\PP_{\rho_0}\big(\mathcal R\big) & = \PP_{\rho_0}\Big(\chi_n^2 >   2\frac{\rho_1}{\rho_1-\rho_0}\log\big(c \Big(\tfrac{\rho_1}{\rho_0}\big)^{n/2}\big)\Big)=\alpha,
\end{align*}
d'où finalement
$$2\frac{\rho_1}{\rho_1-\rho_0}\log\big(c \Big(\tfrac{\rho_1}{\rho_0}\big)^{n/2}\big) = q_{n,1-\alpha}^{\chi^2},$$
et on conclut
$$c= \Big(\frac{\rho_0}{\rho_1}\Big)^n\exp\big(\tfrac{1}{2}\tfrac{\rho_1-\rho_0}{\rho_1}q_{n,1-\alpha}^{\chi^2}\big).$$
Il est plus simple de chercher directement $\kappa$ de sorte que
$$\PP_{\rho_0}\big(U_n > \kappa\big)=\alpha.$$
On a, en utilisant la représentation de $U_n$ sous $\PP_{\rho_0}$, 
$$\PP_{\rho_0}\big(U_n > \kappa\big) = \PP_{\rho_0}\big(\chi_n^2 > \frac{\kappa}{\rho_0}\big) = \alpha$$
d'où $\kappa = \kappa_n(\alpha,\rho_0) = \rho_0q_{n,1-\alpha}^{\chi^2}$.
}
\item[3.] Montrer que pour tout $\alpha \in (0,1)$, on a
$$q^{\chi^2}_{n,\alpha}  = \big(n + \sqrt{2n}\,\Phi^{-1}(1-\alpha)\big)(1+o(1))$$
et en déduire un équivalent de $ \kappa_n(\alpha,\rho_0)$.

\CORRECTION{
On a, par le théorème central limite
$$\PP\big(n^{-1/2}\Big(\frac{\chi_n^2-n}{\sqrt{2}}\Big) > t \big) \rightarrow 1-\Phi(t),$$
où l'on utilise que $\chi_n^2 = \sum_{i = 1}^n X_i^2$ avec les $X_i$ gaussiennes standard, donc $X_i^2$ est de moyenne $1$ et de variance
$\E[X_i^4]-\E[X_i^2] = 3-1=2$. Il vient
\begin{align*}
\alpha = \PP\big(\chi_n^2> q_{n,1-\alpha}^{\chi^2}\big) & = \PP\Big(n^{-1/2}\frac{\chi_n^2-n}{\sqrt{2}}>n^{-1/2}\frac{ q_{n,1-\alpha}^{\chi^2}-n}{\sqrt{2}}\Big) \\
& \sim 1-\Phi\Big(n^{-1/2}\frac{ q_{n,1-\alpha}^{\chi^2}-n}{\sqrt{2}}\Big), 
\end{align*}
d'où
$$ q_{n,1-\alpha}^{\chi^2} \sim n + \sqrt{2n}\,\Phi^{-1}(1-\alpha).$$
}

\item[4.] Montrer que le test de Neyman-Pearson est consistant.

\CORRECTION{
Sous $\PP_{\rho_1}$, on a $U_n=\rho_1 \widetilde \chi_n$, où $\widetilde \chi_n^2$ suit la loi du Chi-deux à $n$ degrés de liberté. On a 
\begin{align*}
\PP_{\rho_1}\big(\mathcal R\big(\kappa_n(\alpha,\rho_0)\big)^c\big) & = \PP_{\rho_1}\Big(\widetilde \chi_n^2 \leq  \frac{\rho_0}{\rho_1}
q_{n,1-\alpha}^{\chi^2}\Big) \\
& = \PP_{\rho_1}\Big(n^{-1}\widetilde \chi_n^2 \leq \frac{\rho_0}{\rho_1}\big(1+\sqrt{2}n^{-1/2}\Phi^{-1}(1-\alpha)\big)+o(n^{-1})\Big).
\end{align*}
On a $n^{-1}\widetilde \chi_n^2 \rightarrow 1$ en $\PP_{\rho_1}$ probabilité par la loi des grands nombres alors que le terme à droite de l'inégalité tends vers $\rho_0/\rho_1 <1$ par hypothèse. Donc  $\PP_{\rho_1}\big(\mathcal R\big(\kappa_n(\alpha,\rho_0)\big)^c\big) \rightarrow 0$.
}
\end{itemize}
On laisse désormais $\rho_1$ dépendre de $n$ : on pose $\rho_1 = \rho_1(n)=\rho_0+v_n$ où $v_n$ est une suite de nombres positifs qui tend vers $0$.
\begin{itemize}
\item[5.] Montrer que si $\sqrt{n} v_n \rightarrow \infty$, le test de Neyman-Pearson est encore consistant. Est-il consistant si 
$$\sup_n \sqrt{n}v_n <\infty\;\; ?$$ 
\end{itemize}

\CORRECTION{
On a alors
$$\frac{\rho_0}{\rho_1(n)} = \frac{1}{1+v_n/\rho_0} \sim 1-v_n/\rho_0$$
et il faut pousser l'approximation à un ordre supérieur à l'aide du théorème central limite :
\begin{align*}
& \PP_{\rho_1}\big(\mathcal R\big(\kappa_n(\alpha,\rho_0)\big)^c\big)  \\
= & \PP_{\rho_1}\Big(n^{-1/2}\frac{\widetilde \chi_n^2 -1}{\sqrt{2}}\leq \sqrt{n}\frac{\frac{\rho_0}{\rho_1(n)}n^{-1}q_{n,1-\alpha}^{\chi^2}-1}{\sqrt{2}}\Big) \\
 = &  \PP_{\rho_1}\Big(n^{-1/2}\frac{\widetilde \chi_n^2 -1}{\sqrt{2}}\leq n^{-1/2}\frac{\big(1-v_n/\rho_0+o(v_n)\big)(n + \sqrt{2n}\,\Phi^{-1}(1-\alpha))+o(1)-1}{\sqrt{2}}\Big).\\
\end{align*}
Le terme à gauche de l'inégalité tend en loi vers la loi gaussienne standard. Si $\sqrt{n} v_n \rightarrow \infty$, le terme à droite de l'inégalité tend vers $-\infty$. Donc le test est encore consistant. Si $\sup_n \sqrt{n}v_n <\infty$, ce n'est plus nécessairement le cas. En effet, il suffit de prendre $v_n$ de sorte que le terme de droite soit asymptotiquement minoré (vérifier qu'on peut  le faire !) ; dans ce cas, le test n'est plus consistant. 
}

\subsection{Le cadre général}
Soit $\rho>0$. On veut désormais tester
$$H_0:\rho=\rho_0\;\;\;\text{contre}\;\;\;H_1(\epsilon_n):\rho \geq \rho_0+\epsilon_n,$$
où $\epsilon_n$ est une suite de nombres positifs qui tend vers $0$ lorsque $n$ tend vers l'infini.
\begin{itemize}
\item[6.] Montrer que si un test simple de $H_0$ contre $H_1(n)$ de niveau $\alpha$ est consistant\footnote{au sens où $\PP_{\rho_1(n)}(\varphi_n=0)\rightarrow 0$ pour toute suite $\rho_1(n)$ de l'alternative $H_1(\epsilon_n)$.}, alors nécessairement $\sqrt{n}\epsilon_n \rightarrow \infty$.

\CORRECTION{
Soit $\rho_1(n)$ une suite de points dans l'alternative. Si $\varphi_n$ est un test de niveau $\alpha$ consistant, son erreur de seconde espèce au point $\rho_1(n)$ est majorée par l'erreur de seconde espèce du test de Neyman-Pearson, donc le test de Neyman-Pearson est consistant. D'après la Question 5, cela nécessite $\sqrt{n}(\rho_1(n)-\rho_0) \rightarrow \infty$ et donc, en choisissant $\rho_1(n)=\rho_0+\epsilon_n$, on obtient
$\sqrt{n}\epsilon_n \rightarrow \infty$. 
}
\item[7.] Montrer qu'il existe un estimateur $\widehat \rho_n$ de $\rho$ vérifiant
$$\sqrt{n}\big(\widehat \rho_n- \rho\big) \stackrel{\PP_\rho}{\longrightarrow} {\mathcal N}\big(0,v(\rho)\big),$$
pour une fonction continue $v(\rho)$ strictement positive.

\CORRECTION{
Prendre pas exemple $n^{-1}U_n$ et appliquer le théorème central-limite. Alternativement, le modèle est régulier et le maximum de vraisemblance est bien défini. Il est asymptotiquement normal, et $v(\rho) = \mathbb I(\rho)^{-1}$.
}

\item[8.] On suppose  $\sqrt{n}\epsilon_n \rightarrow \infty$. Construire un test asymptotiquement de niveau $\alpha$ de $H_0$ contre $H_1(n)$ et montrer qu'il est consistant.

\CORRECTION{
On peut considérer les test de Wald consttrui à partir de $\widehat \rho_n$. Pour la consistance, on reprend la preuve du cours en utilisant la propriété $\rho_1(n)-\rho_0 \geq \epsilon_n$ et $\sqrt{n}\epsilon_n \rightarrow \infty$.
}
\end{itemize}

%\item[13.]  A quelle condition $\widehat \mu^{N_n}$ est-il meilleur que $\widehat \mu^{{\tt mv}}$ au sens du critère $\widetilde {\mathcal R}$ ? Ce 





\end{document}



