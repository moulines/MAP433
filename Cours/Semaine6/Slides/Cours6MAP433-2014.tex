
\documentclass{beamer}
\usetheme[right,hideothersubsections]{Berkeley}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext}
\usepackage{amsfonts}

%\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[applemac]{inputenc}


\newenvironment{disarray}{\everymath{\displaystyle\everymath{}}\array} {\endarray}
\newtheorem{theo}{Théorème}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{cor}{Corollary}[theo]
\newtheorem{lm}{Lemma}
\newtheorem{lemme}{Lemme}
\newtheorem{nota}{Notation}
\newtheorem{rk}{Remark}
\newtheorem{remarque}{Remarque}
\newtheorem{exa}{Example}
\newtheorem{df}{Définition}
\newtheorem{hypothese}{Hypothèse}
\newtheorem{terminologie}{Terminologie}
\newenvironment{dem}{\textbf{Proof}}{\flushright$\blacksquare$\\}

\DeclareMathOperator{\E}{{\mathbb E}}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\IF}{{\mathbb I}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Pour le modèle linéaire

\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bx}{\boldsymbol{x}}
\DeclareMathOperator{\vp}{\boldsymbol{p}}
\DeclareMathOperator{\vq}{\boldsymbol{q}}
\DeclareMathOperator{\estMC}{\widehat \vartheta_n^{\,\,{\tt mc}}}
\DeclareMathOperator{\estMCNL}{\widehat \vartheta_n^{\,\,{\tt mcnl}}}
\DeclareMathOperator{\estMV}{\widehat \vartheta_n^{\,\,{\tt mv}}}
\DeclareMathOperator{\design}{\mathbb{M}}
\DeclareMathOperator{\est}{\widehat \vartheta_{\mathnormal{n}}}
\DeclareMathOperator{\var}{\mathrm{Var}}
\DeclareMathOperator{\estMVc}{\widehat \vartheta_{n,0}^{\,{\tt mv}}}
\DeclareMathOperator{\Xbar}{\overline{\mathnormal{X}}_\mathnormal{n}}




\title{MAP 433 : Introduction aux méthodes statistiques. Cours 6}
%\author{M. Hoffmann}
%\institute{Université Paris-Est and ETG}
\begin{document}
\date{21 mars 2014}
\maketitle



\begin{frame} 
\frametitle{Aujourd'hui} 
\tableofcontents
\end{frame}




%\subsection{R\'egression lin\'eaire multiple}

\begin{frame}
\frametitle{R\'egression lin\'eaire multiple (= Modèle lin\'eaire)}
\begin{itemize}
\item La fonction de r\'egression est $r(\vartheta,\bx_i) = \vartheta^T\bx_i$.
On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
avec
$$\boxed{Y_i = \vartheta^T\bx_i+\xi_i,\;\;i=1,\ldots, n}$$
où $\vartheta \in \Theta = \R^k,\;\;\bx_i \in \R^k$.
\item {\color{red}Matriciellement}
$$\boxed{\boldsymbol{Y} = \mathbb{M}\vartheta + \boldsymbol{\xi}}$$
avec $\boldsymbol{Y} = (Y_1 \cdots Y_n)^T$, $\boldsymbol{\xi} =
(\xi_1 \cdots \xi_n)^T$ et $\mathbb{M}$ la matrice $(n\times k)$
dont les {\color{red} lignes} sont les $\bx_i$.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Réduction \og design\fg{} aléatoire $\longrightarrow$ déterministe}
%\begin{itemize}
%\item Les modèles de régression à \og design\fg{} déterministe ou aléatoire se traitent  {\color{red} essentiellement de la même manière} :
%\end{itemize}
%\begin{hypothese}[Ancillarité des covariables]
%On suppose que la loi $\PP^{\bX}$ des $\bX_i$ ne dépend pas du paramètre inconnu ${\color{red}\vartheta}$.
%\end{hypothese}
%\begin{itemize}
%\item Sous l'hypothèse d'ancillarité, le caractère aléatoire des $\bX_i$ -- observés -- ne joue aucun rôle : on peut faire l'étude mathématique du modèle {\color{red}conditionnellement aux $\bX_i$}.
%\item {\color{red} Désormais} : on se place  dans le modèle de régression à \og design \fg{} {\color{red}déterministe}.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{\og design\fg{} aléatoire vs. déterministe}
%\begin{remarque}
%{\it A posteriori} pourquoi considérer le modèle de régression à \og design \fg{} aléatoire et ne pas se placer d'emblée en signal + bruit ?
%\begin{itemize}
%\item {\color{red}Le design aléatoire} fournit une interprétation en terme de fonction de régression obtenue via l'espérance conditionnelle.
%\item {\color{red}Essentiel} pour le traitement des {\color{red}modèles à réponse binaire} (ou multiple), voir plus loin.
%\end{itemize}
%\end{remarque}
%\end{frame}





%\begin{frame}
%\frametitle{Estimation de $\sigma^2$}
%\begin{itemize}
%\item {\color{red}Estimation de $\sigma$} (ou $\sigma^2$) à partir des observations
%$$\boxed{Y_i = \vartheta_0\,+\vartheta_1\,x_i+{\color{red}\sigma}\, \varepsilon_i,\;\;i=1,\ldots,n}$$
%{\color{red}avec}
%$$\boxed{\E_\vartheta\big[\varepsilon_i\big]=0,\;\E_\vartheta\big[\varepsilon_i^2\big]=1}$$
%\item \underline{Estimateur naturel} de $\sigma^2$ :
%$$\widehat \sigma_n^2 = \frac{1}{n}\sum_{i = 1}^n\big(Y_i-r(\estMC, x_i)\big)^2$$
%\item Somme de variables aléatoires {\color{red}non indépendantes}.
%\item Difficile de progresser {\color{red}sans hypothèse supplémentaire}. Si les $\varepsilon_i$ sont i.i.d. ${\mathcal N}(0,1)$, alors {\color{red}on sait} \og résoudre \fg{} le problème... plus loin.
%\end{itemize}
%\end{frame}





%\subsection{EMV et EMC}

\begin{frame}
\frametitle{EMC en régression linéaire multiple}
\begin{itemize}
\item Estimateur des {\color{red}moindres carrés} en régression
linéaire multiple : tout estimateur $\estMC$ satisfaisant
$$\sum_{i = 1}^n
\big(Y_i-(\estMC)^T\bx_i\big)^2 = \min_{\vartheta \in \R^k}\sum_{i =
1}^n \big(Y_i-{\vartheta}^T\bx_i\big)^2.$$
\item En notation matricielle :
\begin{eqnarray*} \|\boldsymbol{Y}-\design\estMC\|^2 &=& \min_{\vartheta \in
\R^k}\|\boldsymbol{Y}-\design\vartheta\|^2\\
&=& \min_{v \in V}\|\boldsymbol{Y}-v\|^2
\end{eqnarray*}
o\`u $V=\text{Im}(\design) = \{v\in \R^n: v=\design\vartheta, \
\vartheta\in \R^k\}$.
 Projection orthogonale sur $V$.
 \end{itemize}
 \end{frame}

% \subsection{G\'eometrie de l'EMC}

 \begin{frame}
\frametitle{G\'eom\'etrie de l'EMC}
 \begin{itemize}
 \item L'EMC vérifie
$$\boxed{\design {\estMC} = P_V \boldsymbol{Y}}$$
o\`u $P_V$ est le projecteur orthogonal sur $V$.
\item Mais $\design^T  P_V= \design^T  P_V^T = ( P_V\design)^T =
\design^T$. On en d\'eduit {\color{red}les \'equations normales des
moindres carr\'es}:
$$\boxed{\design^T\design {\estMC} =
\design^T\boldsymbol{Y}.}$$
\item \underline{Remarques.}
  \begin{itemize}
  \item L'EMC est un $Z$-estimateur.
  \item Pas d'{\color{red}unicit\'e} de $\estMC$ si la matrice
  $\design^T\design$ n'est pas inversible.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame} \frametitle{G\'eom\'etrie de l'EMC}
\begin{prop}
Si $\design^T\design$ (matrice $k \times k$) inversible, alors
$\estMC$ {\color{red}est unique} et
$$\boxed{\estMC = \big(\design^T\design\big)^{-1}\design^T \boldsymbol{Y}}$$
\end{prop}
\begin{itemize}
\item Contient  la droite de régression simple.
\item Résultat g\'eometrique, {\color{red}non stochastique}.
%\item $\design^T\design\ge0$; \ \ $\design^T\design$
%inversible $\Longleftrightarrow$ $\design^T\design>0$;
%$$\design^T\design>0 \ \Longleftrightarrow \ {\rm rang}(\design)=k
%\ \Longleftrightarrow \ {\rm dim}(V)=k.$$
%$$\design^T\design>0 \quad \Longrightarrow \quad {\color{red} n \geq k}.$$
\end{itemize}
\end{frame}














%\subsection{Le cas gaussien}





%\section{Régression lin\'eaire : rappels}
%
%\subsection{Régression à \og design \fg{} déterministe}
%
%\begin{frame}
%\frametitle{Modèle de régression}
%\begin{df}
%Modèle de régression (à \og design\fg{} déterministe) = donnée de l'observation
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
%avec $Y_i \in \R, \bx_i\in \R^k$, et
%$$Y_i = r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;\E\big[\xi_i\big]=0,\;\;{\color{red} \vartheta \in \Theta \subset \R^d}.$$
%\begin{itemize}
%%\item $\bx \leadsto r({\color{red}\vartheta},\bx)$ fonction de {\color{red} régression}, connue au paramètre
%%$\vartheta$ près.
%\item $\bx_i$ déterministes, donnés (ou choisis) : plan d'expérience, points du \og design\fg{}.
%\item Hypothèse : les $\xi_i$ sont i.i.d. (pour simplifier). {\color{red} Attention : } les $Y_i$ ne sont {\color{red}pas} identiquement distribuées.
%\item Ecriture de la vraisemblance du modèle...
%\end{itemize}
%\end{df}
%\end{frame}
%
%
%\begin{frame}
%\frametitle{Modèle linéaire}
%\begin{itemize}
%\item On observe
%$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$$
%avec
%$$\boxed{Y_i = \vartheta^T\bx_i+\xi_i,\;\;i=1,\ldots, n}$$
%où $\vartheta \in \Theta = \R^k,\;\;\bx_i \in \R^k$.
%\item {\color{red}Matriciellement}
%$$\boxed{\boldsymbol{Y} = \mathbb{M}\vartheta + \boldsymbol{\xi}}$$
%avec $\boldsymbol{Y} = (Y_1,\ldots, Y_n)^T$, $\boldsymbol{\xi} = (\xi_1,\ldots, \xi_n)^T$ et $\mathbb{M}$ la matrice $(n\times k)$ dont les {\color{red} lignes} sont les $\bx_i$.
%\end{itemize}
%\end{frame}
%
%%%\subsection{EMV et EMC}
%
%\begin{frame}
%\frametitle{Estimateur des moindres carrés}
%\begin{df}
%{\color{red}Estimateur des moindres carrés} (EMC) : tout estimateur
%$\estMC$ satisfaisant $\estMC \in \arg \min_{\vartheta \in
%\R^k}\sum_{i = 1}^n \big(Y_i-\vartheta^T\bx_i\big)^2$.
%\end{df}
%
%\begin{itemize}
%\item
%EMC = cas particulier de $M$-estimateur et de $Z$-estimateur
%\end{itemize}
%
%\begin{prop}
%Si $\design^T\design$ (matrice $k \times k$) inversible (cond. néc. : ${\color{red} n \geq k}$), alors $\estMC$ {\color{red}est unique} et
%$$\boxed{\estMC = \big(\design^T\design\big)^{-1}\design^T \boldsymbol{Y}}$$
%\end{prop}
%\end{frame}
%



%\subsection{Modèle linéaire gaussien}


%\subsection{Propriétés statistiques de l'EMC : cas gaussien}

\begin{frame}
\frametitle{Cadre gaussien : loi des estimateurs}
\begin{itemize}
\item \underline{Hyp. 1} : $\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2\mathrm{Id}_n)$.
\item \underline{Hyp. 2} : $\design^T \design>0$.
\end{itemize}
\begin{prop}
\begin{itemize}
\item[(i)] $\estMC \sim {\mathcal N}\big(\vartheta, \sigma^2 \big(\design^T\design\big)^{-1}\big)$
\item[(ii)] $\|\boldsymbol{Y}-\design \estMC\|^2 \sim
\sigma^2\chi^2(n-k)$ {\color{red} loi du Chi 2 à $n-k$ degrés de liberté}
\item[(iii)] $\estMC$ et $\boldsymbol{Y}-\design \estMC$ sont indépendants.
\end{itemize}
\end{prop}
\begin{itemize}
\item \underline{Preuve} : {\color{red}Thm. de Cochran} (Poly, page 18). Si
$\boldsymbol{\xi}\sim {\mathcal N}(0,\mathrm{Id}_n)$ et $A_j$
matrices $n \times n$ projecteurs t.q. $A_jA_i=0$ pour $i\neq j$,
alors : $A_j\,\boldsymbol{\xi} \sim {\mathcal N}\big(0,A_j\big)$,
{\color{red}indépendants}, $\|A_j\boldsymbol{\xi}\|^2\sim
\chi^2(\mathrm{Rang}(A_j))$.
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Propriétés de l'EMC: cadre gaussien}

Estimateur de la variance $\sigma^2$:
$$\boxed{\widehat \sigma_n^2 = \frac{\|\boldsymbol{Y}-\design \estMC\|^2}{n-{\color{red}k}} = \frac{1}{n-{\color{red}k}}\sum_{i = 1}^n\big(Y_i-(\estMC)^T\bx_i\big)^2}$$
D'apr\`es la derni\`ere Proposition :
\begin{itemize}
\item $\widehat
\sigma_n^2/\sigma^2 \sim \chi^2(n-k)$ {\color{red} loi du Chi 2 à
$n-k$ degrés de liberté}
\item C'est un estimateur {\color{red}sans biais}: $$\E_\vartheta\big[\widehat
\sigma_n^2\big]=\sigma^2.$$
\item $\widehat
\sigma_n^2$ est {\color{red}ind\'ependant} de $\estMC$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propriétés de l'EMC: cadre gaussien}
\begin{itemize}
\item Lois des coordonn\'ees de $\estMC$:
$$
(\estMC)_j -\vartheta_j \sim {\mathcal N}\big(0, \sigma^2 b_j)
$$
o\`u $b_j$ est le $j$\`eme \'el\'ement diagonal de $\big(\design^T
\design\big)^{-1}$. $$ \frac{(\estMC)_j -\vartheta_j}{\widehat
\sigma_n \sqrt{b_j}} \sim t_{n-k}$$ {\color{red}loi de Student \`a
$n-k$ degr\'es de libert\'e}.
$$ t_q = \frac{\xi}{\sqrt{\eta/q}}$$
o\`u $q\ge 1$ un entier, $\xi\sim {\mathcal N}\big(0,1)$, $\eta\sim
\chi^2(q)$ et\\ $\xi$ {\color{red}ind\'ependant} de $\eta$.
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Exemple de donn\'ees de r\'egression}
\begin{center}
\vspace{-2.5cm}
\includegraphics[height=2\textheight]{cours4_data1.pdf}\hspace{4cm}
\end{center}
\end{frame}

\begin{frame}
\frametitle{R\'esultats de traitement statistique initial}
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline
(Intercept) &$152.133$&$2.576$&$59.061$&$< 2e-16***$\\
age&$-10.012$&$59.749$&$ -0.168$&$0.867000$\\\hline
sex &$-239.819$&$61.222$&$-3.917$&$0.000104***$\\
bmi&$519.840$&$66.534$&$7.813$&$4.30e-14***$\\\hline
map&$324.390$&$65.422$&$4.958$&$1.02e-06***$\\
tc&$-792.184$&$416.684$&$-1.901$&$0.057947$\\\hline
ldl&$476.746$&$339.035$&$1.406$&$0.160389$\\
hdl&$101.045$&$212.533 $&$0.475$&$0.634721$\\\hline
tch&$177.064$&$161.476$&$ 1.097$&$0.273456$\\
ltg&$751.279$&$ 171.902$&$4.370$&$ 1.56e-05***$\\\hline
glu&$67.625$&$ 65.984$&$1.025$&$0.305998$\\\hline
\end{tabular}
\end{frame}

\begin{frame}
\frametitle{Questions statistiques}
\begin{itemize}
\item {\bf S\'election de variables.} Lesquelles parmi les 10
variables:\\\vspace{3mm}
\centerline{\texttt{age,sex,bmi,map,tc,ldl,hdl,tch,ltg,glu}}\vspace{3mm}
sont significatives? Formalisation math\'ematique: trouver (estimer)
l'ensemble $N= \{j: \vartheta_{j}\ne 0\}$.
\item {\bf Pr\'evison.} Un nouveau patient arrive avec son vecteur
des 10 variables ${\bf x}_0\in \R^{10}$. Donner la pr\'evison de la
r\'eponse $Y$ =\'etat du patient dans 1 an.
\end{itemize}
\end{frame}

\section{S\'election de variables}

\begin{frame}
\frametitle{RSS (Residual Sum of Squares)} Mod\`ele de
r\'egression\vspace{2mm} \centerline{$ Y_i= r(\vartheta, {\bf
x}_i)+\xi_i, \quad i=1,\dots,n.$}
\begin{itemize}
\item {\bf Résidu:} si $\est$ est un estimateur de
$\vartheta$,
$$\widehat \xi_i = Y_i - r(\est, {\bf x}_i)
\;\;\text{{\color{red}résidu} au point}\;i.$$
\item {\bf RSS:} {\color{red} Residual Sum of Squares}, somme
r\'esiduelle des carr\'es. Caract\'erise la qualit\'e
d'approximation.
$${\rm RSS}(={\rm RSS}_{\est})=\|\widehat \xi\|^2
= \sum_{i = 1}^n\big(Y_i - r(\est,{\bf x}_i)\big)^2.$$
\item En r\'egression {\color{red}lin\'eaire}:
$\boxed{{\rm RSS}= \|{\bf Y}-\design\est\|^2.}$
\end{itemize}
\end{frame}

\subsection{Backward Stepwise Regression}

\begin{frame}
\frametitle{S\'election de variables : Backward Stepwise Regression}
\begin{itemize}
\item On se donne un crit\`ere d'\'elimination de variables
{\color{red}(plusieurs choix de crit\`ere possibles...)}.
\item On \'elimine une
variable, la moins significative du point de vue du crit\`ere
choisi.
\item On calcule l'EMC $\widehat\vartheta_{n,k-1}^{\rm mc}$ dans le nouveau mod\`ele, avec seulement
les $k-1$ param\'etres restants, ainsi que le RSS:\vspace{1mm}
\centerline{${\rm RSS}_{k-1}=\|{\bf
Y}-\design\widehat\vartheta_{n,k-1}^{\rm mc}\|^2$.}\vspace{1mm}
\item On continue \`a \'eliminer des variables, une par une,
jusqu'\`a la {\color{red}stabilisation de RSS}: ${\rm
RSS}_{m}\approx {\rm RSS}_{m-1}$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\begin{itemize}
\item {\bf S\'election "na\"{\i}ve"} : \
\{\texttt{sex,bmi,map,ltg}\}
\item {\bf S\'election par Backward Regression}:\\
 {\color{red}Crit\`ere
d'\'elimination: plus grande valeur de} Pr($>|t|$).
\end{itemize}
%\vspace{2mm}


{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline
(Intercept) &$152.133$&$2.576$&$59.061$&$< 2e-16***$\\
{\color{red}age}&$-10.012$&$59.749$&$
-0.168$&${\color{red}0.867000}$\\\hline
sex &$-239.819$&$61.222$&$-3.917$&$0.000104***$\\
bmi&$519.840$&$66.534$&$7.813$&$4.30e-14***$\\\hline
map&$324.390$&$65.422$&$4.958$&$1.02e-06***$\\
tc&$-792.184$&$416.684$&$-1.901$&$0.057947$\\\hline
ldl&$476.746$&$339.035$&$1.406$&$0.160389$\\
hdl&$101.045$&$212.533 $&$0.475$&$0.634721$\\\hline
tch&$177.064$&$161.476$&$ 1.097$&$0.273456$\\
ltg&$751.279$&$ 171.902$&$4.370$&$ 1.56e-05***$\\\hline
glu&$67.625$&$ 65.984$&$1.025$&$0.305998$\\\hline
\end{tabular}
}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\centerline{\bf Backward Regression: It\'eration 2.}

%\vspace{3mm}

\begin{center}

{\color{red}Crit\`ere d'\'elimination: plus grande valeur de}
Pr($>|t|$).

\vspace{4mm}

{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline (Intercept)
&$152.133$&$2.573$&$59.128$&$< 2e-16$
\\\hline
sex &$-240.835$&$60.853$&$-3.958$&$0.000104$\\
bmi&$519.905$&$64.156$&$5.024$&$8.85e-05$\\\hline
map&$322.306$&$65.422$&$4.958$&$7.43e-07$\\
tc&$-790.896$&$416.144$&$-1.901$&$0.058$\\\hline
ldl&$474.377$&$338.358$&$1.402$&$0.162$\\
{\color{red} hdl}&$99.718$&$212.146 $&$0.470$&${\color{red}
0.639}$\\\hline
tch&$177.458$&$161.277$&$ 1.100$&$0.272$\\
ltg&$749.506$&$ 171.383$&$4.373$&$ 1.54e-05$\\\hline glu&$67.170$&$
65.336$&$1.013$&$0.312$\\\hline
\end{tabular}
}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : Backward Regression}

\centerline{\bf Backward Regression: It\'eration 5 (derni\`ere).}

%\vspace{3mm}

\begin{center}

Variables s\'electionn\'ees:\\\vspace{2mm}
\{\texttt{sex,bmi,map,{\color{red}tc,ldl},ltg}\}


\vspace{4mm}

{\small
\begin{tabular}{|c||c|c|c|c|}
\hline &Estimate&Std. Error&t value&Pr($>|t|$)\\\hline (Intercept)
&$152.133$&$2.572$&$59.159$&$< 2e-16$
\\\hline
sex &$-226.511$&$59.857$&$-3.784$&$0.000176$\\
bmi&$529.873$&$65.620$&$8.075$&$6.69e-15$\\\hline
map&$327.220$&$62.693$&$5.219$&$2.79e-07$\\
tc&$-757.938$&$160.435$&$-4.724$&$3.12e-06$\\\hline
ldl&$538.586$&$146.738$&$3.670$&$0.000272$\\
ltg&$804.192$&$80.173$&$10.031$&$< 2e-16$\\\hline
\end{tabular}
}
\end{center}
\end{frame}

\begin{frame}
\frametitle{S\'election de variables : Backward Regression}

Discussion de \texttt{Backward Regression}:

\begin{itemize}
\item M\'ethode de s\'election purement empirique, pas de justification
th\'eorique.
\item Application d'autres crit\`eres d'\'elimination en
\texttt{Backward Regression} peut amener aux r\'esultats diff\'erents.\\
\underline{Exemple.} {\color{red}Crit\`ere $C_p$} de Mallows--Akaike
: on \'elimine la variable $j$ qui r\'ealise
$$
\min_j \Big({\rm RSS}_{m, (-j)} + 2\widehat\sigma^2_n m\Big).
$$
\end{itemize}
\end{frame}

\subsection{LASSO}

\begin{frame}
\frametitle{S\'election de variables : LASSO}

LASSO = Least Absolute Shrinkage and Selection Operator

\begin{itemize}
\item {\color{red}Estimateur LASSO}: tout estimateur $\widehat\vartheta^{L}_n$
v\'erifiant
$$\widehat\vartheta^{L}_n \in \arg \min_{\vartheta \in \R^k}\left(\sum_{i = 1}^n
\big(Y_i-\vartheta^T\bx_i\big)^2 + \lambda \sum_{j =
1}^k|\vartheta_j|\right) \ \ \text{avec} \ \lambda>0.
$$
\item Si $\design^T\design>0$, l'estimateur LASSO $\widehat\vartheta^{L}_n$ est unique.
\item Estimateur des moindres carr\'es {\color{red}p\'enalis\'e}.
P\'enalisation par $\sum_{j = 1}^k|\vartheta_j|$, la norme $\ell_1$
de $\vartheta$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{S\'election de variables : LASSO}
\begin{itemize}
\item Deux utilisations de
LASSO:
\begin{itemize}
\item {\color{red}Estimation de $\vartheta$}: alternative \`a
$\estMC$ si $k>n$.
\item {\color{red}S\'election de variables}: on ne retient que les
variables qui correspondent aux coordonn\'ees non-nulles du vecteur
$\widehat\vartheta^{L}_n$.
\end{itemize}
\item LASSO admet une {\color{red}justification th\'eorique}: sous certaines hypoth\`eses sur la
matrice $\design$,
$$
\lim_{n\to\infty} \PP\{ \widehat N_n = N \} =1,
$$
o\`u $N= \{j: \vartheta_{j}\ne 0\}$ et $\widehat N_n= \{j:
\widehat\vartheta^{L}_{n,j}\ne 0\}$.
\end{itemize}
\end{frame}

\begin{frame}
    \frametitle{Application de LASSO: "regularization path"}
\begin{center}
\vspace{-1cm}
\includegraphics[height=1.35\textheight,angle=-90]{cours4_sacha_graphe.eps}\hspace{3cm}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Donn\'ees de diab\`ete : LASSO}

Application aux donn\'ees de diab\`ete.

\begin{itemize}
\item  L'ensemble de variables s\'electionn\'e par LASSO:
$$
\{\texttt{sex,bmi,map,tc,hdl,ltg,glu}\}
$$
\item \texttt{Backward Regression}:
$$
\{\texttt{sex,bmi,map,tc,ldl,ltg}\}
$$
\item S\'election na\"{\i}ve:
$$
\{\texttt{sex,bmi,map,tc}\}
$$
\end{itemize}
\end{frame}


%\section{Pr\'evision}

%\begin{frame}
%\frametitle{Pr\'evision}
%
%Mod\`ele de r\'egression \vspace{2mm} \centerline{$ Y_i=
%r(\vartheta, {\bf x}_i)+\xi_i, \quad i=1,\dots,n.$} R\'egression
%{\color{red}lin\'eaire}: $r(\vartheta, {\bf x}_i)=\vartheta^T{\bf
%x}_i$. Exemple: ${\bf x}_i$ vecteur de 10 variables explicatives
%(\texttt{age,sex,bmi,...}) pour patient $i$.
%\begin{itemize}
%\item {\bf Probl\`eme de pr\'evision}:
%Un nouveau patient arrive avec son vecteur des 10 variables ${\bf
%x}_0\in \R^{10}$. Donner la pr\'evison de la valeur de fonction de
%r\'egression $r(\vartheta, {\bf x}_0)=\vartheta^T{\bf x}_0$\\
%(=\'etat du patient dans 1 an).
%\item Soit $\est$ un estimateur de $\vartheta$. {\color{red}Pr\'evision par
%substitution:}
% \centerline{$\boxed{ \widehat Y = r(\est, {\bf x}_0).}$}
%\item \underline{Question statistique}: quelle est la qualit\'e de la pr\'evision?
%{\color{red}Intervalle de confiance} pour $r(\vartheta, {\bf x}_0)$
%bas\'e sur $\widehat Y$?
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Pr\'evision: mod\`ele lin\'eaire gaussienne}
%\begin{itemize}
%\item Traitement sur l'exemple: $r(\vartheta, {\bf x})=\vartheta^T{\bf
%x}$, r\'egression {\color{red}lin\'eaire gaussienne} et
%$\est=\estMC$. $\Longrightarrow$ $\boxed{\widehat Y = {\bf
%x}_0^T\estMC}$
%\item \underline{Hyp. 1} : $\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2\mathrm{Id}_n)$.
%\item \underline{Hyp. 2} : $\design^T \design>0$.
%\end{itemize}
%\begin{prop}
%\begin{itemize}
%\item[(i)] $\widehat Y \sim {\mathcal N}\big({\bf
%x}_0^T\vartheta, \sigma^2 {\bf
%x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0\big)$
%\item[(ii)] $\widehat Y-{\bf
%x}_0^T\vartheta$ et $\boldsymbol{Y}-\design \estMC$ sont
%indépendants.
%\end{itemize}
%\end{prop}
%Rappel: $\|\boldsymbol{Y}-\design \estMC\|^2 \sim
%\sigma^2\chi^2(n-k)$ {\color{red} loi du Chi 2 à $n-k$ degrés de
%liberté}.
%\end{frame}
%
%\begin{frame}
%\frametitle{Pr\'evision: mod\`ele lin\'eaire gaussienne}
%\begin{itemize}
%\item D'apr\`es la Proposition,
%$$
%\eta:=\frac{\widehat Y -{\bf x}_0^T\vartheta} {\sqrt{\sigma^2 {\bf
%x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}}\sim {\mathcal
%N}(0,1).
%$$
%\item On replace $\sigma^2$ inconnu par $\widehat \sigma_n^2 =
%{\|\boldsymbol{Y}-\design \estMC\|^2}/({n-k}).$
%\item {\color{red}$t$-statistique:}
%$$
%t:= \frac{\widehat Y -{\bf x}_0^T\vartheta} {\sqrt{\widehat
%\sigma_n^2 {\bf x}_0^T\big(\design^T\design\big)^{-1}{\bf
%x}_0}}=\frac{\eta}{\sqrt{\chi/(n-k)}}\sim t_{n-k},
%$$
%{\color{red}loi de Student à $n-k$ degrés de liberté}, car $\eta\sim
%{\mathcal N}(0,1)$, $\chi:=\|\boldsymbol{Y}-\design
%\estMC\|^2/\sigma^2\sim \chi^2(n-k)$ et $\eta\ind\chi$.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Pr\'evision: intervalle de confiance}
%\begin{eqnarray*}
%&&\PP \Big(-q_{1-\frac{\alpha}{2}}(t_{n-k}) \le \frac{\widehat Y
%-{\bf x}_0^T\vartheta} {\sqrt{\widehat \sigma_n^2 {\bf
%x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}}\le
%q_{1-\frac{\alpha}{2}}(t_{n-k})\Big) \\\hspace{4mm} &&= \PP(-
%q_{1-\frac{\alpha}{2}}(t_{n-k}) \le t\le
%q_{1-\frac{\alpha}{2}}(t_{n-k})) = 1-\alpha.
%\end{eqnarray*}
%$\Longrightarrow$ {\color{red}intervalle de confiance} de niveau
%$1-\alpha$ pour $r(\vartheta,{\bf x}_0)={\bf x}_0^T\vartheta$ est
%{\color{red}$[r_L, r_U]$}, o\`u:
%\begin{eqnarray*}
%{\color{red}r_L}&=&\widehat Y -
%q_{1-\frac{\alpha}{2}}(t_{n-k})\sqrt{\widehat \sigma_n^2
%{\bf x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0},\\
%{\color{red}r_U}&=& \widehat Y +
%q_{1-\frac{\alpha}{2}}(t_{n-k})\sqrt{\widehat \sigma_n^2 {\bf
%x}_0^T\big(\design^T\design\big)^{-1}{\bf x}_0}.
%\end{eqnarray*}
%\end{frame}



\begin{frame}
\frametitle{Limites des moindres carrés et du cadre gaussien}
\begin{itemize}
\item Calcul {\color{red}explicite} (et efficace) de l'EMC  limité à
une fonction de régression {\color{red}linéaire}.
\item Mod\`ele lin\'eaire donne un cadre assez g\'en\'eral:
\begin{itemize}
\item Mod\`ele
polynomial, \item {\color{red}Mod\`eles avec interactions...}
\end{itemize}
\item {\color{red} Hypothèse de gaussianité} = cadre asymptotique implicite.
\item Besoin d'outils pour les modèles  à réponse {\color{red}$Y$ discrète}.
\end{itemize}
\end{frame}

%\section{R\'egression lin\'eaire non-gaussienne}

\begin{frame}
\frametitle{R\'egression lin\'eaire non-gaussienne} Mod\`ele de
r\'egression lin\'eaire \vspace{3mm} \centerline{$ Y_i= \vartheta^T
{\bf x}_i+\xi_i, \quad i=1,\dots,n.$}

\vspace{-2mm}

\begin{itemize}
\item \underline{Hyp. 1'} : {\color{red}$\xi_i$ i.i.d., $\E[\xi_i]
=0$, $\E[\xi_i^2] = \sigma^2>0$.}
\item \underline{Hyp. 2'} : $\design^T \design>0$, {\color{red}$\lim_n\max_{1\le i \le n}{\bf x}_i^T
\big(\design^T \design\big)^{-1}{\bf x}_i =0$.}
\end{itemize}
\begin{prop}[Normalit\'e asymptotique de l'EMC]
$$
\sigma^{-1}\big(\design^T
\design\big)^{1/2}(\estMC-\vartheta)\stackrel{d}{\longrightarrow}
{\mathcal N}\big(0, \mathrm{Id}_k), \quad n\to\infty.
$$
\end{prop}
\begin{itemize}
\item A comparer avec le cadre gaussien:\vspace{2mm}
\centerline{$\sigma^{-1}\big(\design^T
\design\big)^{1/2}(\estMC-\vartheta)\sim {\mathcal N}\big(0,
\mathrm{Id}_k)$ \text{pour tout $n$.}}
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Vitesses de convergence}
%\end{frame}

%\subsection{Propriété de l'EMC: cadre g\'en\'eral (non-gaussien) }
%
%\begin{frame}
%\begin{itemize}
%\item \underline{Hyp. 1} : $\design^T \design$ inversible
%\item  \underline{Hyp. 2} :
%{\color{red}$\E\big[\boldsymbol{\xi}\big]=0$,
%$\E\big[\boldsymbol{\xi}\boldsymbol{\xi}^T\big] = \sigma^2
%\mathrm{Id}_n$}.
%\end{itemize}
%\begin{prop}
%%Sous les hypothèses précédentes
%\begin{itemize}
%\item $\E_\vartheta\big[\estMC\big]=\vartheta$ et
%$$\E_\vartheta\big[\big(\estMC-\vartheta\big)\big(\estMC-\vartheta\big)^T\big]=\sigma^2 \big(\design^T\design\big)^{-1}$$
%\item Si l'on pose
%$$\boxed{\widehat \sigma_n^2 = \frac{\|\boldsymbol{Y}-\design \estMC\|^2}{n-{\color{red}k}} = \frac{1}{n-{\color{red}k}}\sum_{i = 1}^n\big(Y_i-(\estMC)^T\bx_i\big)^2}$$
%alors $\E_\vartheta\big[\widehat \sigma_n^2\big]=\sigma^2.$
%\end{itemize}
%\end{prop}
%\end{frame}





\section{Régression non-linéaire}


\begin{frame}
\frametitle{Régression non-linéaire}
\begin{itemize}
\item On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),$$
où
$$\boxed{Y_i = r({\color{red}\vartheta},\bx_i)+\xi_i,\;\;i=1,\ldots,n}$$
avec
$$\bx_i\in \R^k,\;\;\text{et}\;\; {\color{red}\vartheta \in \Theta \subset \R^d}.$$
\item Si $\xi_i \sim_{\text{i.i.d.}} {\mathcal N}(0,\sigma^2)$,
$${\mathcal L}_n(\vartheta, Y_1,\ldots, Y_n) \propto \exp\Big(-\frac{1}{2\sigma^2}\sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2\Big)$$
et l'estimateur du {\color{red}maximum de vraisemblance} est obtenu en minimisant la fonction
$$\vartheta \leadsto \sum_{i = 1}^n\big(Y_i-r(\vartheta,\bx_i)\big)^2.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Moindre carrés non-linéaires}
\begin{df}
\begin{itemize}
\item $M$-estimateur associé à la {\color{red}fonction de contraste} $\psi:\Theta \times {\color{red}\R^k}\times \R\rightarrow \R$ : tout estimateur $\est$ satisfaisant
$$\sum_{i = 1}^n \psi(\est, \bx_i, Y_i) = \max_{a \in \Theta} \sum_{i = 1}^n \psi(a,\bx_i,Y_i).$$
\item Estimateur des {\color{red}moindres carrés non-linéaires} : associé au contraste $\psi(a,\bx,y) = -\big(y-r(a,\bx)\big)^2$.
\end{itemize}
\end{df}
\begin{itemize}
\item {\color{red}Extension} des résultats en densité
$\rightarrow$ théorèmes limites pour des sommes de v.a.
indépendantes {\color{red} non-équidistribuées}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle à réponse binaire}
\begin{itemize}
\item On observe
$$(\bx_1,Y_1),\ldots, (\bx_n,Y_n),\;\;{\color{red}Y_i \in \{0,1\}},\;\bx_i \in \R^k.$$
\item Modélisation {\color{red}via la fonction de régression}
$$\bx \leadsto p_{\bx}(\vartheta) = \E_\vartheta\big[Y|\bX = \bx\big] = \PP_\vartheta\big[Y = 1|\bX=\bx\big]$$
%$$Y_i = p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big)$$
\item {\color{red}Représentation}
\begin{align*}
Y_i & =  p_{\bx_i}(\vartheta)+\big(Y_i-p_{\bx_i}(\vartheta)\big) \\
& = r(\vartheta,\bx_i)+\xi_i
\end{align*}
avec
$r(\vartheta, \bx_i) = p_{\bx_i}(\vartheta)$ et $\xi_i = Y_i-p_{\bx_i}(\vartheta).$
\item $\E_\vartheta\big[\xi_i\big]=0$ mais structure des $\xi_i$ {\color{red}compliquée} (dépendance en $\vartheta$).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle à réponse discrète}
\begin{itemize}
\item $Y_i $ v.a. de Bernoulli de paramètre $p_{\bx_i}({\color{red}\vartheta})$.

{\color{red} Vraisemblance}
$${\mathcal L}_n(\vartheta,Y_1,\ldots, Y_n) = \prod_{i = 1}^n p_{\bx_i}({\color{red}\vartheta})^{Y_i}(1-p_{\bx_i}\big({\color{red}\vartheta})\big)^{1-Y_i}$$
$\rightarrow$ méthodes de résolution numérique.
\item {\color{red} Régression logistique} (très utile dans les applications)
$$p_{\bx}(\vartheta) = \psi(\bx^T\vartheta),$$
$$\psi(t)=\frac{e^t}{1+e^t},\;t \in \R\;\;{\color{red}{\text{fonction logistique}}}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Régression logistique et modèles latents}
\begin{itemize}
\item {\color{red}Représentation équivalente de la régression logistique} : on observe
$$\boxed{Y_i = 1_{\big\{Y_i^\star >0\big\}},\;\;i=1,\ldots,n}$$
(les $\bx_i$ sont donnés), et $Y_i^\star$ est une  {\color{red}variable latente} ou cachée,
$$\boxed{Y^\star_i ={\color{red}\vartheta}^T \bx_i + U_i,\;\;i=1,\ldots, n}$$
avec {\color{red}$U_i\sim_{\text{i.i.d.}} F$}, où
$$F(t) = \frac{1}{1+e^{-t}},\;t \in \R.$$
\item
\begin{align*}
\PP_\vartheta\big[Y_i^\star>0] & = \PP_\vartheta\big[\bx_i^T\vartheta + U_i >0\big] \\
& = 1-\PP_\vartheta\big[U_i \leq -\bx_i^T\vartheta\big] \\
& = 1-\big(1+\exp(-\bx_i^T\vartheta)\big)^{-1} =  \psi(\bx_i^T\vartheta).
\end{align*}
\end{itemize}
\end{frame}



\section{Comparaison d'estimateurs}

\begin{frame}
\frametitle{Bilan provisoire : modèles paramétriques dominés}
%, construction d'estimateurs dans les situations suivantes :
\begin{itemize}
\item \underline{{\color{red}Modèle de densité :}} on observe 
$$X_1,\ldots,X_n \sim_{\text{i.i.d.}} \PP_\vartheta,\;\;\vartheta \in \Theta \subset \R^d.$$
{\color{blue}Estimateurs :} moments, $Z$- et $M$-estimateurs, {\color{red}EMV}.
\item\underline{{\color{red}Modèle de régression :}} on observe
$$Y_i = r(\vartheta, \bx_i)+\xi_i,\;\;i=1,\ldots, n,\;\;\xi_i\;\text{i.i.d.},\;\vartheta \in \Theta \subset \R^d.$$
{\color{blue} Estimateurs :}
\begin{itemize}
\item Si $r(\vartheta, \bx) = \bx \vartheta^T$, EMC (coïncide avec l'{\color{red}EMV} si les $\xi_i$ gaussiens)
\item Sinon, $M$-estimateurs, {\color{red}EMV}...
\item Autres méthodes selon des {\color{red}hypothèses} sur le \og design \fg{}...
\end{itemize}
\end{itemize}
\end{frame}


%\begin{frame}
%\frametitle{Bilan provisoire (cont.) : précision d'estimation}
%$\est$ estimateur de $\vartheta$ : {\color{red}précision, qualité} de  $\est$ ?
%Approche par {\color{red}région-intervalle de confiance}
%\begin{itemize}
%\item Pour $\alpha \in (0,1)$, on construit ${\mathcal C}_{n,\alpha}(\est)$ {\color{red} ne dépendant pas de $\vartheta$} (observable)
%tel que
%$$\PP_\vartheta \big[\vartheta \in {\mathcal C}_{n,\alpha}(\est)\big] \geq 1-\alpha$$
%asymptotiquement lorsque $n\rightarrow \infty$, uniformément en $\vartheta$...
%La {\color{red}précision} de l'estimateur est le {\color{red} diamètre} (moyen) de ${\mathcal C}_{n,\alpha}(\est)$.
%\item Par exemple : ${\mathcal C}_{n,\alpha}(\est) =$ boule de centre $\est$ et de rayon {\color{red}à déterminer}.
%\end{itemize}
%\end{frame}

\begin{frame}
$\est$ estimateur de $\vartheta$ : {\color{red}précision, qualité} de  $\est$ ? En pratique, on a \og souvent \fg{} 
\begin{itemize}
\item une information {\color{red} non-asymptotique} de type 
$$\E\big[\|\est-\vartheta\|^2\big] \leq c_n(\vartheta)^2,$$
\item ou bien {\color{red} asymptotique} de type
$$v_n(\est-\vartheta) \stackrel{d}{\longrightarrow} Z_\vartheta,\;\;v_n\rightarrow \infty.$$
\end{itemize}
Permet \og souvent\fg{} de construire un(e) région-intervalle de confiance... 
\end{frame}

\begin{frame}
\frametitle{Region-intervalle de confiance : définition formelle}
$\{\PP_\vartheta^n, \vartheta \in \Theta\}$, $\Theta \subset \R^d$, engendrée par l'observation $Z^{(n)}$.
\begin{itemize}
\item {\color{red}Densité} : $Z^{(n)}=(X_1,\ldots, X_n)$, $\PP_\vartheta^n=\PP_\vartheta \otimes \ldots \otimes \PP_\vartheta$
\item {\color{red}Régression} (à design déterministe) : $Z^{(n)} = (Y_1,\ldots, Y_n)$, $\PP_\vartheta^n = \PP_{\vartheta,{\bx_1}} \otimes \ldots \otimes \PP_{\vartheta,\bx_n}$, où $\PP_{\vartheta,\bx_i}$ loi de $Y_i = r(\vartheta,\bx_i)+\xi_i$.
\end{itemize}
\begin{df}
{\color{red}Région de confiance} de niveau $1-\alpha$, $\alpha \in (0,1)$, (resp. asymptotiquement de niveau $\alpha$) : sous-ensemble {\color{red}observable} ${\mathcal C}_{n,\alpha}(Z^{(n)})$ de $\R^d$ t.q.
$$\forall \vartheta \in \Theta : \PP_\vartheta^n\big[\vartheta \in {\mathcal C}_{n,\alpha}(Z^{(n)})\big] \geq 1-\alpha$$
resp.
$$\forall \vartheta \in \Theta : \liminf_{n \rightarrow \infty}  \PP_\vartheta^n\big[\vartheta \in {\mathcal C}_{n,\alpha}(Z^{(n)})\big] \geq 1-\alpha.$$
\end{df}
\end{frame}


\subsection{Risque et admissibilité}

\begin{frame}
\frametitle{Comparaison d'estimateurs}

Etant donné  $\{\PP_\vartheta^n,\vartheta \in \Theta\}$ comment {\color{red}construire} le {\color{red}meilleur} estimateur ? Dans quel sens ?
%et des estimateurs $\widehat \vartheta_{n, i}, i \in {\mathcal I}$, comment choisir 
\begin{itemize}
\item {\color{red}Intuitivement } : $\est$ fournit une précision optimale si on peut lui associer une région de confiance de longueur (moyenne) minimale. 
\item Différence entre point de vue {\color{red}asymptotique} et {\color{red}non-asymptotique}.
\item {\color{red}Dans ce cours}, nous étudions les deux points de vue sous un angle --un peu réducteur-- particulier :
\begin{itemize}
\item \underline{Non-asymptotique} : contrôle du {\color{red}risque quadratique}
\item \underline{Asymptotique} : comparaison des estimateurs {\color{red}asymptotiquement normaux}.
\end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Risque quadratique, admissibilité}
\underline{Situation} : $\widehat \vartheta_{n,i} = \widehat \vartheta_{n,i}(Z^{(n)})$, $i=1,2$ deux estimateurs basés sur l'observation $Z^{(n)}$ qui engendre l'expérience $\{\PP_\vartheta^n,\vartheta \in \Theta\}$, ${\color{red}\Theta \subset \R^1}$.
\begin{df}
{\color{red}Risque quadratique de l'estimateur} $\est$ au point $\vartheta \in \Theta$ :
$${\mathcal R}(\est,\vartheta) = \E_\vartheta^n\big[\big(\est -\vartheta\big)^2\big].$$
\end{df} 
\begin{df}
L'estimateur $\widehat \vartheta_{n,1}$ est {\color{red}préférable} -- au sens du risque quadratique -- à l'estimateur $\widehat \vartheta_{n,2}$ si
$$\forall \vartheta \in \Theta,\;\;{\mathcal R}\big({\color{red}\widehat \vartheta_{n,1}},\vartheta\big) \leq {\mathcal R}\big({\color{red}\widehat \vartheta_{n,2}},\vartheta\big).$$
\end{df}
\end{frame}

\begin{frame}
\frametitle{Absence d'optimalité}
\begin{itemize}
\item Existe-t-il un estimateur {\color{red} optimal} $\vartheta^\star_n$ au sens où 
$$\forall \vartheta \in \Theta,\;\;{\mathcal R}\big(\vartheta_n^\star,\vartheta\big) \leq \inf_{\est}{\mathcal R}\big(\est,\vartheta\big)\;{\color{red}?}$$
\item Si $\Theta = \{\vartheta_1,\vartheta_2\}$ et {\color{red}s'il n'existe pas d'événement} observable $A$ tel que, {\color{red}simultanément} : $$\PP_{\vartheta_1}^n\big[A\big]=0\;\;\text{et}\;\;\PP_{\vartheta_2}^n\big[A\big] = 1,$$
(on dit que $\PP_{\vartheta_1}^n$ et $\PP_{\vartheta_2}^n$ ne sont {\color{red} pas étrangères}), alors {\color{red}il n'existe pas d'estimateur optimal}. 
\item Condition suffisante pour que $\PP_{\vartheta_1}^n$ et $\PP_{\vartheta_2}^n$ ne soient pas étrangères : $\PP_{\vartheta_1}^n \ll \PP_{\vartheta_2}^n$ et $\PP_{\vartheta_2}^n \ll \PP_{\vartheta_1}^n$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Absence d'optimalité (cont.)}
\begin{itemize}
\item \underline{Preuve :} Pour tout estimateur ${\color{red}\vartheta_n^\star}$, on a
$$\max\big\{{\mathcal R}({\color{red}\vartheta_n^\star},\vartheta_1),{\mathcal R}({\color{red}\vartheta_n^\star},\vartheta_2)\big\} >0 \eqno{(\star)}.$$
\item Supposons ${\color{red}\vartheta_n^\star}$ estimateur optimal et ${\mathcal R}({\color{red}\vartheta_n^\star},\vartheta_1) >0$. Alors $\est^{\text{trivial}} := \vartheta_1$ vérifie
$$0 = {\mathcal R}\big(\est^{\text{trivial}},\vartheta_1\big) < {\mathcal R}\big({\color{red}\vartheta^\star_n},\vartheta_1\big)\;\;\;\text{{\color{red}contradiction !}}
$$
et contredit l'optimalité de $\vartheta_n^\star$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Absence d'optimalité (fin.)}
\begin{itemize}
\item Preuve de $(\star)$ : si ${\mathcal R}\big(\vartheta_n^\star,\vartheta_1\big) = {\mathcal R}\big(\vartheta_n^\star,\vartheta_2\big)=0$, alors
$$\vartheta_n^\star=\vartheta_1\;\PP_{\vartheta_1}^n-\text{p.s.}\;\;\;\text{{\color{red}et}}\;\;\;\vartheta_n^\star=\vartheta_2\;\;\PP_{\vartheta_2}^n-\text{p.s.}.$$ 
Soient $A=\{\omega, \vartheta_n^\star(\omega)=\vartheta_1\}$ et $B=\{\omega, \vartheta_n^\star(\omega)=\vartheta_2\}$. Alors $\PP_{\vartheta_1}^n[A]=1$ et donc $\PP_{\vartheta_2}^n[A]>0$. Aussi, $\PP_{\vartheta_2}^n[B]=1$. Donc $A \cap B \neq \emptyset$.
Il existe $\omega_0$ tel que $\vartheta_1 = \vartheta_n^\star(\omega_0) = \vartheta_2$ {\color{red}contradiction !}  
\item \underline{Attention !} La propriété $\PP^n_{\vartheta_1}$ et $\PP^n_{\vartheta_2}$ non étrangères est {\color{red}minimale}. Mais elle disparaît en général lorsque $n\rightarrow \infty$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Notions d'optimalité}
\begin{itemize}
\item {\color{red}Différentes notions existent}. Deux exemples extrêmes :
\begin{df}[Admissibilité et critère minimax]
\begin{itemize}
\item Un estimateur $\vartheta_n^\star$ est {\color{red}admissible} s'il {\color{red}n'existe pas} d'estimateur $\est$ {\color{red}préférable} à $\vartheta_n^\star$ tel que,
%$$\forall \vartheta \in \Theta,\;\;{\mathcal R}(\est,\vartheta) \leq {\mathcal R}(\vartheta_n^\star,\vartheta)$$
pour {\color{red}un point} $\vartheta_0 \in \Theta$ 
$${\mathcal R}(\est,\vartheta_0) < {\mathcal R}(\vartheta_n^\star,\vartheta_0).$$
\item Un estimateur $\vartheta_n^\star$ est {\color{red}minimax} 
si
$$\sup_{\vartheta \in \Theta}{\mathcal R}(\vartheta_n^\star,\vartheta) = \inf_{\est}\sup_{\vartheta \in \Theta}{\mathcal R}(\est,\vartheta).$$
\end{itemize}
%resp.
%$$\lim_{n \rightarrow \infty} \frac{\sup_{\vartheta \in \Theta}{\mathcal R}(\vartheta_n^\star,\vartheta)}{\inf_{\est}\sup_{\vartheta \in \Theta}{\mathcal R}(\est,\vartheta)}=1.$$
\end{df}
\item {\color{red}Admissibilité} : permet d'éliminer des estimateurs absurdes (mais pas tous). 
\item {\color{red}Minimaxité} : notion très {\color{red}robuste mais conservatrice}, à suivre...
\end{itemize}
\end{frame}

\subsection{Approche asymptotique}

\begin{frame}
\frametitle{Approche asymptotique}
\begin{itemize}
\item Hypothèse simplificatrice : $\vartheta \in \Theta\; {\color{red}\subset \R}$. On se restreint aux {\color{red} estimateurs asymptotiquement normaux} c'est-à-dire vérifiant
$$\sqrt{n}\big(\est - \vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0,v(\vartheta)\big)$$
cf. théorèmes limites obtenus pour les $Z$-,$M$-estimateurs.
\item Si $\widehat \vartheta_{n,1}$ et $\widehat \vartheta_{n,2}$ as. normaux de variance asymptotique
 $v_1(\vartheta) \leq v_2(\vartheta),$ alors la précision de $\widehat \vartheta_{n,1}$ est {\color{red}asymptotiquement meilleure} que celle de $\widehat \vartheta_{n,2}$ au point $\vartheta$ : 
\begin{align*}
\widehat \vartheta_{n,1} & = \vartheta+\sqrt{\frac{{\color{red}v_1(\vartheta)}}{n}}\xi^{(n)}\\
\widehat \vartheta_{n,2} & = \vartheta+\sqrt{\frac{{\color{red}v_2(\vartheta)}}{n}}\zeta^{(n)}
\end{align*}
où $\xi^{(n)}$ et $\zeta^{(n)} \stackrel{d}{\rightarrow} {\mathcal N}(0,1)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Comparaison d'estimateurs : cas asymptotique}
\begin{itemize}
\item Si $v_1(\vartheta) < v_2(\vartheta)$, et si $\vartheta \leadsto v_i(\vartheta)$ est continue, on pose
$${\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,i}) = \left[\widehat \vartheta_{n,i}\pm\sqrt{\frac{v_i(\widehat \vartheta_{n,i})}{n}}\Phi^{-1}(1-\alpha/2)\right],\;\;i=1,2$$
où $\alpha \in (0,1)$ et $\Phi(\cdot)$ est la fonction de répartition de la loi normale standard.
\item ${\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,i})$, $i=1,2$ sont deux {\color{red}intervalles de confiance asymptotiquement de niveau $1-\alpha$} et on a
$$\frac{|{\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,1})|}{|{\mathcal C}_{n,\alpha}(\widehat \vartheta_{n,2})|} \stackrel{\PP_\vartheta^n}{\longrightarrow} \sqrt{\frac{v_1(\vartheta)}{v_2(\vartheta)}}<1.$$
\item La notion de {\color{red} longueur minimale possible d'un intervalle de confiance} est en général difficile à manipuler.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion provisoire}
\begin{itemize}
\item Il est {\color{red}difficile en général} de comparer des estimateurs.
\item Cadre asymptotique + normalité asymptotique $\rightarrow$ comparaison de la {\color{red}variance asymptotique} $\vartheta \leadsto v(\vartheta)$.
\item Sous des hypothèses de
régularité du modèle $\{\PP_\vartheta^n, \vartheta \in \Theta\}$
% + restriction de la classe des estimateurs 
%\end{itemize}
alors 
%(cours 6)
\begin{itemize}
\item Il {\color{red}existe} une variance asymptotique $v^\star(\vartheta)$ {\color{red}minimale} parmi les variances de la classe des $M$-estimateurs as. normaux. 
\item Cette fonction est associée à une {\color{red}quantité d'information intrinsèque} au modèle.
\item La variance asymptotique de l'{\color{red}EMV} est $v^\star(\vartheta)$.
\end{itemize}
\item Ceci règle {\color{red}partiellement} le problème de l'optimalité.
% $\rightarrow$ notion de super-efficacité. 
\end{itemize}
\end{frame}

\section{Modèles réguliers et information de Fisher}

\begin{frame}
\frametitle{Régularité d'un modèle statistique et information}
\begin{itemize}
\item \underline{Cadre simplificateur} : modèle de densité
$$X_1,\ldots, X_n\;\;\text{i.i.d. de loi}\;\; \PP_\vartheta$$
dans la famille $\big\{\PP_\vartheta, \vartheta \in \Theta\big\}$ avec $\Theta {\color{red}\; \subset \R}$ pour simplifier.
\item \underline{Notation} : 
$$f(\vartheta, x) = \frac{d\PP_\vartheta}{d\mu}(x),\;\;x\in \R,\vartheta \in \Theta.$$
\item \underline{{\color{red}Hypothèse}} : la quantité
$${\color{red}\boxed{{\mathbb I}(\vartheta) = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]}}$$
est bien définie.
\end{itemize}
\end{frame}

\subsection{Construction de l'information de Fisher}

\begin{frame}
\frametitle{Information de Fisher}
\begin{df}
\begin{itemize}
\item $\mathbb{I}(\vartheta) = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]$ s'appelle {\color{red}l'information de Fisher} de la famille $\{\PP_\vartheta,\vartheta \in \Theta\}$ au point $\vartheta$. Elle ne dépend pas de la mesure dominante $\mu$. 
\item Le cadre d'intérêt est celui où
$${\color{red}0 < \mathbb{I}(\vartheta)< +\infty.}$$
\item $\mathbb{I}(\vartheta)$ quantifie \og l'information \fg{} qu'apporte chaque observation $X_i$ sur le paramètre $\vartheta$.
\end{itemize}
\end{df}
\underline{Remarque} : on a $\PP_\vartheta\big[f(\vartheta, X)>0\big]=1$, donc la quantité $\log f(\vartheta, X)$ est bien définie. 
\end{frame}

\begin{frame}
\frametitle{Information dans quel sens ? Origine de la notion}
\begin{itemize}
\item Supposons l'EMV $\estMV$ bien défini et {\color{red}convergent}.
\item Supposons l'application $(\vartheta,x) \leadsto f(\vartheta, x)$ possédant {\color{red}toutes les propriétés de régularité et d'intégrabilité} voulues.
\item Alors
$$\boxed{\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0,{\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big)}$$
en loi sous $\PP_\vartheta$, où encore
$$\estMV \stackrel{d}{\approx} \vartheta +\frac{1}{{\color{red}\sqrt{n\mathbb{I}(\vartheta)}}} \,{\mathcal N}(0,1)$$
en loi sous $\PP_\vartheta$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Construction de l'information + jeu d'hypothèses attenant}
\begin{itemize}
\item Heuristique : on établira un jeu d'hypothèses justifiant {\color{red}a posteriori} le raisonnement.
\item \underline{Etape 1} : l'EMV $\estMV$ {\color{red}converge} :
$$\estMV \stackrel{\PP_\vartheta}{\longrightarrow} \vartheta$$
via le théorème de convergence des $M$-estimateurs.
\item \underline{Etape 2} : l'EMV $\estMV$ est un {\color{red}$Z$-estimateur} :
$$0 = \partial_\vartheta \Big(\sum_{i = 1}^n \log f(\vartheta,X_i)\Big)_{\vartheta = {\color{red}\estMV}}.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Construction de $\mathbb{I}(\vartheta)$ cont.}
\begin{itemize}
\item \underline{Etape 3} : développement asymptotique {\color{red}autour de $\vartheta$} :
$${\color{red}0}  \approx \sum_{i = 1}^n \partial_\vartheta \log f(\vartheta, X_i) + {\color{red}(\estMV-\vartheta)} \sum_{i = 1}^n \partial^2_\vartheta \log f(\vartheta, X_i),$$
soit
$${\color{red}\estMV-\vartheta} \approx -\frac{\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)}{ \sum_{i = 1}^n {\color{red}\partial^2_\vartheta \log f}({\color{red}\vartheta}, X_i)}$$
%Puis : étude du numérateur et du dénominateur 
\item \underline{Etape 4} : le numérateur. Normalisation et convergence de 
$\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)\;\;?$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Numérateur}
\begin{lemme}
On a $$\E_\vartheta\big[{\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X)\big]=0.$$
\end{lemme}
\begin{proof}[Preuve]
\begin{align*}
\E_\vartheta\big[{\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X)\big] & = \int_{\R} \partial_\vartheta \log f(\vartheta, x) f(\vartheta, x)\mu(dx) \\
& = \int_{\R} \frac{\partial_\vartheta f(\vartheta, x)}{f(\vartheta, x)} f(\vartheta, x) \mu(dx) \\
& = \int_{\R} \partial_\vartheta f(\vartheta, x) \mu(dx) \\
& = \partial_\vartheta \int_{\R} f(\vartheta, x) \mu(dx) = \partial_\vartheta 1 = 0.
\end{align*}
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Dénominateur}
De même $\int_{\R} \partial_\vartheta^2 f(\vartheta, x) \mu(dx)= 0.$
{\color{red}Conséquence} :
 $$\boxed{{\color{red}\mathbb{I}(\vartheta)} = \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big] = {\color{red}-\E_\vartheta\big[\partial_\vartheta^2 \log f(\vartheta, X)\big]}}$$
En effet
\begin{align*}
&{\color{red} \E_\vartheta\big[\partial_\vartheta^2 \log f(\vartheta, X)\big]} & =
%& \int_{\R} \partial_\vartheta^2 \log f(\vartheta, x)f(\vartheta, x)\mu(dx) 
\\
& = \int_{\R} \frac{\partial_\vartheta^2 f(\vartheta,x) f(\vartheta, x)-\big(\partial_\vartheta f(\vartheta, x)\big)^2}{f(\vartheta, x)^2}f(\vartheta, x)\mu(dx)\\
&= \int_{\R}\partial_\vartheta^2 f(\vartheta, x)\mu(dx)-\int_{\R} \frac{\big(\partial_\vartheta f(\vartheta, x)\big)^2}{f(\vartheta, x)}\mu(dx) \\
& = 0-\int_{\R} \Big(\frac{\partial_\vartheta f(\vartheta,x)}{f(\vartheta, x)}\Big)^2f(\vartheta, x) \mu(dx) ={\color{red} -\E\big[\big(\partial_\vartheta\log f(\vartheta, X)\big)^2\big]}.
\end{align*}
\end{frame}




\begin{frame}
\frametitle{Conséquences}
\begin{itemize}
\item Les $\partial_\vartheta \log f(\vartheta, X_i)$ sont i.i.d. et $\E_\vartheta \big[\partial_\vartheta \log f(\vartheta, X)\big]=0$. TCL :
\begin{align*}\frac{1}{\sqrt{n}} \sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i) & \stackrel{d}{\longrightarrow} \mathcal N\big(0, {\color{red}\E_\vartheta \big[\big(\partial_\vartheta \log f(\vartheta, X)\big)^2\big]}\big)\\
& = {\mathcal N}\big(0,{\color{red}\mathbb{I}(\vartheta)}\big).
\end{align*}
\item Les $\partial_\vartheta^2 \log f(\vartheta, X_i)$ sont i.i.d. LGN :
\begin{align*}
\frac{1}{n}\sum_{i = 1}^n {\color{red}\partial_\vartheta^2 \log f}({\color{red}\vartheta}, X_i) & \stackrel{\PP_\vartheta}{\longrightarrow}
\E_\vartheta\big[{\color{red}\partial_\vartheta^2\log f}({\color{red}\vartheta}, X)\big] \\
&\stackrel{\text{conséquence}}{=}-\mathbb{I}(\vartheta).
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item En combinant les deux estimations + lemme de Slutsky :
\begin{align*}
{\color{red}\sqrt{n}}(\estMV-\vartheta) & \approx -\frac{{\color{red}\frac{1}{\sqrt{n}}}\sum_{i = 1}^n {\color{red}\partial_\vartheta \log f}({\color{red}\vartheta}, X_i)}{{\color{red}\frac{1}{n}} \sum_{i = 1}^n {\color{red}\partial^2_\vartheta \log f}({\color{red}\vartheta}, X_i)}\\
& \stackrel{d}{\longrightarrow} \frac{{\mathcal N}\big(0,{\color{red}\mathbb{I}(\vartheta)}\big)}{{\color{red}\mathbb{I}(\vartheta)}} \\
& \stackrel{\text{loi}}{=} {\mathcal N}\Big(0,{\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big).
\end{align*}
\item Le raisonnement est {\color{red} rigoureux dès lors que} : i) on a la convergence de $\estMV$, ii) on peut justifier le lemme et sa conséquence, iii) $\mathbb{I}(\vartheta)$ est bien définie et non dégénérée et iv) on sait contrôler le terme de reste dans le développement asymptotique, {\color{red}partie la plus difficile}. 
\end{itemize}
\end{frame}

\subsection{Modèle régulier}

\begin{frame}
\frametitle{Modèle régulier}
\begin{df} La famille de densités $\{f(\vartheta,\cdot),\vartheta \in \Theta\}$,  par rapport à la mesure dominante $\mu$, $\Theta \subset \R$, est {\color{red}régulière} si
\begin{itemize}
\item $\Theta$ ouvert et $\{f(\vartheta, \cdot)>0\}=\{f(\vartheta', \cdot)>0\}$, $\forall \vartheta, \vartheta' \in \Theta$.
\item $\mu$-p.p. $\vartheta \leadsto f(\vartheta,\cdot)$, $\vartheta \leadsto \log f(\vartheta,\cdot)$ sont ${\mathcal C}^2$.
 \item $\forall \vartheta \in \Theta, \exists {\mathcal V}_\vartheta \subset \Theta$ t.q. pour $a \in {\mathcal V}_\vartheta$
$$|\partial_a^{2}\log f(a,x)|+|\partial_a \log f(a,x)|+\big(\partial_a\log f(a,x)\big)^2\leq g(x)$$
où
$$\int_{\mathbb{R}}g(x)\sup_{a \in {\mathcal V}(\vartheta)}f(a,x)\mu(dx)<+\infty.$$
\item L'information de Fisher est non-dégénérée :
$$\forall \vartheta \in \Theta,\;\;\mathbb{I}(\vartheta) >0.$$
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Résultat principal}
\begin{prop}
\begin{itemize}
\item Si l'expérience engendrée par l'observation $X_1,\ldots, X_n\sim_{\text{i.i.d.}}\PP_\vartheta$ est associée à une famille de probabilités $\{\PP_\vartheta, \vartheta \in \Theta\}$ sur $\R$ {\color{red} régulière} au sens de la définition précédente, alors
$$\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0,\frac{1}{\mathbb{I}(\vartheta)}\Big).$$
\item Si $\est$ est un $Z$-estimateur {\color{red}régulier} asymptotiquement normal de variance $v(\vartheta)$, alors
$$\forall \vartheta \in \Theta,\;\;v(\vartheta) \geq \frac{1}{\mathbb{I}(\vartheta)}.$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve de la proposition}
\begin{itemize}
\item Le premier point consiste à {\color{red}rendre rigoureux} le raisonnement précédent. {\color{red}Point délicat : } le contrôle du terme de reste.
\item {\color{red}Optimalité de la variance de l'EMV parmi celle des $Z$-estimateurs} : on a vu que si $\est$ est un $Z$-estimateur régulier associé à la fonction $\phi$, alors, sa variance asymptotique $v(\vartheta) = v_\phi(\vartheta)$ vaut
$$v_\phi(\vartheta) = \frac{\E_\vartheta\big[\phi(\vartheta,X)^2\big]}{\big(\E_\vartheta\big[\partial_\vartheta \phi(\vartheta, X)\big]\big)^2}.$$
\item {\color{red}A montrer} : pour toute fonction $\phi$ :
$$\boxed{\frac{\E_\vartheta\big[\phi(\vartheta,X)^2\big]}{\big(\E_\vartheta\big[\partial_\vartheta \phi(\vartheta, X)\big]\big)^2} \geq \frac{1}{\mathbb{I}(\vartheta)}}.$$ 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de l'inégalité}
\begin{itemize}
\item Par construction 
$$\partial_a\E_\vartheta\big[\phi(a,X)\big]_{\big|a=\vartheta}=0.$$
\item (avec $\dot\phi(\vartheta, x)=\partial_{\vartheta}\phi(\vartheta,x)$)
\begin{align*}
0&=\int_{\R}\big[\dot \phi(\vartheta,x)f(\vartheta,x)+\phi(\vartheta,x)\partial_\vartheta f(\vartheta,x)\big]\mu(dx)\\
& = \int_{\R}\big[\dot \phi(\vartheta,x)f(\vartheta,x)+\phi(\vartheta, x){\color{red}\partial_\vartheta \log f(\vartheta, x) f(\vartheta,x)}\big]\mu(dx).
\end{align*}
\item \underline{Conclusion}
$$\boxed{{\color{red}\E_\vartheta\big[\dot \phi(\vartheta,X)\big] = -\E_\vartheta\big[\phi(\vartheta, X)\partial_\vartheta \log f(\vartheta,X)\big]}}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve de l'inégalité (fin)}
\begin{itemize}
\item On a 
$$\E_\vartheta\big[\dot \phi(\vartheta,X)\big] = -\E_\vartheta\big[\phi(\vartheta, X)\partial_\vartheta \log f(\vartheta,X)\big]$$
\item \underline{Cauchy-Schwarz} :
$$\big(\E_\vartheta\big[\dot \phi(\vartheta,X)\big] \big)^2 \leq \E_\vartheta\big[\phi(\vartheta, X)^2\big] \E_\vartheta\big[\big(\partial_\vartheta \log f(\vartheta,X)\big)^2\big],$$
c'est-à-dire
$$\boxed{v_\phi(\vartheta)^{-1} = \frac{\big(\E_\vartheta\big[\dot \phi(\vartheta,X)\big] \big)^2}{\E_\vartheta\big[\phi(\vartheta, X)^2\big]} \leq \mathbb{I}(\vartheta).}$$
\end{itemize}
\end{frame}

\subsection{Cadre général et interprétation géométrique}

\begin{frame}
\frametitle{Information de Fisher dans un modèle général}
\begin{df}
\begin{itemize}
\item {\color{red}Situation} : suite d'expériences statistiques
$${\mathcal E}^n=\big(\mathfrak{Z}^n, {\mathcal Z}^n, \{\PP_\vartheta^n,\vartheta \in \Theta\}\big)$$
dominées par $\mu_n$, associées à l'observation $Z^{(n)}$,
$$f_n(\vartheta,z)=\frac{d\PP_\vartheta^n}{d\mu^n}(z),\;\;z\in\mathfrak{Z}^n,{\color{red}\vartheta \in \Theta \subset \R}.$$
\item {\color{red} Information de Fisher} (si elle existe) de l'expérience au point $\vartheta$ :
$$\mathbb{I}(\vartheta\,|\,{\mathcal E}_n)=\E_\vartheta^n\big[\big(\partial_\vartheta \log  f_n(\vartheta, Z^{(n)})\big)^2\big]$$
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Le cas multidimensionnel}
\begin{itemize}
\item {\color{red}Même contexte} que précédemment, avec $\Theta \subset \R^d$, et ${\color{red} d \geq 1}$.
\item {\color{red} Matrice d'information de Fisher}
$$\mathbb{I}(\vartheta)= \E_\vartheta \big[\nabla_{\vartheta}\log f(\vartheta, Z^{n}) \nabla_{\vartheta}\log f(\vartheta, Z^{n})^T\big]$$
{\color{red}matrice symétrique positive}. 
\item Si $\mathbb{I}(\vartheta)$ définie et si ${\mathcal E}^n$ {\color{red}modèle de densité}, en généralisant à la dimension $d$ les conditions de régularité, on a {\color{red}}
$$\sqrt{n}\big(\estMV-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0, {\color{red}\mathbb{I}(\vartheta)^{-1}}\Big).$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Interprétation géométrique}
\begin{itemize}
\item On pose $\mathbb{D}(a,\vartheta)=\E_\vartheta\big[\log f(a,X)\big]$. On a vu (inégalité d'entropie) que
\begin{align*}
\mathbb{D}(a,\vartheta) & = \int_{\R}\log f(a,x) f(\vartheta,x)\mu(dx) \\
&  \leq \int_{\R}\log f({\color{red}\vartheta}, x) f(\vartheta,x)\mu(dx) = \mathbb{D}({\color{red}\vartheta},\vartheta).
\end{align*}
\item On a
$$\boxed{\mathbb{I}(\vartheta)=\partial_a^2 \mathbb{D}(a,\vartheta)_{\big|a=\vartheta}.
}$$
\begin{itemize}
\item Si $\mathbb{I}(\vartheta)$ est \og petite \fg{}, le {\color{red}rayon de courbure de $a \leadsto \mathbb{D}(a,\vartheta)$ est grand} dans un voisinage de $\vartheta$ : la stabilisation d'un maximum empirique (l'EMV) est plus difficile, rendant moins précis l'estimation.
\item Si $\mathbb{I}(\vartheta)$ est \og grande \fg{}, le {\color{red} rayon de courbure est petit} et le maximum de l'EMV est mieux localisé.
 \end{itemize}
%\item {\color{red}Faiblesse de cette approche} : nécessite de la régularité en $\vartheta$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Information de Fisher et régression}
\begin{itemize}
\item ${\mathcal E}^n$ expérience engendrée par $(\bx_1,Y_1),\ldots, (\bx_n,Y_n)$ avec 
$$Y_i = r(\vartheta,\bx_i)+\xi_i,$$
$\xi_i$ : densité {\color{red}$g$ par rapport à la mesure de Lebesgue} + \og design \fg{} déterministe.
\item Observation : $Z^n = (Y_1,\ldots, Y_n)$, $\mu^n=dy_1\ldots dy_n$, $z=(y_1,\ldots, y_n)$
et
$$f_n(\vartheta, Z^n) = \prod_{i = 1}^n g\big(Y_i-r(\vartheta, \bx_i)\big)$$
\item {\color{red} Information de Fisher}
$$\mathbb{I}(\vartheta|{\mathcal E}^n) = \E_\vartheta\big[\big(\partial_\vartheta \log f_n(\vartheta, Z^n)\big)^2\big]$$
\end{itemize}
\end{frame}

\subsection{Exemples, applications}


\begin{frame}
\frametitle{Information de Fisher et régression}
\begin{itemize}
\item {\color{red}Formule explicite} pour la log-vraisemblance
$$\partial_\vartheta \log f_n(\vartheta, Z^n)  = \sum_{i = 1}^n \partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)$$
\item {\color{red}Propriété analogue avec le modèle de densité} : $\E_\vartheta\big[\partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)\big]=0$. 
\item {\color{red}Information de Fisher} par indépendance + centrage :
\begin{align*}
\mathbb{I}(\vartheta|{\mathcal E}^n) & = \sum_{i = 1}^n\E_\vartheta^n\big[\big(\partial_\vartheta \log g\big(Y_i-r(\vartheta, \bx_i)\big)\big)^2\big] \\
& = \ldots
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemples et applications}
A {\color{red}titre d'exercice}, savoir calculer l'information de Fisher pour :
\begin{itemize}
\item L'estimation du paramètre d'une loi de Poisson dans le modèle de densité.
\item L'estimation de la moyenne-variance pour un échantillon gaussien.
\item {\color{red}La régression logistique}
\item L'estimation du paramètre d'une loi exponentielle {\color{red} avec ou sans} censure.
\end{itemize} 
\end{frame}

\begin{frame}
\frametitle{Efficacité à un pas}
\begin{itemize}
%\item {\color{red}Situation : modèle de densité} $Z^n = (X_1,\ldots, X_n)$.
\item Dans un modèle régulier, le {\color{red}calcul numérique} de l'EMV peut être difficile à réaliser.
\item Si l'on dispose d'un estimateur $\est$ {\color{red}asymptotiquement normal} et si les évaluations 
$$\ell'_n(\vartheta) = \tfrac{1}{n}\sum_{i = 1}^n \partial_\vartheta \log f(\vartheta, X_i),\;\;\ell''_n(\vartheta)=\tfrac{1}{n}\sum_{i = 1}^n\partial_\vartheta^2 \log f(\vartheta, X_i)$$
sont {\color{red}faciles}, alors on peut {\color{red} corriger} $\est$ de sorte d'avoir le même comportement asymptotique que l'EMV :
$$\widetilde \vartheta_n = \est - \frac{\ell'_n(\est)}{\ell''_n(\est)}\;\;\;\text{(algorithme de Newton)}$$
satisfait
$$\boxed{\sqrt{n}\big(\widetilde \vartheta_n-\vartheta\big) \stackrel{d}{\longrightarrow} {\mathcal N}\Big(0, {\color{red}\frac{1}{\mathbb{I}(\vartheta)}}\Big)}$$
\end{itemize}
\end{frame}













\end{document}









