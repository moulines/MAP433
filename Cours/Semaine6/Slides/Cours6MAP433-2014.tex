

\input{../../def/defslide}

\title{MAP 433 : Introduction aux mÈthodes statistiques. Cours 7}
%\author{M. Hoffmann}
%\institute{UniversitÈ Paris Dauphine}
\begin{document}
\date{2 Octobre 2015}
\maketitle



\begin{frame}
\frametitle{Aujourd'hui}
\tableofcontents
\end{frame}


\section{Tests statistiques}

\subsection{Notion de test et d'erreur de test}



\begin{frame}
\frametitle{Exemple introductif}
\begin{itemize}
\item On observe 10 lancers d'une piËce de monnaie et on obtient le rÈsultat suivant :
$$(P, P, F, F, P, F, P, P, F, P).$$
\alert{La piËce est-elle ÈquilibrÈe} ?
\item \alert{RÈpondre} à cette question revient à \alert{construire une procÈdure de dÈcision} :
$$\varphi = \varphi(P, P, F, F, P, F, P, P, F, P)$$
$$ =
\left\{
\begin{array}{ll}
0 & \text{\alert{on accepte} l'hypothèse \og la piËce est ÈquilibrÈe\fg{}} \\
1 & \text{\alert{on rejette} l'hypothèse \og la piËce est ÈquilibrÈe\fg{}}
\end{array}
\right.$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{RÈsolution}
\begin{itemize}
\item On associe l'expÈrience statistique (par exemple)
$${\mathcal E}^{10} = \big(\{0,1\}^{10}, \text{{\small parties de}}(\{0,1\}^{10}), \{\PP_\vartheta^{10}, \vartheta \in [0,1]\}\big),$$
avec ($P=0$, $F=1$)
$$\PP^{10}_\vartheta=\big(\vartheta\delta_{0}(dx)+(1-\vartheta)\delta_{1}(dx)\big)^{\otimes 10}.$$
\item\underline{Hypothèse nulle} :  \alert{ \og la piËce est ÈquilibrÈe \fg{}}
$$\boxed{H_0: \vartheta = \frac{1}{2}}$$
\item \underline{Hypothèse alternative} :  \alert{\og la piËce est truquÈe\fg{}}
$$\boxed{H_1 : \vartheta \neq \frac{1}{2}}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{RÈsolution (cont.)}
\begin{itemize}
\item On note $Z$ l'observation.
\item On \alert{construit} une \alert{rËgle de dÈcision simple} :
$$\varphi = 1_{\big\{Z \in {\mathcal R}\}} =
\left\{
\begin{array}{ll}
0 & \text{on accepte l'hypothèse} \\
1 & \text{on rejette l'hypothèse.}
\end{array}
\right.$$
\item ${\mathcal R} \subset {\mathfrak Z}$ (espace des observables) : \alert{zone de rejet} ou \alert{rÈgion critique}.
\item \underline{Exemple}\footnote{lÈger abus de notation...}
$${\mathcal R} = \big\{\big|\widehat \vartheta(Z)-\tfrac{1}{2}\big| > t_0\big\},\;\;\widehat \vartheta(Z) = \estMV \big(\stackrel{exemple}{=} 0,6\big)$$
où $t_0$ est un seuil à choisir... \alert{Comment ?}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Erreur de dÈcision}
\begin{itemize}
\item
Lorsque l'on prend la dÈcision $\varphi$, on peut se \alert{tromper de deux manières} :
$$\text{\alert{Rejeter}}\; H_0\;\;(\varphi=1)\;\;\text{\alert{alors que}}\;\;\vartheta = \frac{1}{2}$$
ou encore
$$\text{\alert{Accepter}}\;H_0\;\;(\varphi=0)\;\;\text{\alert{alors que}}\;\;\vartheta  \neq \frac{1}{2}.$$
\item \underline{Erreur de première espèce}  \alert{(=rejeter à tort)}
$$\PP_{\tfrac{1}{2}}^{10}\big[\varphi=1\big]$$
\item \underline{Erreur de seconde espèce}  \alert{(=accepter à tort)}
$$\big(\PP_\vartheta^{10}\big[\varphi=0\big],\;\;\vartheta \neq \frac{1}{2}\big).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion provisoire}
\begin{itemize}
\item Un \og bon test \fg{} $\varphi$ doit garantir \alert{ simultanÈment} des erreurs de première et seconde espèce \alert{petites}.
\item \alert{Un test optimal existe-t-il ?}
\item Si \alert{non}, comment aborder la notion d'optimalitÈ et comment construire un test optimal ?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{DÈfinition formelle}
\begin{itemize}
\item \underline{Situation} : ${\mathcal E} = \big({\mathcal Z}, \mathfrak{Z}, \{\PP_\vartheta, \vartheta \in \Theta\}\big)$ engendrÈe par l'observation $Z$.
\item \alert{Hypothèse nulle et alternative} : $\Theta_0 \subset \Theta$ et $\Theta_1 \subset \Theta$  t.q.
$$\Theta_0 \cap \Theta_1 = \emptyset.$$
\begin{df}[Test simple] Un test (simple) de l'hypothèse nulle $H_0: \vartheta \in \Theta_0$ contre l'alternative $H_1:\vartheta \in \Theta_1$ est une statistique $\varphi  = \varphi(Z) \in \{0,1\}$.
 (Fonction d') \alert{erreur de première espèce} :
$$\vartheta \in \Theta_0 \leadsto \PP_\vartheta\big[\varphi = 1\big]$$
 (Fonction d') \alert{erreur de seconde espèce}
$$\vartheta \in \Theta_1 \leadsto \PP_\vartheta \big[\varphi = 0\big] = 1- \text{\alert{puissance}}_\varphi(\vartheta).$$
\end{df}
\end{itemize}
\end{frame}

\subsection{Hypothèse simple contre alternative simple}

\begin{frame}
\frametitle{Hypothèse simple contre alternative simple}
\begin{itemize}
\item Cas où $\Theta = \{\vartheta_0, \vartheta_1\}$ avec $\vartheta_0 \neq \vartheta_1$.
\item Existe-t-il un test $\varphi^\star$ \alert{optimal}, au sens où :
$\forall \varphi$ test simple, on a \alert{simultanÈment}
$$\PP_{\vartheta_0}\big[\varphi^\star=1\big]\leq \PP_{\vartheta_0}\big[\varphi=1\big]$$
\alert{et}
$$\PP_{\vartheta_1}\big[\varphi^\star=0\big] \leq \PP_{\vartheta_1}\big[\varphi=0\big]\;\;\;\alert{?}$$
\item Si $\PP_{\vartheta_0}$ et $\PP_{\vartheta_1}$ ne sont pas \alert{Ètrangères} (cf. Cours 6) un tel test $\varphi^\star$ \alert{ne peut pas exister}.
%\item \underline{Esquisse de dÈmonstration}
 \end{itemize}
\end{frame}

\begin{frame}
\frametitle{Absence d'optimalitÈ stricte}
\begin{itemize}
%\item \underline{Esquisse de dÈmonstration}
\item \alert{Equivalence} tests simples $\leftrightsquigarrow$ estimateurs $\widehat \vartheta$ de $\vartheta$ \alert{via la reprÈsentation}:
$$\widehat \vartheta = \vartheta_0 1_{{\mathcal R}^c}+\vartheta_1 1_{{\mathcal R}} \leftrightsquigarrow \varphi = 1_{\mathcal R}.$$
\item \alert{Fonction de risque}
$${\mathcal P}(\varphi, \vartheta) = \E_\vartheta\big[1_{\widehat \vartheta \neq \vartheta}\big],\;\;\vartheta = \vartheta_0,\vartheta_1.$$
\item La fonction de perte $\ell(\widehat \vartheta, \vartheta) = 1_{\widehat \vartheta \neq \vartheta}$ joue le même rôle que la perte quadratique $(\widehat \vartheta - \vartheta)^2$ dans le Cours 6.
\item Test optimal $\varphi^\star$ $\leftrightsquigarrow$ estimateur optimal $\vartheta^\star$ pour ${\mathcal P}$.
\item \alert{Comme pour le cas du risque quadratique}, dès que $\PP_{\vartheta_0}$ et $\PP_{\vartheta_1}$ ne sont pas Ètrangères, un estimateur optimal \alert{n'existe pas} (cf. Cours 6).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Riposte : principe de Neyman}
\begin{itemize}
\item
On \og \alert{disymÈtrise} \fg{} les hypothèses $H_0$ et $H_1$
: $H_0$ est \og plus importante \fg{} que $H_1$ dans le sens suivant : on \alert{ impose} une \alert{erreur de première espèce prescrite}.
\end{itemize}
\begin{df}
Pour $\alpha \in [0,1]$, un test $\varphi = \varphi_\alpha$ de l'hypothèse nulle $H_0:\vartheta \in \Theta_0$ contre une alternative $H_1$ est de niveau $\alpha$ si
$$\sup_{\vartheta \in \Theta_0}\PP_\vartheta\big[\varphi_\alpha = 1\big] \leq \alpha.$$
\end{df}
\begin{itemize}
\item Un test de niveau $\alpha$ ne dit \alert{rien} sur l'erreur de seconde espèce (comportement sur l'alternative).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Principe de Neyman (cont.)}
\begin{itemize}
\item Choix de la \og disymÈtrisation \fg{} = choix de modÈlisation.
%Cas Èvident (dÈpistage d'une maladie), cas moins Èvident (dÈtection de missile, efficacitÈ d'un mÈdicament).
\item \underline{\alert{Principe de Neyman}}  : $\alpha \in (0,1)$, parmi les test de niveau $\alpha$, chercher celui (ou ceux) ayant \alert{une erreur de seconde espèce minimale}.
\end{itemize}
\begin{df}
Un test de niveau $\alpha$ est dit \alert{UniformÈment Plus Puissant} (UPP) si son erreur de seconde espèce est minimale parmi celles des tests de niveau $\alpha$.
\end{df}
\begin{itemize}
\item Pour le cas d'une \alert{hypothèse simple} contre une \alert{alternative simple}, un test UPP existe.
\end{itemize}
\end{frame}

\subsection{Lemme de Neyman-Pearson}

\begin{frame}
\frametitle{Principe de construction}
% \underline{\alert{Principe de construction}}
\begin{itemize}
\item $f(\vartheta, z) = \frac{d\PP_{\vartheta}}{d\mu}(z),\;\;z \in \mathfrak{Z},\;\;\vartheta =\vartheta_0,\vartheta_1$, $\mu$ mesure dominante. L'\alert{EMV} --si bien dÈfini-- s'Ècrit
$$\estMV = \vartheta_0 1_{\{f(\vartheta_1,Z)<f(\vartheta_0, Z)\}}+\vartheta_1 1_{\{f(\vartheta_0,Z)< f(\vartheta_1, Z)\}}.$$
\item On choisit une \alert{ rÈgion critique} de la forme
$$\boxed{{\mathcal R}(c) = \big\{f(\vartheta_1, Z) > c f(\vartheta_0,Z)\big\},\;\;c>0}$$
et on \alert{calibre} $c = c_\alpha$ de sorte que
$$\boxed{\PP_{\vartheta_0}\big[Z \in {\mathcal R}(c_\alpha)\big]=\alpha.}$$
\item Le test ainsi construit (si cette Èquation admet une solution) \alert{est de niveau $\alpha$}. On \alert{montre} qu'il est UPP.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lemme de Neyman-Pearson}
\begin{prop} Soit $\alpha \in [0,1]$. S'il existe $c_\alpha $ solution de
$$\boxed{\PP_{\vartheta_0}\big[f(\vartheta_1, Z) > c_\alpha f(\vartheta_0, Z)\big] = \alpha}$$
alors le test de rÈgion critique ${\mathcal R}_\alpha = \big\{f(\vartheta_1,Z) > c_\alpha f(\vartheta_0, Z)\big\}$ \alert{est de niveau $\alpha$} et \alert{UPP} pour tester $H_0:\vartheta=\vartheta_0$ contre $H_1:\vartheta=\vartheta_1$.
\end{prop}
\begin{itemize}
\item Si $U = f(\vartheta_1,Z)/f(\vartheta_0,Z)$ bien dÈfinie et ${\mathcal L}(U)\ll dx$ (sous $\PP_{\vartheta_0}$), alors
$\PP_{\vartheta_0}\big[U > c_\alpha\big]=\alpha$ admet une solution.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple de mise en oeuvre}
\begin{itemize}
\item On observe
$$Z=(X_1,\ldots, X_n) \sim_{\text{i.i.d.}} {\mathcal N}(\vartheta, 1).$$
\item \alert{Construction du test de N-P.} de $H_0:\vartheta = \vartheta_0$ contre $H_1:\vartheta = \vartheta_1$, avec $\vartheta_0 < \vartheta_1$.
\item \alert{Mesure dominante} $\mu^n=\;$mesure de Lebesgue sur $\R^n$ et
$$f(\vartheta, Z)=\tfrac{1}{(2\pi)^{n/2}}\exp\big(-\frac{1}{2}\sum_{i = 1}^n X_i^2+n\vartheta\overline{X}_n-\frac{n\vartheta^2}{2}\big).$$
\item \alert{Rapport de vraisemblance}
$$\frac{f(\vartheta_1,Z)}{f(\vartheta_0,Z)} = \exp\big(n(\vartheta_1-\vartheta_0)\overline{X}_n-\frac{n}{2}(\vartheta_1^2-\vartheta_0^2)\big).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple (cont.)}
\begin{itemize}
\item \alert{Zone de rejet} du test de N-P. :
\begin{align*}
& \big\{f(\vartheta_1,Z) >  cf(\vartheta_0,Z)\big\}\\
  =& \big\{n(\vartheta_1-\vartheta_0)\overline{X}_n-\frac{n}{2}(\vartheta_1^2-\vartheta_0^2) > \log c\big\} \\
 =& \big\{\overline{X}_n > \frac{\vartheta_0+\vartheta_1}{2}+\tfrac{\log c}{n(\vartheta_0-\vartheta_1)}\big\}.
\end{align*}
\item \alert{Choix de $c$}. On rÈsout
$$\PP_{\vartheta_0}\big[\overline{X}_n > \tfrac{1}{2}(\vartheta_0+\vartheta_1)+\tfrac{\log c}{n(\vartheta_0-\vartheta_1)}\big]=\alpha.$$
\item \alert{Approche standard} : on raisonne sous $\PP_{\vartheta_0}$. On a
$$\overline{X}_n = \vartheta_0 + \frac{1}{\sqrt{n}}\xi^{n,\vartheta_0},$$
où $\xi^{n,\vartheta_0}$ est une gaussienne standard ${\mathcal N}(0,1)$ sous $\PP_{\vartheta_0}$ \alert{mais pas sous une autre probabilitÈ $\PP_\vartheta$ si $\vartheta \neq \vartheta_0$!}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple (fin)}
\begin{itemize}
\item \alert{RÈsolution de}
$$\PP_{\vartheta_0}\big[\vartheta_0 + \frac{1}{\sqrt{n}}\xi^{n,\vartheta_0} > \frac{1}{2}(\vartheta_0+\vartheta_1)+\frac{\log c}{n(\vartheta_0-\vartheta_1)}\big]=\alpha.$$
\item \alert{Equivalent à}
$\PP_{\vartheta_0}\big[\xi^{n\vartheta_0} > \frac{\sqrt{n}}{2}(\vartheta_1-\vartheta_0)+\frac{1}{\sqrt{n}}\frac{\log c}{\vartheta_0-\vartheta_1}\big]=\alpha,$
soit
$$\frac{\sqrt{n}}{2}(\vartheta_1-\vartheta_0)+\frac{1}{\sqrt{n}}\frac{\log c}{\vartheta_0-\vartheta_1}=\Phi^{-1}(1-\alpha),$$
où $\Phi(x) = \int_{-\infty}^x e^{-u^2/2}\tfrac{du}{\sqrt{2\pi}}$.
\item \alert{Conclusion}
$$\boxed{c_\alpha = \exp\big(n\frac{(\vartheta_1-\vartheta_0)^2}{2}+\sqrt{n}(\vartheta_0-\vartheta_1)\Phi^{-1}(1-\alpha)\big)}$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Bilan provisoire}
\begin{itemize}
\item Si l'on accepte \alert{le principe de Neyman}, on sait rÈsoudre le problème à deux points.
\item Que faire si l'hypothèse nulle $H_0$ ou l'alternative $H_1$ sont \alert{composites} ?
\begin{itemize}
\item On peut proposer des extensions si l'on dispose de structures particulières sur la vraisemblance du modèle (Poly. Ch. 7.3, hors programme).
\item On sait dire \alert{ beaucoup de choses} dans le cas gaussien.
\end{itemize}
\item \alert{ Critique mÈthodologique de l'approche de Neyman} $\leadsto$ notion de $p$-valeur.
\item On ne sait \alert{toujours pas} rÈpondre à la question de l'exemple introductif... \alert{cadre asymptotique} Cours 8.
\end{itemize}
\end{frame}


\end{document}



\section{Tests gaussiens}

\subsection{Tests sur la moyenne}

\begin{frame}
\frametitle{Tests gaussiens incontournables}
\begin{itemize}
\item On observe
$$\bY=(Y_1,\ldots, Y_n) \sim {\mathcal N}(\mu, \sigma^2 \text{Id}_n).$$
\item \underline{\alert{Test sur la moyenne, variance connue}}
$$H_0:\mu \leq \mu_0\;\;\;\text{contre}\;\;\;H_1:\mu > \mu_0$$
\item \alert{ Principe} on estime $\mu$ et on rejette $H_0$ si l'estimateur est \og plus grand \fg{} que $\mu_0$.
$${\mathcal R}(c_\alpha) = \big\{\overline{Y}_n - \mu_0 \geq c_\alpha\big\},\;\;\;c_\alpha\;\;\text{à dÈterminer}.$$
\item On choisit $c_\alpha$ de sorte que
$$\sup_{\mu \leq \mu_0}\PP_\mu\big[{\mathcal R}(c_\alpha)\big] \leq \alpha.$$
\item Il y a \alert{plusieurs choix possibles}. On fait le choix rendant  ${\mathcal R}(c_\alpha)$ \alert{  maximale}.
%\item On a \alert{seulement} optimalitÈ parmi la classe de test de zone de rejet de la forme ${\mathcal R}(c), c >0$ sans outil supplÈmentaire.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul de $c_\alpha$}
\begin{itemize}
\item \alert{Majoration de l'erreur de première espèce}. Si $\mu \leq \mu_0$, on a
\begin{align*}
\PP_{\mu}\big[\overline{Y}_n - \mu_0 \geq c_\alpha\big] & = \PP_\mu\big[(\mu+\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu})-\mu_0 \geq c_\alpha\big] \\
& =   \PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha + (\mu_0-\alert{\mu})\big] \\
& \leq \PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha\big].
\end{align*}
où $\xi^{n,\mu}$ est, en loi sous $\PP_\mu$, une gaussienne standard.
\item \alert{ Petit miracle} : la loi de $\xi^{n,\mu}$ sous $\PP_\mu$ ne \alert{dÈpend pas de} $\mu$
% (on parle de statistique libre).
Donc
\begin{align*}
\PP_{\mu}\big[\tfrac{\sigma}{\sqrt{n}}\xi^{n,\mu} \geq c_\alpha\big] & =1-\Phi\big(\tfrac{\sqrt{n}}{\sigma}c_\alpha\big) \\
& \stackrel{\text{on veut}}{\leq} \alpha.
\end{align*}
\item Le choix $c_{\alpha,n} = \tfrac{\sigma}{\sqrt{n}}\Phi^{-1}(1-\alpha)$ conduit à la zone de rejet ${\mathcal R}(c_\alpha)$ \alert{maximale}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Contrôle de l'erreur de seconde espèce}
\begin{itemize}
\item On a construit un test de niveau $\alpha$ parmi une classe \alert{donnÈe a priori} de tests basÈs sur un estimateur \og raisonnable\fg{}, de sorte que l'on ait une zone de rejet maximale. DÈsormais, $c_{\alpha, n}$ est \alert{fixÈ}.
\item On \alert{Èvalue à la main} l'erreur de seconde espèce ou la \alert{fonction de puissance}
\begin{align*}
\mu \in (\mu_0,+\infty) & \leadsto \PP_\mu\big[\overline{Y}_n - \mu_0 < c_{\alpha,n}\big] \\
&= 1 - {\text{puissance du test au point }}\mu
\end{align*}
\item \alert{Montrer que} pour tout $\mu > \mu_0$, on a $\PP_\mu\big[\overline{Y}_n - \mu_0 < c_{\alpha,n}\big] \rightarrow 0$ lorsque $n\rightarrow \infty$.
\item Pour l'optimalitÈ dans un sens plus fort, il faut \alert{ d'autres outils}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Autres tests classiques gaussiens}
\begin{itemize}
%\item \underline{\alert{Test sur la moyenne et la  variance}}.
\item \alert{IngrÈdient principal} :
$$s_n^2:=\frac{1}{n-1}\sum_{i = 1}^n (Y_i-\overline{Y}_n)^2 = \frac{n}{n-1}\big(\widehat \sigma^2_n\big)^{{\tt mv}}$$
alors
$$(n-1)\frac{s_n^2}{\sigma^2} \sim \chi^2(n-1)$$
et
$$\frac{\sqrt{n}(\overline{Y}_n-\mu)}{s_n} \sim \text{Student}(n-1)$$
et ces variables sont \alert{pivotales} : leur loi ne dÈpend pas de $\mu,\sigma^2$ sous $\PP_{\mu,\sigma^2}$.
\item Les lois du \alert{$\chi^2$} et de \alert{Student} (à $k$ degrÈs de libertÈ) sont classiques et s'Ètudient indÈpendamment.
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Tests sur la moyenne}
%\begin{itemize}
%\item On teste \alert{$H_0 : \mu \leq \mu_0$ contre $H_1: \mu > \mu_0$}. Un test de niveau $\alpha$ : donnÈ par
%$${\mathcal R}_\alpha = \big\{T(\bY)>q_{1-\alpha, n-1}^{\mathfrak T}\big\}$$
%où
%$$T(Y)  = \frac{\sqrt{n}(\overline{Y}_n-\mu_0)}{s_n}$$
%%et $q_{1-\alpha, n-1}^{\mathfrak{T}}$ = quantile d'ordre $1-\alpha$ de la loi de Student à $n-1$ degrÈs de libertÈ :
%où
%$$\PP\big[\text{Student}_{n-1} > q_{1-\alpha, n-1}^{\mathfrak{T}}\big]=\alpha$$
%\item On teste \alert{ $H_0 : \mu = \mu_0$ contre $H_1:\mu \neq \mu_0$}. Un test de niveau $\alpha$: donnÈ par
%${\mathcal R}_\alpha = \big\{\big|T(\bY)\big| > q_{1-\alert{\alpha/2}, n-1}^{\mathfrak{T}}\big\}$.
%\end{itemize}
%\end{frame}
%\subsection{Tests sur la variance}
%\begin{frame}
%\frametitle{Test sur la variance}
%\begin{itemize}
%\item On teste \alert{ $H_0:\sigma^2 \leq \sigma_0^2$ contre $H_1:\sigma^2 > \sigma_0^2$}. Un test de niveau $\alpha$ : donnÈ par
%$${\mathcal R}_\alpha = \big\{V(\bY)>q_{1-\alpha,n-1}^{\chi^2}\big\},$$
%où
%$$V(\bY) = \frac{1}{\sigma_0^2}\sum_{i = 1}^n (Y_i - \overline{Y})^2$$
%et
%$$\PP\big[\text{Chi-deux}_{n-1} > q_{1-\alpha,n-1}^{\chi^2}\big] = \alpha.$$
%\item \alert{Mêmes remarques mÈthodologiques} sur l'optimalitÈ de ces tests que prÈcÈdemment.
%\end{itemize}
%\end{frame}


\section{ComplÈments : $p$-valeur et liens entre tests et rÈgions de confiance}

\begin{frame}
\frametitle{$p$-valeurs}
\begin{itemize}
\item \alert{Exemple} : on observe
$$X_1,\ldots, X_n\sim_{\text{i.i.d.}}{\mathcal N}(\mu,\sigma^2),\;\;\;\sigma^2\;\;\text{connu}.$$
\item \alert{Objectif}: tester $H_0:\mu=0$ contre $H_1:\mu\neq 0$.
\item Au niveau $\alpha=5\%$, on rejette si
$$\big| \overline{X}_n \big| > \frac{\phi^{-1}(1-\alpha/2)}{\sqrt{n}}$$
\item \alert{Application numÈrique} : $n=100$, $\overline{X}_{100}=0.307$. On a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.196$. \alert{on rejette l'hypothèse...}.
%\item
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{$p$-valeur (cont.)}
\begin{itemize}
\item \alert{Et pour un autre choix de $\alpha$ ?}. Pour $\alpha=0.01$, on a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.256$. On rejette toujours... Pour $\alpha=0.001$, on a $\frac{\phi^{-1}(1-0.05/2)}{\sqrt{100}} \approx 0.329$. \alert{On accepte $H_0$ !}
\item Que penser de cette petite expÈrience ?
\begin{itemize}
\item En pratique, on a une observation une bonne fois pour toute (ici $0.307$) et on \og choisit \fg{} $\alpha$... \alert{comment ?}
\item On ne veut pas $\alpha$ trop grand (trop de risque), mais en prenant $\alpha$ de plus en plus petit... on va \alert{ fatalement} finir par accepter $H_0$ !
\end{itemize}
\item DÈfaut de mÈthodologie inhÈrent au principe de Neyman (contrôle de l'erreur de première espèce).
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{p-valeur}
\begin{itemize}
\item QuantitÈ \alert{significative} : non par le niveau $\alpha$, mais le \alert{seuil de basculement de dÈcision} : c'est la $p$-valeur ($p$-value) du test.
\end{itemize}
\begin{df}
Soit ${\mathcal R}_\alpha$ une famille de zones de rejet d'un test de niveau $\alpha$ pour une hypothèse $H_0$ contre une alternative $H_1$. Soit $Z$ l'observation associÈe à l'expÈrience. On a $Z \in \mathfrak{Z}$ et ${\mathcal R}_0 = \mathfrak{Z}$.
On appelle \alert{$p$-valeur du test} la quantitÈ
$$p-\text{valeur}(Z) = \inf\{\alpha, Z \in {\mathcal R}_\alpha\}.$$
\end{df}
\end{frame}

\begin{frame}
\frametitle{InterprÈtation de la $p$-valeur}
\begin{itemize}
\item Une grande valeur de la $p$-valeur s'interprète en faveur de \alert{ne pas vouloir rejeter l'hypothèse}.
\item \og Ne pas vouloir rejeter l'hypothèse \fg{} peut signifier deux choses :
\begin{itemize}
\item L'hypothèse est vraie
\item L'hypothèse est fausse \alert{ mais} le test n'est pas \alert{puissant} (erreur de seconde espèce \alert{grande}).
\end{itemize}
\item \alert{Souvent :} la $p$-valeur est la probabilitÈ (sous $H_0$) que la statistique de test d'une expÈrience \og copie \fg{} soit $\geq$ à la statistique de test observÈe.
\item \alert{Exemple du test du $\chi^2$ et de l'expÈrience de Mendel} (à suivre) %
\end{itemize}
\end{frame}


%\begin{frame}
%\frametitle{Poly. Errata Ch. 6}
%{\small
%\begin{itemize}
%\item p. 142. Dans l'Ènonce de la proposition 6.3, lire
%$$\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}  = + \E_{\vartheta}\big[\partial_\vartheta^2 \ell(\vartheta, X)\big] = - \mathbb{I}(\vartheta)$$
%\item p 143. Dans la preuve de la proposition 6.3, on doit Ècrire pour la première formule
%$$\partial_a \mathcal{F}(a,\vartheta) = + \int \partial_\vartheta \ell(\vartheta, x)f(\vartheta, x) \mu(dx)$$
%puis remplacer \og{} minimum \fg{} par \og{} maximum \fg{} dans la phrase en-dessous...
%\item Après le lemme 6.3.3., on obtient alors $\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   = -\mathbb{I}(\vartheta)$
%\item Finalement, p 144, dÈbut du paragraphe 6.3.4, on se retrouve avec
%$$\mathbb{I}(\vartheta) = - \partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta} \geq 0$$
%et tout (re)devient cohÈrent...
%\end{itemize}
%}
%\end{frame}
%

\end{document}















\subsection{Test d'appartenance à un sous-espace linÈaire}

\begin{frame}
\frametitle{Test d'appartenance à un sous-espace linÈaire}
\begin{itemize}
\item \alert{Modèle linÈaire gaussien} $\vartheta \in \R^d$ et
$$\bY = \design \vartheta + \boldsymbol{\xi},\;\;\boldsymbol{\xi} \sim {\mathcal N}(0,\sigma^2 \text{Id}_n),$$
avec $\det\design^T\design >0$.
\item $a \in \R$, $j \in \{1,\ldots, d\}$ donnÈ. \alert{Test de $H_0: \vartheta_j=a$ contre $H_1:\vartheta_j \neq a$}, $\vartheta = (\vartheta_1,\ldots, \vartheta_d)^T$.
\item \alert{On a} (exercice !)
$$\frac{\big(\estMC\big)_j-\vartheta_j}{s_n \sqrt{(\design^T\design)_{jj}^{-1}}} \stackrel{d}{=} \text{Student}(n-d).$$
\item \alert{Test  de niveau $\alpha$} dÈfini par la zone de rejet
$${\mathcal R}_{\alpha} = \Big\{\Big|\frac{\big(\estMC\big)_j-a}{s_n \sqrt{(\design^T\design)_{jj}^{-1}}}\Big| > q_{1-\alpha/2, n-d}^{\mathfrak{T}}\Big\}.$$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{SÈlection de variables}
\begin{itemize}
\item On Ècrit le modèle linÈaire (gaussien) avec $d \geq 2$:
$$Y_i = \vartheta^T\bx_i+\xi_i = \sum_{i = 1}^d \vartheta_i x_i + \xi_i,\;\;i=1,\ldots, n$$
\item $1 \leq k <d$ fixÈ. \alert{Test d'influence des $k$ premières variables seulement}. On teste
$$\boxed{H_0:\vartheta_{k+\ell}=0,\;\;\ell = 1,\ldots, d-k}$$
contre
$$\boxed{H_1: \text{il existe} \;1 \leq \ell \leq d-k,\;\text{t.q.}\;\vartheta_{k+\ell} \neq 0}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Formulation du problème : F-tests.}
\begin{itemize}
\item Poly. pp. 186--188.
\item $\mathbb{G}$ matrice d'une application linÈaire de $\R^d \rightarrow \R^m$ de la forme
$$
\mathbb{G} =
\left(
\begin{array}{llllll}
0 & \ldots & 0 & \;\;1 & \ldots & 0 \\
\vdots & \ddots &\vdots &\;\; \vdots & \ddots & \vdots \\
0 & \ldots & 0 &\;\; 0 & \ldots & 1
\end{array}
\right),
$$
bloc de $0$ : $m$ lignes et $d-m$ colonnes.
% alors que le second bloc est la matrice identitÈ à $m$ lignes et $m$ colonnes.
et $\boldsymbol{b} = (a_1,\ldots, a_m)^T$ donnÈ.
\item \alert{On teste}
$$\boxed{H_0:\mathbb{G}\vartheta = \boldsymbol{b}}$$
\alert{contre}
$$\boxed{H_1:\mathbb{G}\vartheta \neq \boldsymbol{b}}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{F-tests (fin)}
\begin{itemize}
\item Sous l'hypothèse (sous $\PP_\vartheta$ tel que $\mathbb{G}\vartheta=\boldsymbol{b}$) on a (Cochran)
$$\mathbb{G}\estMC \sim {\mathcal N}\big(\boldsymbol{b},\sigma^2 \mathbb{G}( \design^T \design )^{-1}\mathbb{G}^T\big)$$
\item En posant ${\bf U} = \sigma^2 \mathbb{G}(\design^T\design)^{-1}\mathbb{G}^T$, on \alert{montre}
$$(\mathbb{G}\estMC-{\bf b})^T{\bf U}^{-1}(\mathbb{G}\estMC-{\bf b}) \sim \chi^2(m)\;\;\alert{\text{sous}\; H_0}.$$
\item Si $\sigma^2$ inconnu, on l'estime par $\widehat \sigma_n^2 = \frac{\|{\bf Y}-\design \,\estMC\|^2}{n-d}$. Alors la loi de
$$\frac{(\mathbb{G}\estMC-{\bf b})^T\widehat {\bf U}^{-1}(\mathbb{G}\estMC-{\bf b})}{m}$$
ne \alert{dÈpend pas de $\vartheta$ ni de $\sigma^2$} sous $H_0$ et suit la loi de Fisher-Snedecor à $(m,n-d)$ degrÈs de libertÈ.
\end{itemize}
\end{frame}



\section{Notion de $p$-valeur}

\begin{frame}
\frametitle{Poly. Errata Ch. 6}
{\small
\begin{itemize}
\item p. 142. Dans l'Ènonce de la proposition 6.3, lire
$$\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}  = + \E_{\vartheta}\big[\partial_\vartheta^2 \ell(\vartheta, X)\big] = - \mathbb{I}(\vartheta)$$
\item p 143. Dans la preuve de la proposition 6.3, on doit Ècrire pour la première formule
$$\partial_a \mathcal{F}(a,\vartheta) = + \int \partial_\vartheta \ell(\vartheta, x)f(\vartheta, x) \mu(dx)$$
puis remplacer \og{} minimum \fg{} par \og{} maximum \fg{} dans la phrase en-dessous...
\item Après le lemme 6.3.3., on obtient alors $\partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   = -\mathbb{I}(\vartheta)$
\item Finalement, p 144, dÈbut du paragraphe 6.3.4, on se retrouve avec
$$\mathbb{I}(\vartheta) = - \partial_a^2{\mathcal F}(a,\vartheta)_{\big|a=\vartheta}   0$
et tout (re)devient cohÈrent...
\end{itemize}
}
\end{frame}

\end{document}









