
\documentclass{beamer}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amstext,amsthm}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{beamerthemesplit}

%\usepackage[latin1]{inputenc}
\usepackage[french]{babel}
\usepackage[applemac]{inputenc}

\usetheme{Antibes}
\mode<presentation>
\useoutertheme{tree}
\usecolortheme{beaver}
\useinnertheme{rectangles}

\setbeamerfont{block title}{size={}}
%\usecolortheme[rgb={0.55,0.1,0.05}]{structure}
%\usecolortheme[rgb={0.75,0.1,0.05}]{structure}
\usepackage{color}

\newenvironment{disarray}{\everymath{\displaystyle\everymath{}}\array} {\endarray}
\newtheorem{theo}{Th\'eor\`eme}
\newtheorem{prop}[theo]{Proposition}
\newtheorem{conj}[theo]{Conjecture}
\newtheorem{cor}{Corollary}[theo]
\newtheorem{lem}{Lemme}
\newtheorem{nota}{Notation}
\newtheorem{rk}{Remark}
\newtheorem{exa}{Example}
\newtheorem{df}{Definition}
\newtheorem{terminologie}{Terminologie}
\def\rme{\mathrm{e}}
\def\rmi{\mathrm{i}}
\def\rset{\mathbb{R}}
\def\dlim{\stackrel{d}{\rightarrow}}
\def\plim{\stackrel{\PP}{\longrightarrow}}
\def\iid{i.i.d.}
\newenvironment{dem}{\textbf{Proof}}{\flushright$\blacksquare$\\}
%\def\blankframe{
%\mode<presentation>{
%  { \setbeamertemplate{background canvas}[default]
%    \setbeamercolor{background canvas}{bg=black}
%    \begin{frame}[plain]{}
%    \end{frame}
%  }
%}
%\mode<presentation>{
%\setbeamertemplate{background canvas}[default]
%\setbeamercolor{background canvas}{bg=white}}
%\mode*
%}
\def\eqsp{\,}
\DeclareMathOperator{\E}{{\mathbb E}}
\def\PE{\E}
\DeclareMathOperator{\F}{{\mathbb F}}
\DeclareMathOperator{\G}{{\mathbb G}}
\DeclareMathOperator{\D}{{\mathbb D}}
\DeclareMathOperator{\R}{{\mathbb R}}
\DeclareMathOperator{\C}{{\mathbb C}}
\DeclareMathOperator{\Z}{{\mathbb Z}}
\DeclareMathOperator{\N}{{\mathbb N}}
\DeclareMathOperator{\K}{{\mathbb K}}
\DeclareMathOperator{\T}{{\mathbb T}}
\DeclareMathOperator{\PP}{{\mathbb P}}
\DeclareMathOperator{\QQ}{{\mathbb Q}}
\DeclareMathOperator{\Q}{{\mathbb Q}}
\DeclareMathOperator{\IF}{{\mathbb I}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Pour le modèle lin\'eaire

\DeclareMathOperator{\bX}{\boldsymbol{X}}
\DeclareMathOperator{\bY}{\boldsymbol{Y}}
\DeclareMathOperator{\bx}{\boldsymbol{x}}
\DeclareMathOperator{\vp}{\boldsymbol{p}}
\DeclareMathOperator{\vq}{\boldsymbol{q}}
\DeclareMathOperator{\estMC}{\widehat \theta_n^{\,\,{\tt mc}}}
\DeclareMathOperator{\estMCNL}{\widehat \theta_n^{\,\,{\tt mcnl}}}
\DeclareMathOperator{\estMV}{\widehat \theta_n^{\,\,{\tt mv}}}
\DeclareMathOperator{\design}{\mathbb{M}}
\DeclareMathOperator{\est}{\widehat \theta_{\mathnormal{n}}}
\DeclareMathOperator{\var}{\mathrm{Var}}
\DeclareMathOperator{\estMVc}{\widehat \theta_{n,0}^{\,{\tt mv}}}
\DeclareMathOperator{\Xbar}{\overline{\mathnormal{X}}_\mathnormal{n}}

\newcommand{\indi}[1]{\mathbbm{1}_{\{#1\}}}
\newcommand{\coint}[1]{\left[#1\right)}
\newcommand{\ocint}[1]{\left(#1\right]}
\newcommand{\ooint}[1]{\left(#1\right)}
\newcommand{\ccint}[1]{\left[#1\right]}
\newcommand{\Id}{\mathrm{I}}


\title{MAP 433 : Introduction aux m\'ethodes statistiques}
%\author{M. Hoffmann}
%\institute{Universit\'e Paris-Est and ETG}
\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%\AtBeginSection[]
%{
%  \begin{frame}
%    \tableofcontents[currentsection,hideothersubsections]
%  \end{frame}
%}

%\AtBeginSubsection[]
%{
%  \begin{frame}[plain]{Plan}
%    \tableofcontents[currentsection,currentsubsection]
%  \end{frame}
%}


\section{Pr\'esentation}

\begin{frame}
\frametitle{Organisation : \'equipe enseignante}

%{\large {\color{gray} Equipe enseignante}} \vspace{0.5cm}

{\color{red} Cours \vspace{2mm}}

%Alexandre Tsybakov, ENSAE (cours 1 \`a 5)\\
%{\small \hspace{3mm} alexandre.tsybakov@ensae.fr}\\
Eric  MOULINES, Ecole Polytechnique \\
%\begin{center}
{\small \hspace{3mm} eric.moulines@polytechnique.edu}
%\end{center}
\vspace{4mm}

{\color{red} PC}
\begin{itemize}
\item Olivier Capp\'e, DR CNRS, T\'el\'ecom-ParisTech
\item Gersende Fort, DR CNRS, T\'el\'ecom-ParisTech,
\item Lucas G\'erin, \'Ecole Polytechnique,
\item Christophe Giraud, Universit\'e Paris-Sud et \'Ecole Polytechnique,
\item Marc Lavielle, DR INRIA, INRIA Saclay,
\item Matthieu Lerasle, CR CNRS, Universit\'e de Nice,
\item Mathieu Rosenbaum, Universit\'e Pierre-et-Marie Curie,
\item Francois Roueff, Professeur, T\'el\'ecom ParisTech.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Organisation : mat\'eriel}
%{\color{gray} Materiel}
\begin{itemize}
\item {\color{red}Transparents} du cours t\'el\'echargeables à l'adresse
\begin{center}
{\small https://moodle.polytechnique.fr/course/view.php?id=1717}
\end{center}
\item {\color{red} Polycopi\'e \small document autonome contenant l'int\'egralit\'e du cours et plus, t\'el\'echargeable \`a la m\^eme adresse}.
\item Les documents et {\color{red} exercices} de PC. [les exercices obligatoires et pour aller plus loin]
\item Des liens pour des exp\'eriences num\'eriques.
\end{itemize}
\end{frame}


%\begin{frame}
%\frametitle{Pr\'esentation (succincte) du cours}
%\begin{itemize}
%\item Introduction aux statistiques et rappels de probabilit\'es {\color{red} (1 cours)}.
%\item Introduction th\'eorie de la d\'ecision  {\color{red} (1 cours)}.
%\item R\'egression lin\'eaire et non-lin\'eaire (1 cours).
%\item M\'ethodes d'estimation classique  {\color{red} (2 cours)}.
%\item Information statistique, th\'eorie asymptotique pour l'estimation  {\color{red} (1 cours)}.
%\item D\'ecision statistique et tests  {\color{red} (2 cours)}.
%\end{itemize}
%\end{frame}

%\begin{frame}
%%\frametitle{La Statistique dans le monde du travail}
%\centerline{Classement Forbes "10 Best Jobs for 2010"}
%\begin{enumerate}
%\item {\color{red} Actuary}
%\item Software engineer
%\item Computer systems analyst
%\item Biologist
%\item Historian
%\item {\color{red} Mathematician}
%\item Paralegal assistant
%\item {\color{red} Statistician}
%\end{enumerate}
%\end{frame}

%\begin{frame}
%\frametitle{Plan}
%\begin{itemize}
%\item {\color{red} Probl\'ematique statistique} : de quoi s'agit-il ?
%\item {\color{red} Echantillonnage}.
%\item Estimation d'une distribution inconnue à partir d'un $n$-\'echantillon, {\color{red} m\'ethodes empiriques}.
%\end{itemize}
%\end{frame}


\begin{frame}
\frametitle{Biostatistique}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \begin{tabular}{cc}
  \includegraphics[width=0.4\textwidth]{prostate} &   \includegraphics[width=0.4\textwidth]{biopuce1}
  \end{tabular}
  \caption{Identifier les facteurs de risque pour le d\'eveloppement d'un cancer; classifier des tissus en fonctions de donn\'ees d'expression de g\`enes}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{\'Econom\'etrie}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
\begin{tabular}{c}
\includegraphics[width=0.6\textwidth]{income} \\
 \includegraphics[width=0.5\textwidth]{or}
\end{tabular}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{T\'el\'e-d\'etection}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.8\textwidth]{landsat}
  \caption{Segmenter des images satellitaires}
\end{figure}
\end{frame}


\begin{frame}
\frametitle{Et plein d'autres choses}
\begin{itemize}
\item Opinion
\item Marketing
\item Sport
\item Assurance
\item Analyse du risque
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Data Scientist}
\begin{figure}
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[width=0.9\textwidth]{bestjob}
\end{figure}

\end{frame}

%\subsection{Les donn\'ees hier...}
%
%\begin{frame}
%\frametitle{Retour en arriè\`ere : les donn\'ees hier...}
%\begin{itemize}
%\item \og Statistik\fg\; (d\'eriv\'e du latin Statisticum). Allemagne, 1740 (Achenwall). {\color{red}Ensemble de mesures} et recueil de donn\'ees n\'ecessaires au fonctionnement et à l'organisation de l'\'etat : recensements et estimations de la population, des richesses, de l'impôt, des arm\'ees.
%\item Les progrès de la statistique : repr\'esenta\-tion graphique et {\color{red}organisation des donn\'ees en tableaux} (statistique descriptive, domin\'ee par l'\'ecole allemande au $18^e$ siècle). Activit\'e importante aussi en Grande Bretagne\footnote{William Playfair (1759--1823), 1786, "The Commercial and Political Atlas" contenant le premier diagramme en barres connu.} et dans une France centralis\'ee.\footnote{Vauban, 1686,  "M\'ethode g\'en\'erale et facile pour faire le d\'enombrement des peuples".}
%\end{itemize}
%\end{frame}




%\begin{frame}
%\frametitle{Statistique et probabilit\'es}
%\begin{itemize}
%\item  $17^e$ siècle : {\color{red}Invention des probabilit\'es}. Incorporation d'un raisonnement probabiliste -- et donc un modèle du hasard -- dans le traitement d'observations.
%\item Basculement de la statistique vers une {\color{red}discipline scientifique} à part entière. Pr\'efigure l'actuariat moderne\footnote{les frêres Hyugens, premier calcul de l'esp\'erance de vie humaine en 1669, Graunt (1620--1674), William Petty (1623--1687), Laplace.}.
%\item L'exemple historique incontournable : {\color{red}John Arbuthnott} (1667--1735) et le d\'eficit des naissances et morts selon le sexe.
%-- la première reflexion \og moderne \fg\; de statistique.
%% et qui pr\'efugurent l'actuariat.
%%Les noms souvent cit\'es sont  Nous exposons brièvement la figure de
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{John Arbuthnott et \og{}la divine providence\fg{}}
%\begin{itemize}
%\item 1712, Arbuthnott (m\'edecin de la Reine Anne) examine le nombre de baptêmes de filles et de garçons à Londres, entre 1629 et 1710.
%%, voir le Tableau \ref{}, extrait\footnote{{\tt http://www.taieb.net/auteurs/Arbuthnot/Arbuthnot.html}.} de Arbuthnott \ref{ARBU}, pp. 189--190.
%\item Sur 82 ann\'ees retenues, le nombre de naissances masculines est toujours sup\'erieur au nombre de naissance f\'eminines.
%\item Arbuthnott calcule la probabilit\'e que les naissances masculines (avec \'equi-probabilit\'e filles/garçons) soient plus nombreuses que les naissances f\'eminines, 82 fois de suite ($=(1/2)^{82}$), {\it \og which will be found easily by the Table of Logarithms to be 1/4 8360 0000 0000 0000 0000 0000 \fg}.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{An Argument for Divine Providence}
%% {\it An Argument for Divine Providence, taken frome the constant Regularity observed in the Births of both Sexes}
%\begin{itemize}
%\item {\it An Argument for Divine Providence, taken frome the constant Regularity observed in the Births of both Sexes}
%%l'exc\'edent de naissnces masculines de la manière suivante
%\item {\small  {\it \og [...] This Event is wisely prevented by the Oeconomy of Nature; and to the judge of the wisdom of the Contrivance, we must observe that the external Accidents to which Males are subject (who must seek their food with danger) do make a great havock of them, and that this loss exceeds far that of the other Sex, occasioned by Disease incident to it, as Experience convinces us. To repair that Loss, provident Nature, by the Disposal of its wife Creator, brings more Males than Females ; and this in almost a constant proportion\fg\;.}}
%\end{itemize}
%\end{frame}
\section{Paradigme statistique}
\begin{frame}
\frametitle{Probl\'ematique statistique}
\begin{itemize}
\item {\color{red} Point de d\'epart} : des observations (des nombres r\'eels)
$${\tt x_1},\ldots, {\tt x_n}.$$
\item {\color{red} Mod\'elisation statistique} :
\begin{itemize}
\item
les observations sont des r\'ealisations
$$X_1(\omega),\ldots, X_n(\omega)\;\;\text{de v.a.r.}\;\;X_1,\ldots, X_n.$$
%de variables al\'eatoires
%$X_1,\ldots, X_n$.
\item
La {\color{red}loi} $\PP^{(X_1,\ldots, X_n)}$ de $(X_1,\ldots, X_n)$ {\color{red} est inconnue}, mais appartient à une famille donn\'ee
$$\boxed{\big\{\PP_\theta^n,\theta \in \Theta\big\}.}$$
\end{itemize}
\item {\color{red} Probl\'ematique}: à partir de \og l'observation \fg{} $X_1,\ldots, X_n$, peut-on {\color{red}retrouver} $\PP_\theta^n$ ? et donc $\theta$ ?
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Probl\'ematique statistique (suite)}
\begin{itemize}
\item $\theta$ est le {\color{red} paramètre} et $\Theta$ {\color{red} l'ensemble} des paramètres.
%\item Deux grandes classes de problèmes
%\begin{itemize}
\item {\color{red}Estimation}
: à partir de $X_1,\ldots, X_n$, construire $\varphi_n(X_1,\ldots,X_n)$ qui \og approche au mieux \fg{} $\theta$.
\item {\color{red} Test} : à partir de  $X_1,\ldots, X_n$, \'etablir une {\color{red}d\'ecision} $\varphi_n(X_1,\ldots,X_n) \in \{\text{ensemble de d\'ecisions}\}$ concernant $\theta$ pouvant être vraie ou fausse.
%\end{itemize}
%\item Si $\Theta$ est un sous-ensemble de $\R^d$, statistique {\color{red}param\'etrique}. Si $\Theta$ sous-ensemble d'un espace fonctionnel, statistique {\color{red}non-param\'etrique}.
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Exemple le plus simple}
\begin{itemize}
\item On lance une pièce de monnaie $n$ fois et on observe ($P=0$, $F=1$)
$$0,0,0,1,1,0,1,0,0,1,1,0,1,0,0,1,1,0.$$
\item Modèle statistique : on observe $n$ variables al\'eatoires $X_i$ ind\'ependantes, de Bernoulli de paramètre {\color{red}inconnu} $\theta \in \Theta = [0,1]$.
\begin{itemize}
\item {\color{red} Estimation}. Estimateur $\bar X_{n} = n^{-1} \sum_{i = 1}^{n} X_i$. Quelle pr\'ecision ? Peut-on construire un meilleur estimateur ?
\item {\color{red} Test}. D\'ecision à prendre : \og la pièce est-elle \'equilibr\'ee \fg{} ?. Par exemple : on compare $\bar X_{n}$ à 0.5.
Si $|\bar X_{n}-0.5|$ \og petit\fg{}, on accepte l'hypothèse \og la pièce est \'equilibr\'ee\fg{}. Sinon, on rejette. Quel seuil choisir, et avec quelles cons\'equences ?
\end{itemize}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Echantillonnage}
\begin{itemize}
\item
\underline{L'exp\'erience statistique la plus \'el\'ementaire} : on observe la r\'ealisation de $X_1,\ldots, X_n$, v.a.r. o\`u   $X_i$ sont {\color{red} ind\'ependantes}, {\color{red} identiquement distribu\'ees}, de m\^eme loi  $\PP^X$.
\item Que dire de la loi $\PP^X$ commune des $X_i$ ?
\item Structure stochastique {\color{red} très simple} (variable al\'eatoires ind\'ependantes, de \^eême loi) mais espace de param\`etres immense.
\end{itemize}
\end{frame}

\section{Rappels de probabilit\'es}
%\subsection{Loi d'une variable al\'eatoire}
\begin{frame}
\frametitle{Rappel : loi d'une variable al\'eatoire r\'eelle}
\begin{definition}
$$X:\big(\Omega, {\mathcal A}, \PP\big) \longrightarrow \big(\R, {\mathcal B}\big)$$
{\color{red} Loi de $X$}: mesure de probabilit\'e sur $(\R, {\mathcal B})$, not\'ee $\PP^X$, d\'efinie  par
$$\PP^X\big[A\big] = \PP\big[X^{-1}(A)\big],\;\;A\in {\mathcal B}.$$
\end{definition}
\underline{Formule d'int\'egration}
$$\boxed{\E\big[\varphi(X)\big]= \int_{\Omega}\varphi\big(X(\omega)\big)\PP(d\omega) = \int_{\R} \varphi(x)\PP^X(dx)}
$$
$\varphi$ fonction test.
\end{frame}
%\begin{frame}
%\frametitle{Loi d'une variable al\'eatoire (suite)}
%{\color{red} Exemple 1 :} $X$ suit la loi de Bernoulli de paramètre $1/3$.
%\begin{itemize}
%\item \underline{La loi de $X$} est d\'ecrite par
%$$\PP\big[X=1\big]=\tfrac{1}{3} = 1- \PP\big[X=0\big].$$
%\item \underline{Ecriture de} $\PP^X(dx)$ :
%$$\boxed{\PP^X(dx) = \tfrac{1}{3}\delta_1(dx)+\tfrac{2}{3}\delta_0(dx).}$$
%\item {\small \underline{Formule de calcul}
%\begin{align*}
%\E\big[\varphi(X)\big] & = \int_{\R} \varphi(x) \PP^X(dx)
%%= \int_{\R} \varphi(x)  \big(\tfrac{1}{3}\delta_1(dx)+\tfrac{2}{3}\delta_0(dx)\big)
%\\
%&= \tfrac{1}{3}\int_{\R}\varphi(x)\delta_1(dx)+\tfrac{2}{3}\int_{\R}\varphi(x)\delta_0(dx) \\
%&= \tfrac{1}{3}\,\varphi(1)+\tfrac{2}{3}\,\varphi(0).
%\end{align*}}
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Loi d'une variable al\'eatoire (suite)}
%{\color{red} Exemple 2 :} $X \sim$ loi de Poisson de paramètre $2$.
%\begin{itemize}
%\item \underline{La loi de $X$} est d\'ecrite par
%$$\PP\big[X=k\big]=e^{-2}\frac{2^k}{k!},\;\;k=0,1,\ldots$$
%\item \underline{Ecriture de} $\PP^X(dx)$ :
%$$\boxed{\PP^X(dx) = e^{-2}\sum_{k \in \N}\tfrac{2^k}{k!}\delta_k(dx).}$$
%\item {\small \underline{Formule de calcul}
%$$\E\big[\varphi(X)\big] = \int_{\R}\varphi(x)\PP^X(dx) = e^{-2}\sum_{k\in \N}\varphi(k)\tfrac{2^k}{k!}.$$}
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Loi d'une variable al\'eatoire (suite)}
%{\color{red} Exemple 3 :} $X \sim {\mathcal N}(0,1)$ (loi normale standard).
%\begin{itemize}
%\item \underline{La loi de $X$} est d\'ecrite par
%$$\PP\big[X \in [a,b]\big]= \int_{[a,b]}e^{-x^2/2}\tfrac{dx}{\sqrt{2\pi}}$$
%\item \underline{Ecriture de} $\PP^X(dx)$ :
%$$\boxed{\PP^X(dx) = \tfrac{1}{\sqrt{2\pi}}e^{-x^2/2} {\color{red}dx}}$$
%{\color{red}$dx$: mesure de Lebesgue}.
%\item {\small \underline{Formule de calcul}
%$$\E\big[\varphi(X)\big] = \int_{\R}\varphi(x)\PP^X(dx) = \int_{\R}\varphi(x)e^{-x^2/2}\tfrac{dx}{\sqrt{2\pi}}.$$}
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Loi d'une variable al\'eatoire (suite)}
%{\color{red} Exemple 4 :} $X = Z \wedge 1$, où la loi de $Z$ a une densit\'e $f$ par rapport à la mesure de Lebesgue sur $\R$\vspace{4mm}.
%
%\underline{{\color{red} Loi de $X$}}
%\begin{itemize}
%\item Sur l'\'ev\'enement $\big\{Z < 1\big\}$, on observe $X = Z$.
%\item Sur l'\'ev\'enement $\big\{Z \geq 1\big\}$, on observe $X = 1$.
%\end{itemize}
%
%\underline{{\color{red}Ecriture de} $\PP^X(dx)$} :
%$$
%\PP^X(dx) = f(x)1_{\big\{x < 1\big\}}dx + \PP\big[Z \geq 1\big]\delta_1(dx),
%$$
%\end{frame}
%\begin{frame}
%c'est-à-dire
%$$\boxed{\PP^X(dx) = f(x)1_{\big\{x < 1\big\}}dx + \Big( \int_{\coint{1,+\infty}}f(u)du\Big)\delta_1(dx)}$$
%\underline{Formule de calcul}
%\begin{align*}
%\E\big[\varphi(X)\big] &= \int_{\R}\varphi(x)\PP^X(dx)\\
%& = \int_{(-\infty,1)}\varphi(x)f(x)dx+\Big(\int_{\coint{1,+\infty}}f(u)du\Big)\varphi(1).
%\end{align*}
%\end{frame}
\subsection{Fonction de r\'epartition empirique}
\begin{frame}
\frametitle{Identification de la loi : fonction de r\'epartition}
\begin{itemize}
\item La loi d'une variable al\'eatoire $X$ est un \og objet compliqu\'e \fg{} :
\begin{itemize}
\item elle peut être discrète (somme de masses de Dirac)
\item elle peut être (absolument) continue (densit\'e par rapport à la mesure de Lebesgue)
\item elle peut-être une combinaison des deux, ou encore autre chose....
\end{itemize}
\item On peut {\color{red} caract\'eriser la loi} de $X$ par un objet plus simple à manipuler : une fonction croissante born\'ee : la {\color{red} fonction de r\'epartition}.
\item Plus facile à \'etudier dans un {\color{red} contexte de statistique}.
\item (Il y aura bien sûr des limites à cette approche...)
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Fonction de r\'epartition}
\begin{definition}
$X$ variable al\'eatoire r\'eelle. Fonction de r\'epartition de $X$ :
$$F(x) := \PP\big[X \leq x\big],\;x\in \R.$$
\end{definition}
\begin{itemize}
\item
$F$ est croissante, cont. à droite, $F(-\infty)=0$, $F(+\infty)=1$
\item $F$ {\color{red} caract\'erise} la loi $\PP^X$ :
$$\boxed{\PP^X\big[\ocint{a,b}\big] = \PP\big[a < X \leq b\big] = F(b)-F(a)}$$
\item D\'esormais, la {\color{red} loi (distribution)} de $X$ d\'esignera indiff\'eremment $F$ ou $\PP^X$.
\end{itemize}
\end{frame}
\section{Fonction de r\'epartition empirique}
\begin{frame}
\frametitle{Probl\'ematique statistique}
\begin{itemize}
\item On \og observe \fg{}
$$X_1,\ldots, X_n \sim_{i.i.d.} F,$$
$F$ fonction de r\'epartition {\color{red} quelconque, inconnue}.
\item Terminologie :
$(X_1,\ldots, X_n)$ est un {\color{red} $n$-\'echantillon} de la loi
$F$.
\item Comment {\color{red} retrouver} $F$ à partir des observations $X_1,\ldots, X_n$ ?
\item {\color{red} D\'emarche} : on construit une fonction (al\'eatoire)
$x \leadsto \widehat F_n(x) = F_n(x; X_1,\ldots, X_n)$
ne d\'ependant pas de $F$ (inconnu) telle que
$$\widehat F_n(x)-F(x)$$
petit lorsque $n$ grand... Comment ? Petit dans quel sens ?
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{Fonction de r\'epartition empirique}
\begin{definition}
Fonction de r\'epartition empirique associ\'ee au $n$-\'echantillon $(X_1,\ldots, X_n)$ :
$$\widehat F_n(x) = \frac{1}{n}\sum_{i = 1}^n 1_{\{X_i \leq x\}},\;x \in \R.$$
\end{definition}
\begin{itemize}
\item Terminologie : $\widehat F_n$ est un {\color{red} estimateur} : fonction des observations qui ne d\'epend {\color{red} pas} de la quantit\'e inconnue.
\item Pour tout $x_0\in \R$,
$$\boxed{\widehat F_n(x_0) \stackrel{\PP}{\longrightarrow} F(x_0),\;\;n\rightarrow \infty}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence en probabilit\'e}
\begin{itemize}
\item Mode de convergence \og naturel \fg{} en statistique
\item {\color{red} Rappel} : $X_n \stackrel{\PP}{\longrightarrow} X$ si
$$\forall \varepsilon >0,\;\PP\big[|X_n-X|\geq \varepsilon\big] \rightarrow 0,\;\;\; \rightarrow \infty.$$
\item {\color{red} Interpr\'etation} : pour tout niveau de risque $\alpha >0$ (petit) et tout niveau de pr\'ecision $\varepsilon >0$, il existe un rang
$N = N(\alpha, \varepsilon)$ tel que
$$n>N\;\;\text{implique}\;\;|X_n-X| \leq \varepsilon\;\;\text{avec proba.}\;\geq 1-\alpha.$$
\item En pratique, on souhaite simultan\'ement $N$, $\alpha$ et $\varepsilon$ petits. Quantit\'es {\color{red} antagonistes} (à suivre...).
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi faible des grands nombres}
\begin{theo}
Soit $(Y_i)_{i=1}^\infty$ une suite de v.a. i.i.d. int\'egrables (v\'erifiant $\PE[ |Y_1|]< \infty$). Alors,
\[
n^{-1} \sum_{i=1}^n Y_i \plim \PE[Y_1] \eqsp.
\]
\end{theo}
\end{frame}

\begin{frame}
\begin{figure}[htbp]
  \centering
  %\color{black}
  %\pagecolor{white}
  \includegraphics[height=6cm, width=8cm]{Fig1.pdf}
 \caption{{\small $\widehat F_n$ (noir), $F$ (rouge), $n=20$. $F \sim {\mathcal N}(0,1)$.}}
\end{figure}
\end{frame}



\begin{frame}
\begin{figure}[htbp]
  \centering
  %\color{black}
  %\pagecolor{white}
  \includegraphics[height=6cm, width=8cm]{Fig2.pdf}
   \caption{{\small $\widehat F_n$ (noir), $F$ (rouge), $n=100$. $F \sim {\mathcal N}(0,1)$.}}
\end{figure}
\end{frame}

\begin{frame}
\begin{figure}[htbp]
  \centering
  %\color{black}
  %\pagecolor{white}
  \includegraphics[height=6cm, width=8cm]{Fig3.pdf}
  \caption{{\small $\widehat F_n$ (noir), $F$ (rouge), $n=1000$. $F \sim {\mathcal N}(0,1)$.}}
\end{figure}
\end{frame}


\subsection{pr\'ecision d'estimation}

\begin{frame}
\frametitle{Vers la pr\'ecision d'estimation}
\begin{itemize}
\item On a $\forall x_0 \in \R,\,\widehat F_n(x_0)\stackrel{\PP}{\rightarrow} F(x_0)$. Avec {\color{red}quelle pr\'ecision ?}
%Si on se donne un nombre d'observation $n$ et un niveau de risque $\alpha$, quelle pr\'ecision $\varepsilon$ garantir ?
Problèmes de même types :
\begin{itemize}
\item $n$ {\color{red}information} et $\alpha$ {\color{red}risque} donn\'es $\rightarrow$ quelle  {\color{red} pr\'ecision} $\varepsilon$ ?
\item risque $\alpha$ et pr\'ecision $\varepsilon$ donn\'es $\rightarrow$ quel nombre minimal de donn\'ees $n$ n\'ecessaires ?
\item quel risque prend-on si l'on suppose une pr\'ecision $\varepsilon$ avec $n$ donn\'ees ?
\end{itemize}

\item Plusieurs approches :
\begin{itemize}
\item non-asymptotique naïve
\item non-asymptotique
\item {\color{red} approche asymptotique (via des th\'eorèmes limites)}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{In\'egalit\'e de Markov}
\begin{itemize}
\item Si $Y$ est une v.a. positive et $t \geq 0$, \alert{$Y \indi{ Y \geq t} \geq t \indi{Y \geq t}$}
\item \alert{In\'egalit\'e de Markov}
\[
\PP( Y \geq t) \leq t^{-1} \PE[ Y ]  \eqsp.
\]
\item  Si $\phi$ est une fonction positive monotone croissante, $\phi(t) > 0$ pour tout $t > 0$,
\[
\PP( Y \geq t)= \PP( \phi(Y) \geq \phi(t))  \leq \PE [\phi(Y)] / \phi(t) \, .
\]
\item Bien entendu, cette in\'egalit\'e est int\'eressante ssi $\PE[\phi(Y)] < \infty$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Approche na\"ive : contr\^ole de la variance}
Soit {\color{red}$\alpha >0$ donn\'e} (petit). On veut {\color{red} trouver $\varepsilon$}, le plus petit possible, de sorte que
$$\PP\left(|\widehat F_n(x_0)-F(x_0)|\geq \varepsilon\right) \leq \alpha.$$
On a {\color{red}(Tchebychev)}
\begin{align*}
\PP\big[|\widehat F_n(x_0)-F(x_0)|\geq \varepsilon\big] & \leq \frac{1}{\varepsilon^2}\text{Var}\big[\widehat F_n(x_0)\big]\\
& = \frac{{\color{red}F(x_0)}\big(1-{\color{red}F(x_0)}\big)}{n\varepsilon^2} \\
& \leq \frac{1}{4n\varepsilon^2} {\color{red} \leq \alpha}
%%\;\;\;\text{(ce qu'on veut)}.
\end{align*}
Conduit à
$$\boxed{\varepsilon = \frac{1}{2\sqrt{n\alpha}}}$$
\end{frame}


\begin{frame}
\frametitle{Intervalle de confiance}
\underline{Conclusion} : pour tout $\alpha >0$,
$$\PP\Big[|\widehat F_n(x_0)-F(x_0)|\geq \frac{1}{2\sqrt{n\alpha}}\Big] \leq \alpha.$$
\begin{terminologie}
L'intervalle
$$\boxed{{\mathcal I}_{n,\alpha} = \left[\widehat F_n(x_0)\pm  \frac{1}{2\sqrt{n\alpha}}\right]}
$$
est un intervalle de confiance pour $F(x_0)$ au niveau de confiance $1-\alpha$.
\end{terminologie}
\end{frame}
\begin{frame}
\frametitle{Pr\'ecision catastrophique !}
\begin{itemize}
\item Si $\alpha = 5\%$ et $n=100$, pr\'ecision $\varepsilon = 0.22$.
\item \underline{Autres exemples} : $\varepsilon_{\alpha=1/1000,n=100} = 1.58$, $\varepsilon_{\alpha = 1/100, n=100}=0.5$. {\color{red} aucune pr\'ecision d'estimation !\vspace{0.5cm}}
\item D'où vient le d\'efaut de cette pr\'ecision ?
\begin{itemize}
\item Mauvais choix de l'estimateur ? ($\rightarrow$ on verra que {\color{red} non}).
\item Mauvaise estimation de l'erreur ?
%($\rightarrow$ {\color{red}problème de nature probabiliste : l'in\'egalit\'e de Tchebychev est trop grossière}).
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{In\'egalit\'e de Markov}
\begin{itemize}
\item \alert{Moments d'ordres plus \'elev\'es:} Pour tout $q > 0$,  on a en posant $\phi(t) = t^q$
\[
\PP\big(|\widehat F_n(x_0)-F(x_0)|\geq \varepsilon\big)  \leq \frac{1}{\varepsilon^q} \PE\left( |F_n(x_0)-F(x_0)|^q \right)
\]
\item ... \`a part pour $q=2$ (ou plus g\'en\'eralement $q$ entier pair), $ \PE\left( |F_n(x_0)-F(x_0)|^q \right)$ ne se calcule par tr\`es ais\'ement...
\item Plus int\'eressant de consid\'erer une in\'egalit\'e \alert{exponentielle}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{In\'egalit\'e exponentielle}
\begin{itemize}
\item On pose $\phi(t) = \rme^{\lambda t}$. Dans ce cas, l'in\'egalit\'e de Markov implique
\[
\PP(Z > t) \leq \rme^{-\lambda t} \PE[ \rme^{\lambda Z}]
\]
o\`u $\lambda \mapsto \PE[\rme^{\lambda Z}]$ est la \alert{fonction g\'en\'eratrice} des moments ou \emph{transform\'ee de Laplace}.
\item En notant $\psi_Z(\lambda)= \log \PE[ \rme^{\lambda Z}]$ le logarithme la transfom\'ee de Laplace et en introduisant
\[
\psi_Z^*(t)= \sup_{\lambda \geq 0} \{ \lambda t - \psi_Z(\lambda) \}
\]
nous obtenons la borne de \alert{Cram\'er-Chernoff}
\[
\PP( Z > t) \leq \exp(- \psi_Z^*(t) ) \eqsp.
\]
\end{itemize}
\end{frame}


\begin{frame}\frametitle{In\'egalit\'e de Chernoff pour une somme de variables ind\'ependantes}
\begin{itemize}
 \item Posons $Z=X_{1}+\cdots+ X_{n}$ o\`u $X_{1},\ \ldots,\ X_{n}$ sont des variables \iid\
 \item On note  $\psi_{X}(\lambda)=\log \PE[\rme^{\lambda X}]$ et la transform\'ee de Cram\'er correspondante par     $\psi_{X}^{*}(t)$ .
\item \alert{Ind\'ependance}:
\begin{align*}
\psi_Z(\lambda)&= \log \PE\left[ \rme^{\lambda \sum_{i=1}^n X_i}  \right] \\
&= \log \prod_{i=1}^n \PE\left[ \rme^{\lambda X_i}\right] = n \psi_X(\lambda)
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{In\'egalit\'e de Chernoff pour les sommes de variables ind\'ependantes}
\begin{itemize}
\item \alert{Transform\'ee de Cram\'er}de la somme
\begin{align*}
\psi_Z^*(t) &= \sup_{\lambda > 0} \left( \lambda t - \psi_Z(\lambda) \right) \\
&= \sup_{\lambda > 0} \left( \lambda t - n \psi_X(\lambda) \right) = n \psi_X^*(t/n)
\end{align*}
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Lemme de Hoeffding}
\begin{lem}
Soit $Y$ une variable al\'eatoire telle que $\PE[Y]= 0$ et $Y \in \ccint{a,b}$ avec une probabilit\'e $1$. On pose $\psi_Y(\lambda)= \log \PE[\rme^{\lambda Y}]$. Alors
$$
\psi''_Y(\lambda) \leq (b-a)^2/4
$$
et
$$
\psi_Y(\lambda) \leq \lambda^2 (b-a)^2/8 \eqsp.
$$
\end{lem}
\end{frame}



\begin{frame}
\frametitle{In\'egalit\'e de Hoeffding}
\begin{prop}
$Y_1,\dots, Y_n$ i.i.d. de loi de Bernoulli de param\`etre $p$. Alors
$$\PP\left(\left|n^{-1} \sum_{i = 1}^nY_i-p\right|\geq t\right) \leq 2\exp(-2nt^2).
$$
\end{prop}
\alert{Application} : on pose $Y_i=1_{\{x_I \leq x_0\}}$ et $p = F(x_0)$. On en d\'eduit
$$\PP\big[\big|\widehat F_n(x_0)-F(x_0)\big|\geq \varepsilon \big] \leq 2\exp(-2n\varepsilon^2).$$
On r\'esout en $\varepsilon$: $2\exp(-2n\varepsilon^2) = \alpha,$ soit
$$\boxed{\varepsilon = \sqrt{\frac{1}{2n}\log \frac{2}{\alpha}}}.$$
\end{frame}
\begin{frame}
\frametitle{Comparaison Tchebychev vs. Hoeffding}
Nouvel intervalle de confiance
$$\boxed{{\mathcal I}_{n,\alpha}^{{\tt hoeffding}} = \left[\widehat F_n(x_0)\pm \sqrt{\frac{1}{2n}\log \frac{2}{\alpha}}\right]},$$
à comparer avec
$${\mathcal I}_{n,\alpha}^{{\tt tchebychev}} = \left[\widehat F_n(x_0)\pm  \frac{1}{2\sqrt{n\alpha}}\right].$$
\begin{itemize}
\item Même ordre de grandeur en $n$.
\item Gain {\color{red} significatif} dans la limite $\alpha \rightarrow 0$. La \og prise de risque\fg{} devient marginale par rapport au nombre d'observations.
\item {\color{red} Optimalit\'e d'une telle approche ?}
\end{itemize}
\end{frame}
\begin{frame}
\frametitle{L'approche asymptotique}
\begin{itemize}
\item Vers une notion d'optimalit\'e : on se place dans la limite $n \rightarrow \infty$ (l'information \og explose \fg{}). On \'evalue
$$\PP\big[\big|\widehat F_n(x_0)-F(x_0)\big| \geq \varepsilon \big], n \rightarrow \infty$$
pour une normalisation $\varepsilon = \varepsilon_n$ appropri\'ee.
\item Outil : {\color{red} Th\'eorème central-limite.}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence en loi}
La suite $(X_n)_{n \geq 0}$ converge en loi vers $X$ ($X_n \dlim X$) ssi l'une des conditions \alert{\'equivalente} est v\'erifi\'ee:
\begin{itemize}
\item Pour toute fonction $f$ continue born\'ee,
\[
\lim_{n \to \infty} \PE[f(X_n)]= \PE[f(X)] \eqsp.
\]
\item   $$\PP\left(X_n \leq x\right) \rightarrow \PP\left(X \leq x\right)$$
en tout point $x$ o\`u la fonction de r\'epartition de $X$ est continue
\item Pour tout $u \in \rset$,
$$
\lim_{n \to \infty} \PE[ \rme^{\rmi u X_n}]= \PE[\rme^{\rmi u X}] \eqsp.
$$
\end{itemize}
\end{frame}

%\begin{frame}
%\frametitle{Convergence en loi}
%\begin{itemize}
%\item \alert{Attention...}  ce sont les lois qui \alert{convergent}... Si $X$ et $-X$ ont la m\^eme loi (par exemple, $X \sim \mathcal{N}(0,1)$), on a simultan\'ement
%$$
%X_n \dlim X \quad \text{et} \quad X_n \dlim -X
%$$
%\item On peut avoir $X_n \dlim X$ et $Y_n \dlim Y$ \alert{sans avoir}
%$(X_n,Y_n) \dlim (X,Y)$ (on n'a d'ailleurs pas sp\'ecifi\'e la loi jointe du couple $(X,Y)$)
%\item Par contre, si $(X_n,Y_n) \dlim (X,Y)$, on a pour toute fonction $\phi$ continue, $\phi(X_n,Y_n) \dlim \phi(X,Y)$, et donc $X_n \dlim X$ et $Y_n \dlim Y$.
%\end{itemize}
%\end{frame}




\begin{frame}
\frametitle{Rappel : th\'eorème central-limite}
\begin{theo}
Si $Y_1,\ldots, Y_n$ i.i.d., $\mu=\E\big[Y_i\big]$, $0< \sigma^2=\text{Var}[Y_i]<+\infty$, alors
$$\sqrt{n}\Big(\frac{1}{n}\sum_{i = 1}^n Y_i-\mu\Big) \stackrel{d}{\longrightarrow} {\mathcal N}(0,\sigma^2).$$
\end{theo}
\end{frame}



\begin{frame}
\frametitle{Interpr\'etation et application}
\begin{itemize}
\item Interpr\'etation du TCL :
$$\frac{1}{n}\sum_{i = 1}^n Y_i = \mu + \frac{\sigma}{\sqrt{n}}\, \xi^{(n)},\;\;\xi^{(n)} \stackrel{d}{\approx} {\mathcal N}(0,1).$$
\item \underline{Application} : $Y_i = 1_{\{X_i \leq x_0\}}$, $\mu = F(x_0)$, $\sigma({\color{red}F}) = F(x_0)^{1/2}(1-F(x_0)\big)^{1/2}$.

On a
\begin{align*}
\PP\big[\big|\widehat F_n(x_0)-F(x_0)\big|\geq \varepsilon_n\big] & = \PP\Big[\Big|\xi^{(n)}\Big| \geq \frac{\sqrt{n}\,\varepsilon_n}{\sigma({\color{red}F})}\Big] \\
& = \PP\Big[\Big|\xi^{(n)}\Big|\geq \frac{\varepsilon_0}{\sigma({\color{red} F})}\Big]
\end{align*}
pour la calibration $\varepsilon_n = \varepsilon_0/\sqrt{n}$ ($\varepsilon_0$ reste à choisir).
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{TCL et intervalle de confiance (suite)}
Il vient
\begin{align*}
 \PP\Big[\Big|\xi^{(n)}\Big|\geq \frac{\varepsilon_0}{\sigma({\color{red} F})}\Big]
& \rightarrow \int_{|x|\geq \varepsilon_0/\sigma({\color{red}F})}e^{-x^2/2}\frac{dx}{\sqrt{2\pi}} \\
& = 2\Big(1-\Phi\big(\varepsilon_0/\sigma({\color{red}F})\big)\Big) \\
&\leq \alpha,
\end{align*}
avec $\Phi(x) = \int_{-\infty}^xe^{-t^2/2}dt$, ce qui donne
$$\boxed{\varepsilon_0 = \sigma({\color{red}F})\Phi^{-1}\big(1-\alpha/2\big).}$$
\end{frame}
\begin{frame}
\frametitle{TCL et intervalle de confiance : (suite)}
\begin{itemize}
\item On a montr\'e
$$\PP\big[\big|\widehat F_n(x_0)-F(x_0)\big|\geq \frac{\sigma({\color{red}F})}{\sqrt{n}}\Phi^{-1}\big(1-\alpha/2\big)\big] \rightarrow \alpha.$$
\item {\color{red}\underline{Attention !}} ceci ne fournit {\color{red}pas} un intervalle de confiance : $\sigma({\color{red}F})={\color{red}F}(x_0)^{1/2}\big(1-{\color{red}F}(x_0)\big)^{1/2}$ est inconnu !
\item \underline{Solution} : remplacer $\sigma({\color{red}F})$ par  $\widehat F_n(x_0)^{1/2}\big(1-\widehat F_n(x_0)\big)^{1/2}$ observable.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Lemme de Slutsky}
\begin{lem}
Si $X_n \stackrel{d}{\rightarrow} X$ et $Y_n \stackrel{\PP}{\rightarrow} c$ (constante), alors $(X_n,Y_n) \stackrel{d}{\rightarrow} (X,Y)$.
\end{lem}
\end{frame}


\begin{frame}
\frametitle{TCL et intervalle de confiance : conclusion}
\begin{prop}
Pour tout $\alpha \in (0,1)$,
$${\mathcal I}_{n,\alpha}^{{\tt asymp}} = \left[\widehat F_n(x_0)\pm\frac{\widehat F_n(x_0)^{1/2}\big(1-\widehat F_n(x_0)\big)^{1/2}}{\sqrt{n}}\Phi^{-1}(1-\alpha/2)\right]$$
est un intervalle de confiance asymptotique pour $F(x_0)$ au niveau de confiance $1-\alpha$ :
$$\PP\big[F(x_0)\in{\mathcal I}_{n,\alpha}^{{\tt asymp}} \big] \rightarrow 1-\alpha.$$
\end{prop}
Le passage $\sigma({\color{red}F}) \longrightarrow \widehat F_n(x_0)^{1/2}\big(1-\widehat F_n(x_0)\big)^{1/2}$ est licite via le lemme de Slutsky.
\end{frame}
%\begin{frame}
%\frametitle{Observation finale}
%Comparaison des longueurs des 3 intervalles de confiance :
%\begin{itemize}
%\item \underline{Tchebychev (non-asymptotique)}
%$\frac{2}{\sqrt{n}} \frac{1}{2} \frac{1}{\sqrt{\alpha}}$
%\item \underline{{\color{red}Hoeffding (non-asymptotique)}}
%$\frac{2}{\sqrt{n}} \sqrt{\frac{\alpha}{2}\log \frac{2}{\alpha}}$
%\item \underline{TCL (asymptotique)}
%$\frac{2}{\sqrt{n}} \widehat F_n(x_0)^{1/2}\big(1-\widehat F_n(x_0)\big)^{1/2} \Phi^{-1}(1-\alpha/2).$
%\item La longueur la plus petite est ({\color{red}sans surprise !}) celle fournie par le TCL. Mais
%% la longueur de l'intervalle de confiance fournie par l'in\'egalit\'e de
%Hoeffding {\color{red} comparable} au TCL en $n$ et $\alpha$ (dans la limite $\alpha\rightarrow 0$).
%\end{itemize}
%\end{frame}
%\subsection{Estimation uniforme}
%\begin{frame}
%\frametitle{Estimation uniforme}
%\begin{itemize}
%\item On \og sait \fg{} estimer $F(x_0)$, pour un $x_0$ donn\'e. Qu'en est-il de l'estimation {\color{red} globale} de $F$ :
%$$\big(F(x), x \in \R\big) ?$$
%\item 3 r\'esultats pour passer de l'estimation en un point à {\color{red}l'estimation globale} :
%\begin{itemize}
%\item Glivenko-Cantelli (convergence uniforme)
%\item Kolomogorov-Smirnov (vitesse de convergence, asymptotique)
%\item In\'egalit\'e de DKV (vitesse de convergence, non-asymptotique)
%\end{itemize}
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Glivenko-Cantelli, Kolmogorov-Smirnov}
%$X_1,\ldots, X_n$ i.i.d. de loi $F$, $\widehat F_n$ leur fonction de r\'epartition empirique.
%\begin{prop}
%\begin{itemize}
%\item (Glivenko-Cantelli) $$\sup_{x \in \R}\big|\widehat F_n(x)-F(x)\big| \stackrel{\mathrm{p.s.}}{\rightarrow}0.$$
%\item (Kolmogorv-Smirnov) Si $F$ est continue,
%$$\sqrt{n}\sup_{x \in \R}\big|\widehat F_n(x)-F(x)\big| \stackrel{d}{\rightarrow} \mathbb{B},$$
%$\mathbb{B}$ loi connue {\color{red} ind\'ependante} de $F$.
%\end{itemize}
%\end{prop}
%\end{frame}
%\begin{frame}
%\frametitle{In\'egalit\'e de DKW}
%$X_1,\ldots, X_n$ i.i.d. de loi $F$ {\color{red} continue}, $\widehat F_n$ leur fonction de r\'epartition empirique.
%\begin{prop}[In\'egalit\'e de Dvoretsky-Kiefer-Wolfowitz] Pour tout $\varepsilon >0$.
%$$\PP\big[\sup_{x \in \R}\big|\widehat F_n(x)-F(x)\big|\geq \varepsilon\big] \leq 2 \exp\big(-2n\varepsilon^2\big).$$
%\end{prop}
%\begin{itemize}
%\item R\'esultat difficile (th\'eorie des processus empiriques).
%\item Permet de construire des {\color{red} r\'egions} de confiance avec des r\'esultats similaires au cadre ponctuel :
%$$\PP\Big[\forall x \in \R, F(x)\in \big[\widehat F_n(x)\pm\sqrt{\tfrac{1}{2n}\log \tfrac{2}{\alpha}}\big]\Big]\geq 1-\alpha.$$
%\end{itemize}
%\end{frame}
%\subsection{Estimation de fonctionnelles}
%\begin{frame}
%\frametitle{Estimation de fonctionnelles}
%\begin{itemize}
%\item {\color{red} Objectif :} estimation d'une fonctionnelle $T(F) \in \R$. \\
%\item Exemples
%\begin{itemize}
%\item \underline{D\'ejà vu} : valeur en un point  $T(F) = F(x_0)$
%\item \underline{Fonctionnelle r\'egulière} :
%$$T(F) = h\left(\int_{\R}g(x)dF(x)\right),$$
%où $g,h: \R \rightarrow \R$ sont {\color{red} r\'egulières}
%\item {\color{red}Autres cas...}
%\end{itemize}
%\item \underline{Principe} : si $F \leadsto T(F)$ est \og r\'egulière \fg{}, un estimateur \og naturel \fg{} est $T(\widehat F_n)$
%({\color{red}estimateur par plug-in}).
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Exemples}
%\begin{itemize}
%\item \underline{Moyenne} : $m(F) = \int_{\R}xdF(x)$.
%\item \underline{Variance} :
%\begin{align*}
%\sigma^2(F) & = \int_{\R}\big(x-m(F)\big)^2dF(x)\\
%& = \int_{\R}x^2 dF(x)-\big(\int_{\R}xdF(x)\big)^2
%\end{align*}
%\item \underline{Asym\'etrie (skewness)} :
%$$\alpha(F)=\frac{\int_{\R}\big(x-m(F)\big)^3dF(x)}{\sigma^2(F)^{3/2}}=\cdots.$$
%\item \underline{Aplatissement (kurtosis)} :
%$$\kappa(F) = \frac{\int_{\R}\big(x-m(F)\big)^4dF(x)}{\sigma^2(F)^2}=\cdots.$$
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Estimation de fonctionnelles r\'egulières}
%\begin{itemize}
%%\item \underline{Principe} : si $F \leadsto T(F)$ est \og r\'egulière \fg{}, alors $T(\widehat F_n)$ est un \og bon \fg{} estimateur de $T(F)$ (estimateur par plug-in).
%\item Cas où $T(F) = h\big(\int_{\R}g(x)dF(x)\big)$
%%\item $\int_{\R} \varphi(x)dF(x) = \int_{\R}\varphi(x) \PP^X(dx)$, où $X \sim F$.
%\item \underline{Formule de calcul} :
%$$\boxed{\int_{\R}\varphi(x)d\widehat F_n(x) = \frac{1}{n}\sum_{i = 1}^n\varphi(X_i).}$$
%Traduction : {\color{red}une variable al\'eatoire de loi $\widehat F_n$ prend les valeurs $X_i$ avec probabilit\'e $1/n$.}
%\item Estimateur par {\color{red} substitution} ou plug-in de $T(F)$ :
%$$\boxed{T(\widehat F_n) = h\Big(\tfrac{1}{n}\sum_{i = 1}^ng(X_i)\Big)}$$
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Performance de l'estimateur par substitution}
%\begin{itemize}
%\item {\color{red}Convergence} si $g,h:\R\rightarrow \R$ continues, alors $T(\widehat F_n)\stackrel{\mathrm{p.s.}}{\rightarrow} T(F)$ (loi forte des grands nombres).
%\item {\color{red} Vitesse de convergence, Etape 1.}
%TCL :
%$$\sqrt{n}\big(\tfrac{1}{n}\sum_{i=1}^ng(X_i)-\int_{\R}g(x)dF(x)\big) \stackrel{d}{\rightarrow} {\mathcal N}\big(0,\mathrm{Var}\big[g(X)\big]\big),$$
%où
%\begin{align*}
%\text{Var}\big[g(X)\big] &= \E\big[g(X)^2\big]-\big(\E[g(X)]\big)^2 \\
%&= \int_{\R}g(x)^2dF(x)-\big(\int_{\R}g(x)dF(x)\big)^2.
%\end{align*}
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Vitesse de convergence (suite)}
%\begin{itemize}
%\item{\color{red} Etape 2.} On a $\sqrt{n}(Z_n -c_1)\stackrel{d}{\rightarrow} {\mathcal N}(0,c_2)$.
%Comment transf\'erer ce r\'esultat à $h(Z_n)\rightarrow h(c_1)$ ?
%\item {\color{red} M\'ethode \og delta\fg{}} : si $h$ continûment diff\'erentiable
%$$\sqrt{n}\big(h(Z_n)-h(c_1)\big) = \sqrt{n}(Z_n-c_1)h'(\eta_n),\;\;\eta_n \in \big[Z_n,c_1\big].$$
%On a  $\sqrt{n}(Z_n-c_1)\stackrel{d}{\rightarrow} {\mathcal N}(0,c_2)$ et $h'(\eta_n)\stackrel{\PP}{\rightarrow}h'(c_1)$.
%%car $Z_n\stackrel{\PP}{\rightarrow} c_1$.

%{\color{red}Lemme de Slutsky} :
%$$ \sqrt{n}(Z_n-c_1)h'(\eta_n) \stackrel{d}{\rightarrow} {\mathcal N}(0,c_2) h'(c_1).$$
%% \stackrel{d}{=} {\mathcal N}\big(0,h'(c_1)^2c_2\big).$$
%Finalement
%$$\boxed{\sqrt{n}\big(h(Z_n)-h(c_1)\big) \stackrel{d}{\rightarrow} {\mathcal N}\big(0,c_2h'(c_1)^2\big)}$$
%\end{itemize}
%\end{frame}
%\begin{frame}
%\frametitle{Conclusion}
%\begin{prop}
%Si $\E[g(X)^2]<+\infty$ et $h$ continûment diff\'erentiable, alors
%$$\sqrt{n}\big(T(\widehat F_n)-T(F)\big)\stackrel{d}{\rightarrow} {\mathcal N}\big(0,v({\color{red}F})\big),
%$$
%où $v({\color{red}F}) = h'\big(\E\big[g(X)\big]\big)^2\mathrm{Var}\big[g(X)\big]$.
%\end{prop}
%Pour construire un {\color{red} intervalle de confiance}, il faut encore remplacer $v({\color{red}F})$ par $v(\widehat F_n)$. Alors
%$v(\widehat F_n)\stackrel{\PP}{\rightarrow} v(F)$ et, via le lemme de Slutsky,
%$$
%\sqrt{n}\frac{T(\widehat F_n)-T(F)}{v(\widehat F_n)^{1/2}}\stackrel{d}{\rightarrow} {\mathcal N}\big(0,1\big).
%$$
%On peut maintenant en d\'eduire une intervalle de confiance asymptotique comme pr\'ec\'edemment.
%\end{frame}
%%\begin{frame}
%%\frametitle{Exemple en dimension 1}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Le cas de la dimension $d>1$}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Limites de l'approche empirique}
%%L'estimation de $T(F)$ par $T(\widehat F_n)$ n'est pas toujours {\color{red}possible} ou {\color{red}souhaitable} :
%%\begin{itemize}
%%\item La fonctionnelle $F \leadsto T(F)$ n'est pas \og r\'egulière\fg{},
%%\item On dispose d'information {\color{red} a priori} suppl\'ementaire : $F$ appartient à une sous-classe {\color{red} particulière} de distributions, et il y a des choix plus judicieux que l'estimateur par plug-in,
%%\item La param\'etrisation $F \leadsto T(F)$ ne donne {\color{red}pas} lieu à une {\color{red}forme analytique simple}.
%%%et ne s'\'etudie pas à force d'arguments standard bas\'es sur la loi des grands nombres et le TCL
%%$\rightarrow$ autres approches.
%%\end{itemize}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Exemple 1 : fonctionnelle irr\'egulière}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Exemple 2 : information suppl\'ementaire}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Exemple 3 : param\'etrisation non-standard}
%%\end{frame}
%%\begin{frame}
%%\frametitle{Conclusion}
%%\end{frame}
%%%\item {\color{red} hypothèse suppl\'ementaire} : $F$ admet une densit\'e {\color{red}continue} $x \leadsto f(x)$ par rapport à la mesure de Lebesgue. Estimation de
%%%$$T(F) = f(x_0)\;?$$
%%%On a $F'(x) = f(x)$, mais {\color{red} on ne peut pas r\'ealiser} $\widehat F_n'(x_0)$ : $x \leadsto \widehat F_n(x)$ est constante par morceaux !
%%%\item  {\color{red} hypothèse suppl\'ementaire} : $F$ est de la forme $F_\theta(x) = G(x-\theta)$, $\theta \in \R$, où
%%%$$G(x) = x1_{x \in [0,1)}+1_{\{x >1\}}\;\;(\text{loi uniforme sur $[0,1]$}).$$
%%%Alors
%%%\end{itemize}
%%%\item kjh
%%%\end{itemize}
%%%\end{frame}

\setbeamertemplate{background canvas}[default]
\setbeamercolor{background canvas}{bg=black}
\begin{frame}[plain]{}
\end{frame}
\end{document}
