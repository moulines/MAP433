\documentclass[a4paper,11pt,fleqn]{article}

\usepackage[francais]{babel}
%\usepackage[latin1]{inputenc}
%\usepackage[applemac]{inputenc}
\usepackage{a4wide,amsmath,amssymb,bbm,fancyhdr,hyperref, graphicx}

\RequirePackage[OT1]{fontenc}

\usepackage[latin1]{inputenc}
% THE variable
\newcommand{\thisyear}{Ann\'ee 2015-2016}

% Definitions (pas trop!)
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\ac}[1]{\left\{#1\right\}}


\newcommand{\bY}{\mathbf{Y}}
\newcommand{\hatbY}{\widehat{\bY}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\bZ}{\mathbf{Z}}
\newcommand{\bxi}{\mathbf{\xi}}
\newcommand{\G}{\mathbf{G}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\bL}{\mathbf{L}}


% Style
\pagestyle{fancyplain}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\footnotesize {\sf
MAP433 Statistique, \thisyear / PC5}}}
\rhead[\fancyplain{}{\footnotesize {\sf MAP433 Statistique,
\thisyear / \rightmark}}]{\fancyplain{}{\thepage}}
\cfoot[\fancyplain{}{}]{\fancyplain{}{}}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\parindent=0mm


%-- Si cette ligne est lue, le corrigé ne s'affiche pas :
%%\newcommand{\Corrige}[1]{}
%-- Si la ligne suivante est lue, le corrigé s'affiche :
\newcommand{\Corrige}[1]{\noindent {\small {\bf Corrigé :}\\ #1} }

% Titre
%\title{{\bf MAP433 Statistique}}
\title{\includegraphics[width=3.5cm]{logo_x.jpg}\hfill {\bf MAP433
    Statistique}\hfill \quad\quad\quad\quad\quad\quad \ } \author{{\bf PC 5:
    R\'egression}} \date{25 septembre 2015}

\begin{document}

\maketitle




\section{Mod\`ele de r\'egression multiple}
On consid\`ere le mod\`ele de regression multiple
\[
\bY = \beta_{0} \be + \X \beta+ \sigma \bxi, \quad\textrm{o\`u}\ \ 
\E[\bxi]=0,~\E[\bxi \bxi^T]= I_n,\ \be=(1,1,\ldots,1)^T
\] avec $\X$ une
matrice $n\times k$ de rang $k$ et $\bY$, $\bxi$ des vecteurs de
$\mathbb{R}^n$.  Les param\`etres $\beta_{0}\in\R$ et $\beta\in\R^k$ sont
inconnus. On note $\hat \beta_{0}$ et $\hat \beta$ les estimateurs des moindres
carr\'es de $\beta_{0}$ et $\beta$.
\begin{enumerate}
\item On note \quad $\hatbY = \hat \beta_{0} \be+ \X \hat \beta$ \qquad
  $\overline Y= n^{-1} \be^T \bY$ et $\quad \overline{\hatbY} = n^{-1} \be^T
  \hatbY$. Montrer que $\overline{\hatbY} =\overline Y$.  En d\'eduire que
  $\overline Y = \hat\beta_{0}+ \left(n^{-1} \be^T \X \right) \hat \beta$.
\item Montrer l'\'equation d'analyse de la variance:
  $$\|\bY- \overline Y \be\|^2=\|\bY-\hatbY\|^2+\|\hatbY-\overline Y \be\|^2.$$
  En
  d\'eduire que le {\it coefficient de d\'etermination}
  $$R^2=\frac{\sum_{i=1}^n(\hat Y_i-\bar Y)^2}{\sum_{i=1}^n(Y_i-\bar Y)^2}
  \qquad \text{o\`u} \ \ \bY = 
  \begin{bmatrix}
    Y_1 \\
    \cdots \\
    Y_n
  \end{bmatrix} \qquad \qquad \hatbY = 
  \begin{bmatrix}
    \hat Y_1 \\
    \cdots \\
    \hat Y_n
  \end{bmatrix}
  $$
  est toujours inf\'erieur \`a 1.
\item Supposons que $\bZ=[\be,\X]$ est de rang $k+1$.  Calculez en fonction de
  $\bZ$ la matrice de covariance de $(\hat \beta_{0},\hat\beta)$. Comment
  acc\`ede-t-on \`a Var$(\hat\beta_{j})$, pour $j=0,\ldots,k$?
\item Proposer un estimateur sans biais de $\sigma^2$ puis de la matrice de
  covariance de $(\hat \beta_{0},\hat \beta)$.
\item On suppose dor\'enavant que $\beta_{0}=0$ et donc \quad $\bY=\X \beta+
  \sigma \bxi$ \quad avec $\E[\bxi]=0$ et $\E[\bxi\bxi^T]=I_n$.  L'estimateur
  des moindres carr\'es $\tilde \beta$ dans ce mod\`ele est-il \'egal \`a $\hat
  \beta$?
\item A-t-on la relation $\overline{\hatbY} =\overline Y$? Que dire du $R^2$
  dans ce mod\`ele?
\end{enumerate}

\Corrige{ Dans ce corrigé, on note $\bZ = [\be,\X]$.
  \begin{enumerate}
  \item D'une part, $\bar Y \be$ est le projeté orthogonal de $\bY$ sur $\be$.
    D'autre part, $\hatbY$ est le projeté orthogonal de $\bY$ sur
    $\mathrm{Im}(\bZ)$; puis $\overline{\hatbY}$ est le projeté
    orthogonal de $\hatbY$ sur $\be$. Donc $\bar Y = \overline{ \hatbY}$. Comme
\[
\overline{\hatbY} = n^{-1} \be^T \left( \hatbY \right) = n^{-1} \be^T
\left(\hat\beta_{0} \be + \X \hat \beta \right)
\]
on obtient le résultat demandé.
\item On écrit $\bY- \overline Y \be = \bY - \hatbY +\hatbY - \overline Y \be$.
  Puisque $\bY - \hatbY$ est dans l'orthogonal de $\mathrm{Im}(\bZ)$ et $\hatbY
  - \overline Y \be$ est dans $ \mathrm{Im}(\bZ)$, on obtient la décomposition
  de la variance. On en déduit que $R^2 \leq 1$ en observant que $R^2 =
  \|\hatbY - \bar Y \be \|^2 / \|\bY - \bar Y \be \|^2$.
\item $\bZ$ est une matrice $n \times (k+1)$; si elle est de rang $k+1$, alors
  $Z^T Z$ est inversible. La covariance de $(\hat \beta_0, \hat \beta)$ est
  $\sigma^2 (\bZ^T \bZ)^{-1}$. La diagonale de cette matrice collecte la
  variance de chaque composante du vecteur $(\hat \beta_0, \hat \beta_1,
  \cdots, \hat \beta_k)$.
\item Estimateur sans biais
\[
\hat{\sigma}^2_n = \frac{1}{n-(k+1)} \|\bY - \hatbY \|^2 
\]
\item $\tilde \beta \neq \beta$.
\item on n'a plus la relation ``$\overline{\hatbY} = \bar Y$''. On peut avoir
  $R^2 \geq 1$.
  \end{enumerate}
}

\section{Le mod\`ele ANOVA}
On dispose d'observations de variables al\'eatoires
$$
Y_{ij}= m_i + \xi_{ij}, \quad i=1,\dots, k, \quad j=1,\dots, l,
$$
o\`u $\m=(m_1,\dots,m_k)^T\in \R^k$ et les $\xi_{ij}$ sont des variables
al\'eatoires i.i.d. de loi ${\cal N}(0,\sigma^2)$.
\begin{enumerate}
\item Montrer qu'il s'agit d'un mod\`ele de r\'egression lin\'eaire
avec la matrice ${\bf X}$ que l'on pr\'ecisera. Que vaut $B={\bf X}^T{\bf X}$?
\item Montrer que la condition $m_1=m_2=\dots=m_k$  s'\'ecrit sous la forme $\G \m =0$ avec une
matrice $\G$ que l'on pr\'ecisera.
\item On estime $\m$ par l'estimateur des moindre carr\'es $\widehat \m$.
  Quelle est la covariance de $\widehat \m$?
\item Proposer un estimateur de $\G \m$. Quel est son biais? sa covariance?
\item Proposer un estimateur $\hat \sigma^2$ de $\sigma^2$. Quelle est sa distribution?
\end{enumerate}
\Corrige{ On note $\be_{1:q}$ le vecteur (colonne) de $\rset^q$ dont toutes les
  composantes valent $1$.
  \begin{enumerate} 
  \item On a
\[
\X = 
\begin{bmatrix}
  \be_{1:l} & \cdots & &  \\
  \cdots & \be_{1:l} &\cdots & \cdots \\
  \cdots & \cdots &\cdots & \cdots \\
  \cdots & \cdots & \cdots  & \be_{1:l}
\end{bmatrix} \in \rset^{n \times k} \qquad \m = 
\begin{bmatrix}
  m_1 \\
\cdots \\
m_k
\end{bmatrix} \in \rset^k.
\]
Par suite,
\[
\X^T \X = l \ \mathrm{I}_{k \times k}
\]
\item On a 
\[
\G = 
\begin{bmatrix}
  1 & -1 & \cdots & \cdots & \cdots & \cdots \\
0 & 1 & -1 & \cdots & \cdots & \cdots \\
 \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
 \cdots & \cdots & \cdots & \cdots & 1 & -1\\
\end{bmatrix} \in \rset^{(k-1) \times k}
\]
\item
\[
\widehat{\m} = \left(\X^T \, \X \right)^{-1} \ \X^T \bY = l^{-1} \X^T \bY = 
\begin{bmatrix}
  l^{-1} \sum_{j=1}^l Y_{1,j} \\
  \cdots \\
  l^{-1} \sum_{j=1}^l Y_{k,j}
\end{bmatrix}
\]
La covariance de $\widehat{\m}$ est $\sigma^2 l^{-1} \ I$.
\item Un estimateur de $\G \m$ est $ \G \widehat{\m}$; qui est sans biais et
  dont la matrice de covariance est $\sigma^2 l^{-1} \, \G \G^T$.
\item Un estimateur sans biais de $\hat \sigma^2$ est 
\[
\hat \sigma^2 = \frac{1}{n-k} \| \bY - \X \widehat{\m}\|^2 = \frac{1}{k(l-1)} \|
\bY - \X \widehat{\m}\|^2 = \frac{1}{k(l-1)} \sum_{i=1}^k \sum_{j=1}^l \left(
  Y_{i,j} - \bar \bY_{i\cdot} \right)^{2}.
\]
Et sa loi est $\frac{\sigma^2}{k(l-1)} \ \chi^2(k(l-1))$.
  \end{enumerate}
}





\section{Th\'eor\`eme de Gauss-Markov}
On consid\`ere le mod\`ele de r\'egression \quad
$\underset{(n,1)}{\bY}=\underset{(n,k)}{\X}\underset{(k,1)}{\beta} +\sigma
\underset{(n,1)}{\bxi}.$ On suppose que $\X$ est une matrice d\'eterministe,
$\E[\bxi]=0$, $\E[\bxi\bxi^T]= I_n$, Rang$(\X)=k$. On note $\hat{\beta}$
l'estimateur des MC de $\beta$.
\begin{enumerate}
\item Montrer que $\hat{\beta}$ est sans biais et expliciter
sa matrice de covariance.
\item Soit $\tilde\beta$ un estimateur de $\beta$ lin\'eaire en $\bY$, i.e.,
  $\tilde\beta = \bL \bY$ pour une matrice $\bL \in\R^{k\times n}$ d\'eterministe.
  Donner une condition n\'ecessaire et suffisante sur $\bL$ pour que $\tilde\beta$
  soit sans biais. On supposera maintenant cette hypoth\`ese v\'erifi\'ee.
\item Calculer la matrice de covariance de $\tilde\beta$.
En posant $\Delta=\bL-(\X^T \, \X)^{-1} \X^T$ montrer que $\Delta \X=0$ et cov$(\tilde \beta)=\ $cov$(\hat\beta)+\sigma^2\Delta\Delta^T$.
En d\'eduire que
$$
\E[(\tilde\beta - \beta)(\tilde\beta - \beta)^T]\ge \E[(\hat{\beta} -
\beta)(\hat{\beta} - \beta)^T] \quad\textrm{(in\'egalit\'e au sens matriciel).}
$$
\item En passant aux risques quadratiques $\E\big[\|\tilde\beta -
  \beta\|^2\big]$ et $\E\big[\|\hat{\beta} - \beta\|^2\big]$, en d\'eduire que
  l'estimateur des MC est optimal dans la classe de tous les estimateurs
  lin\'eaires sans biais.
\end{enumerate}
\Corrige{
  \begin{enumerate}
  \item $\E\left[\hat \beta\right] = \beta$ et $\mathrm{Cov}(\hat \beta) =
    \sigma^2 (\X^T \X)^{-1}$.
  \item $\L \X = I$.
  \item $\mathrm{Cov}(\tilde \beta) = \sigma^2 \L \L^T$. De plus, 
\[
\Delta \Delta^T = \L \L^T + (\X^T \X)^{-1} - \L \X (\X^T \X)^{-1} - (\X^T
\X)^{-1} \X^T \L^T
\]
et les deux derniers termes valent $-2 (\X^T \X)^{-1}$ puisque $\L \X = I$.  On
en déduit la relation sur les matrices de covariance; puis la relation de
domination en observant que $\lambda^T \Delta \Delta^T \lambda = \|\Delta^T
\lambda\|^2 \geq 0$ pour tout $\lambda \in \rset^k$.
\item La relation entre les matrices entraine en particulier que tout $i$,
  $\mathrm{Var}(\tilde \beta_i) \geq \mathrm{Var}(\beta_i)$.  Le risque
  quadratique est la trace de la matrice de covariance, et c'est aussi la somme
  des variances de chaque composante.
  \end{enumerate}
}


\section{R\'egression Ridge}
On consid\`ere le mod\`ele de r\'egression \qquad
$\underset{(n,1)}{\bY}=\underset{(n,k)}{\X}\underset{(k,1)}{\beta}+ \sigma
\underset{(n,1)}{\bxi}$. \quad On suppose que $\X$ est une matrice
d\'eterministe, $\E[\bxi]=0$, $\E[\bxi\bxi^T]=I_n$.
\begin{enumerate}
\item On suppose que $k> n$. Que dire de l'estimation par moindres carr\'es?
\item On appelle estimateur \texttt{Ridge regression} de param\`etre de
  r\'egularisation $\lambda>0$ l'estimateur
  $$
  \hat \beta_{\lambda}= \arg \min_{\beta\in\R^k}\left\{ \|\bY-
    \X\beta\|^2+\lambda \|\beta\|^2\right\}.
  $$
  Exprimez $\hat \beta_{\lambda}$ en fonction de $\X$, $\bY$ et $\lambda$.
  Cet estimateur est-il d\'efini pour $k>n$?
\item Calculez la moyenne et la matrice de covariance de
l'estimateur Ridge. Est-il sans biais?
\item On suppose maintenant que $k=1$, ce qui correspond au mod\`ele de
  r\'egression simple.  Montrer qu'il existe une valeur de $\lambda$ telle que,
  pour certaines valeurs de $\beta$, le risque $\E\big[(\hat\beta_\lambda -
  \beta)^2\big]$ de l'estimateur Ridge de param\`etre $\lambda$ est inf\'erieur
  au risque $\E\big[(\hat\beta_0 - \beta)^2\big]$ de l'estimateur des MC.
\end{enumerate}
\Corrige{
  \begin{enumerate}
  \item Il n'y a pas unicité de l'estimateur MC.
  \item On a $\hat \beta_\lambda = \left(\X^T \X + \lambda I \right)^{-1} \X^T
    \bY$ qui est défini pour $k>n$.
\item Son espérance et sa matrice de covariance sont respectivement
 \[
 \left(\X^T \X + \lambda I \right)^{-1} \X^T \X \beta,  \qquad \qquad \sigma^2 \ 
 \left(\X^T \X + \lambda I \right)^{-1} \X^T \X \left(\X^T \X + \lambda I
 \right)^{-1}.
\]
\item Notons $R_\lambda(\beta)$ le risque de l'estimateur Ridge; et
  $R_0(\beta)$ celui de l'estimateur MC. Si $\beta^2 \|\X\|^2/\sigma^2 <1$
  alors pour tout $\lambda>0$, $R_\lambda(\beta) < R_0(\beta)$. Si $\beta^2
  \|\X\|^2/\sigma^2 >1$ alors pour tout $\lambda$ {\bf assez petit},
  $R_\lambda(\beta) < R_0(\beta)$.
  \end{enumerate}
}


\end{document}