
\input{../../def/defslide}


\title{MAP 433 : Introduction aux méthodes statistiques. Cours 7}
%\author{Marc Hoffmann}
%\institute{Université Paris-Dauphine}
\begin{document}
\date{9 Octobre 2015}
\maketitle



\begin{frame}
\frametitle{Aujourd'hui}
\tableofcontents
\end{frame}






%\section{Construction d'un test : hypothèses générales}
%
%
%%\frametitle
%
%\begin{frame}
%\frametitle{Situation}
%\begin{itemize}
%\item \underline{Situation} : on part d'une expérience statistique $\big(\Xset, \Xsigma,\{\PP_\truetheta, \truetheta \in \Theta\}\big)$ engendrée par l'observation $Z$.
%\item On souhaite tester :
%$$H_0:\;\truetheta \in \Theta_0 \subset \Theta\;\;\;\text{\alert{contre}}\;\;\;H_1:\truetheta \in \Theta_1$$
%avec $\alert{\Theta_0 \cap \Theta_1 = \emptyset}$.
%\item Si $\Theta_0 = \{\truetheta_0\}$ et $\Theta_1 = \{\truetheta_1\}$, on a Neyman-Pearson. \alert{ Et sinon ?}
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Principe de construction}
%\begin{itemize}
%\item  Trouver  une \alert{statistique libre sous l'hypothèse} : toute quantité $\phi(Z)$ \alert{observable} dont on connait la loi sous l'hypothèse, c'est-à-dire la loi de $\phi(Z)$ sous $\PP_\truetheta$ avec $\truetheta \in \Theta_0$.
%\item On  regarde  si le comportement de $\phi(Z)$ est \alert{typique} d'un comportement sous l'hypothèse.
%\item Si oui, on \alert{accepte} $H_0$, si non on \alert{rejette} $H_0$.
%\item On quantifie  oui/non  par le niveau $\alpha$ du test.
%\end{itemize}
%\end{frame}
%
%\subsection{Retour sur un exemple}
%\begin{frame}
%\frametitle{Exemple : test sur la variance}
%\begin{itemize}
%\item On observe $Z=(Y_1,\ldots, Y_n)$,
%$$Y_1,\ldots, Y_n \sim_{\text{i.i.d.}}{\mathcal N}(\mu,\sigma^2)$$
%avec $\truetheta =(\mu,\sigma^2) \in \Theta = \R \times (0,+\infty)$.
%\item \alert{Premier cas} : on teste
%$$ \alert{ H_0:\sigma^2 = \sigma_0^2\;\;\;\text{ contre}\;\;\; H_1:\sigma^2 > \sigma_0^2.}$$
%\item Sous l'hypothèse (c'est-à-dire sous $\PP_\truetheta$ avec $\truetheta = (\mu,\sigma_0)$ et $\mu \in \R$ quelconque), on a
%$$\boxed{(n-1)\frac{s_n^2}{\sigma_0^2} \sim \chi^2(n-1)}$$
%avec $s_n^2:=\frac{1}{n-1}\sum_{i = 1}^n (Y_i-\overline{Y}_n)^2$.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (cont.)}
%\begin{itemize}
%\item Donc, \alert{sous l'hypothèse}, le comportement  typique de
%$$\phi(Z) = (n-1)\frac{s_n^2}{\sigma_0^2} $$
%est celui d'une variable aléatoire de loi du $\chi^2$ à $n-1$ degrés de liberté.
%\item Soit $q_{1-\alpha,n-1}^{\chi^2}>0$ tel que si $U \sim {\chi^2}(n-1)$, alors
%$$\PP\big[U > q_{1-\alpha,n-1}^{\chi^2}\big] = \alpha.$$
%\item \alert{Sous l'hypothèse} $\phi(Z)\stackrel{d}{=} U$ et donc la probabilité pour que $\phi(Z)$ dépasse $q_{1-\alpha,n-1}^{\chi^2}$ est inférieure (égale) à $\alpha$ (comportement atypique si $\alpha$ petit).
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (cont.)}
%\begin{itemize}
%\item \alert{Règle de décision :} On accepte l'hypothèse si
%$$\phi(Z) \leq  q_{1-\alpha,n-1}^{\chi^2}.$$
%On la rejette sinon.
%\item Par construction, on a un \alert{test de niveau $\alpha$}.
%\item On ne \alert{sait rien dire sur l'erreur de seconde espèce}, mis à part qu'elle est minimale parmi les tests de zone de rejet de la forme de $\{\phi(Z) > c\}$, $c>0$...
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (fin)}
%\begin{itemize}
%\item \alert{Deuxième cas :} On teste
%$$H_0: \alert{ \sigma^2 \leq \sigma_0^2}\;\;\;\text{ contre}\;\;\; H_1:\sigma^2 > \sigma_0^2.$$
%\item \alert{Pas de statistique libre évidente...} Mais, pour $\sigma^2 \leq \sigma_0^2$, on a
%\begin{align*}
%\PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma_0^2} > q_{1-\alpha,n-1}^{\chi^2}\big]
%= & \PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma^2} > \alert{\tfrac{\sigma_0^2}{\sigma^2} } q_{1-\alpha,n-1}^{\chi^2}\big] \\
%\leq &  \PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma^2} > q_{1-\alpha,n-1}^{\chi^2}\big] \\
%= & \alpha.
%\end{align*}
%\item La même statistique de test convient pour contrôler l'erreur de première espèce que pour l'hypohèse nulle simple. On choisit \alert{ ici} la \alert{même} règle de décision.
%\end{itemize}
%\end{frame}
%
%\subsection{Principe de construction}
%
%\begin{frame}
%\frametitle{Conclusion provisoire}
%\begin{itemize}
%\item Pour contruire un test de l'hypothèse $H_0:\truetheta \in \Theta_0$ contre $H_1:\truetheta \in \Theta_1$, on cherche \alert{une statistique libre} sous l'hypothèse et on rejette pour un seuil qui dépend de la loi de la statistique sous $H_0$, de sorte de fournir une zone de rejet \alert{ maximale}.
%%étudie séparément l'erreur de seconde espèce
% \item Le plus souvent, la statistique est obtenue via un estimateur. Sauf exception (comme la cas gaussien) une telle statistique est difficile à trouver en général.
% \item \alert{Simplification} cadre asymptotique (où la gaussianité réapparaît le plus souvent...).
%\end{itemize}
%
%\end{frame}

\section{Tests asymptotiques}
\subsection{Elements de la théorie asymptotique des tests}
\begin{frame}
\frametitle{Quelques définitions}
\begin{itemize}
\item Soit $(\PP_\truetheta, \truetheta \in \Theta)$ une famille de probabilités sur $(\Xset,\Xsigma)$ admettant des densités $\{ f(\truetheta,x), \truetheta \in \Theta \}$ par rapport à une mesure de domination $\mu$.
\item Supposons que nous disposions d'un $n$-échantillon $(X_1,X_2,\dots, X_n)$ de ce modèle statistique.
\item Considérons le problème de tester l'hypothèse de base $H_0: \theta \in \Theta_0$ contre l'alternative $H_1: \theta \in \Theta_1$, où $\Theta_0 \cap \Theta_1= \emptyset$ et $\Theta_0 \cup \Theta_1= \Theta$.
\item Un \alert{test} pour un échantillon de taille $n$ est une  fonction mesurable
\[
\varphi_n : \Xset^n \to \ccint{0,1} \eqsp.
\]
\item Si le test est \alert{non randomisé} $\varphi_n \in \{0,1\}$ et l'ensemble
\[
\{ (x_1,\dots, x_n) \in \Xset^n, \varphi_n(x_1,\dots,x_n)= 1\}
\]
est appelée la \alert{région critique du test}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tests asymptotiques}
\begin{itemize}
\item  On dit qu'une suite de  tests $\{\varphi_n, n \in \nset\}$ est \alert{asymptotiquement de niveau $\alpha$} pour $\alpha \in \ccint{0,1}$ si
\alert{
\[
\lim_{n \to \infty} \PE^n_\truetheta[ \varphi_n(X_1,\dots,X_n)] \leq \alpha \eqsp, \text{pour tout $\truetheta \in \Theta_0$}
\]
}
\item La puissance de ce test est la fonction
\[
\truetheta \mapsto \puissance_n(\truetheta)= \PE^n_{\truetheta}[ \varphi_n(X_1,\dots,X_n)]
\]
\item On dit qu'une suite de tests $\{\varphi_n, n \in \nset\}$ est asymptotiquement \alert{consistante} si, pour tout $\theta \in \Theta_1$,
\alert{
\[
\lim_{n \to \infty} \puissance_n(\truetheta) = 1 \eqsp.
\]
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle régulier}
\begin{df} La famille de densités $\{f(\truetheta,\cdot),\truetheta \in \Theta\}$,  par rapport à la mesure dominante $\mu$, $\Theta \subset \rset^d$, est \alert{régulière} si
\begin{itemize}
\item $\Theta$ ouvert et $\{f(\truetheta, \cdot)>0\}=\{f(\truetheta', \cdot)>0\}$, $\forall \truetheta, \truetheta' \in \Theta$.
\item $\mu$-p.p. $\truetheta \leadsto f(\truetheta,\cdot)$ est ${\mathcal C}^2$.
 \item Pour tout $\truetheta \in \Theta$, il existe un voisinage ${\mathcal V}_\truetheta \subset \Theta$ t.q. pour $\tilde{\truetheta} \in {\mathcal V}_\truetheta$
$$|\nabla_\truetheta^{2}\log f(\tilde{\truetheta},x)|+|\nabla_\truetheta \log f(\tilde{\truetheta},x)|+\big(\nabla_\truetheta\log f(\tilde{\truetheta},x)\big)^2\leq g(x)$$
où
$$\int_{\Xset}g(x)\sup_{\tilde{\truetheta} \in {\mathcal V}(\truetheta)}f(\tilde{\truetheta},x)\mu(dx)<+\infty.$$
\item L'information de Fisher est non-dégénérée : pour tout $\truetheta \in \Theta$, la matrice d'information de Fisher 
$\mathbb{I}(\truetheta)$ est définie positive.
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Consistance du test de Neyman-Pearson}
\begin{itemize}
\item Supposons que $\Theta= \{\theta_0,\theta_1\}$ avec $\theta_0 \ne \theta_1$ et que l'on cherche à tester $H_0: \theta = \theta_0$ contre $H_1: \theta= \theta_1$.
\item Le lemme de Neyman-Pearson montre que le test qui rejette $H_0$ si
\[
\Lambda_n(\truetheta_0,\truetheta_1) = \frac{\prod_{i=1}^n f(\truetheta_1,X_i)}{\prod_{i=1}^{n} f(\truetheta_0,X_i)} > c_{n,\alpha}
\]
est U.P.P.
\item De façon équivalente, en prenant le logarithme de chaque membre de l'identité, le test de N.P. rejette $H_0$ si
\[
\log \Lambda_n(\truetheta_0,\truetheta_1)  = \sum_{i=1}^n \left\{ \log \frac{f(\truetheta_1;X_i)}{f(\truetheta_0;X_i)} \right\} \geq k_{n,\alpha}
\]
où $\ell(x;\truetheta)= \log f(\truetheta,x)$ et $k_{n,\alpha}$ est choisi de telle sorte que
\[
\PP_{\truetheta_0}^n [ \log \Lambda_n(\truetheta_0,\truetheta_1)  \geq  k_{n,\alpha}] = \alpha
\]
(on suppose qu'une telle valeur existe, autrement il faudrait utiliser un test randomisé)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul asymptotique du seuil critique}
\begin{itemize}
\item En pratique, il est souvent difficile de déterminer exactement le seuil critique $k_{n,\alpha}$... mais il est souvent facile de déterminer
une suite $\{k_{n,\alpha}, n \in \nset\}$ telle que
\[
\lim_{n \to \infty} \PP^n_{\theta_0} ( \log \Lambda_n(\truetheta_0,\truetheta_1) \geq  k_{n,\alpha}) = \alpha.
\]
\item En effet, le théorème central limite montre que, sous $H_0$,
\[
n^{-1/2} \sum_{k=1}^n \left\{ \log \frac{f(\theta_1,X_k)}{f(\theta_0,X_k)}  + \KL{\truetheta_0}{\truetheta_1} \right\} \dlim_{\PP^n_{\truetheta_0}} \gauss(0,J(\truetheta_0,\truetheta_1))
\]
où $\KL{\truetheta_0}{\truetheta_1}$ est la \alert{divergence de Kullback-Leibler} définie par
\[
\KL{\truetheta_0}{\truetheta_1} = \PE_{\truetheta_0}\left[ \log \frac{ f(\theta_0,X_1)}{f(\theta_1,X_1)} \right] > 0
\]
et
\[
J(\truetheta_0,\truetheta_1)= \PVar_{\truetheta_0}\left( \log \frac{ f(\theta_0,X_1)}{f(\theta_1,X_1)} \right) \eqsp.
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul asymptotique du seuil critique}
\begin{itemize}
\item Pour $\alpha \in \ooint{0,1}$, on note $z_{1-\alpha}$ le quantile $1-\alpha$ de la loi gaussienne standardisée.
\item  Nous avons donc:
\[
\lim_{n \to \infty} \PP_{\truetheta_0}^n \left( \frac{1}{\sqrt{n J(\truetheta_0,\truetheta_1)}} \{ \log \Lambda_n(\truetheta_0,\truetheta_1) +
  n \KL{\truetheta_0}{\truetheta_1} \} \geq z_{1-\alpha} \right) = \alpha
\]
ce qui implique, en posant
\[
k_{n,\alpha}= -n \KL{\truetheta_0}{\truetheta_1} + n^{1/2} z_{1-\alpha} \sqrt{J(\truetheta_0,\truetheta_1)}
\]
que le test de région critique $\{ \log \Lambda_n(\truetheta_0,\truetheta_1) \geq
k_{n,\alpha} \}$ est asymptotiquement de niveau $\alpha$,
\[
\lim_{n \to \infty} \PP^n_{\truetheta_0} [ \log \Lambda_n(\truetheta_0,\truetheta_1)
\geq k_{n,\alpha} ] = \alpha \eqsp.
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distribution du test sous l'hypothèse alternative}
\begin{itemize}
\item Sous $\PP^n_{\theta_1}$, nous avons
\begin{multline*}
\PP_{\theta_1}^n \left( \sum_{k=1}^n \left\{ \log \frac{f(\theta_1,X_k)}{f(\theta_0,X_k)}  \right\} \geq -n \KL{\theta_0}{\theta_1} + \sqrt{n} J^{1/2}(\theta_0,\theta_1) z_{1-\alpha} \right) \\
=
\PP_{\theta_1}^n \left( \frac{1}{n} \sum_{k=1}^n \left\{ \log \frac{f(\theta_1,X_k)}{f(\theta_0,X_k)} - \KL{\theta_1}{\theta_0} \right\} \geq - I(\theta_0,\theta_1) + \frac{\sqrt{J(\theta_0,\theta_1)}}{\sqrt{n}} z_{1-\alpha} \right) \eqsp,
\end{multline*}
où
\[
I(\truetheta_0,\truetheta_1)= \KL{\truetheta_0}{\truetheta_1} + \KL{\truetheta_1}{\truetheta_0}  \eqsp.
\]
\pause \item Par conséquent, \item \alert{Conclusion} Si $\KL{\truetheta_0}{\truetheta_1} \ne 0$ alors
\[
\lim_{n \to \infty} \puissance_n(\truetheta_1)= 1 \eqsp.
\]
Si le modèle est identifiable, alors il existe un test de niveau asymptotique $\alpha$ consistant (la puissance du test tend  vers 1).
\end{itemize}
\end{frame}



\subsection{Efficacité asymptotique relative}
\begin{frame}
\frametitle{Efficacité asymptotique... à travers un exemple}
\begin{itemize}
\item Supposons que $(X_1,\dots,X_n)$ est un $n$-échantillon indépendant de densité $f(\theta,x)= f(x-\truetheta)$ par rapport à la mesure de Lebesgue sur $\rset$.
\item \alert{Hypothèses}
\begin{itemize}
\item Variance finie: $\sigma^2_f \int |x|^2 f(x) \rmd x < \infty$
\item Parité: $f$ est une fonction paire (donc $\theta$ est la moyenne et la
  médiane de la loi)
\item $f$ est continue et $f(0) > 0$ : unicité de la médiane
\end{itemize}
\item On note $F$ la cdf associée à la densité $f$
\item On cherche à tester $\theta= 0$ contre $H_1: \theta > 0$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Un exemple}
\begin{itemize}
\item On considère deux statistiques:
\begin{align*}
U_n&= \frac{1}{n} \sum_{i=1}^n \indi{X_i > 0} &&\text{test du signe} \\
\bar{X}_n&= n^{-1} \sum_{i=1}^n X_i     &&\text{z-test}
\end{align*}
\item  Sous $H_0$, $\PP_0( X_1 > 0)=1/2$, et donc $U_n \plim[\PP_{0}^n] 1/2$. Si $\theta > 0$, $\PP_\theta(X_1 > 0) >  1 - F(-\theta)$ et 
ce qui suggère de considérer un test $\{ U_n -1/2 > c_{n,\alpha} \}$ 
\item Sous $H_0$,  $\PE_0[X_1]= 0$ et donc $\bar{X}_n \plim[\PP_{0}^n] 1/2$. Si $\theta > 0$, $\PE_\theta[X_1]= \theta$ ce qui suggère de
considérer un test $\{ \bar{X}_n > d_{n,\alpha} \}$
\item \alert{Questions:} 
\begin{enumerate}
\item Comment calibrer  les constantes $c_{n,\alpha}$  et $d_{n,\alpha}$ ? facile !
\item quel test dois je choisir ? En quoi mon choix est-il dépendant de la distribution des observations ?
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Asymptotique du test du signe}
$$
U_n= n^{-1} \sum_{i=1}^n \indi{X_i > 0}
$$
\begin{itemize}
\item Par le théorème de la limite centrale
$$
n^{1/2} \sigma^{-1}(\theta) (U_n - \mu(\theta)) \dlim_{\PP^n_\theta} \gauss(0,1)
$$
où
\[
\mu(\theta) = 1 - F(-\theta) \quad \qquad \sigma^2(\theta) = (1-F(-\theta)) F(-\theta) \eqsp.
\]
\item Par conséquent, sous $H_0: \theta = 0$, $\mu(0)=1/2$ et $\sigma^2(0)= 1/4$ ce qui implique:
\[
2 \sqrt{n} (U_n - 1/2) \dlim_{\PP^n_0} \gauss(0,1) \eqsp.
\]
\item Le test de région critique  $\{ 2 \sqrt{n} (U_n -1/2) > z_{ 1-\alpha} \}$ est un test de niveau asymptotique $\alpha$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Puissance du test de signe}
La puissance du test de signe de niveau asymptotique $\alpha$ est donnée pour tout $\theta > 0$, par
\begin{align*}
\puissance^{\mathrm{sign}}_{n,\alpha}(\theta)
&= \PP_\theta( \sqrt{n}(U_n - 1/2) > 1/2 z_{1-\alpha}) \\
&= \PP_\theta\left( U_n - \mu(\theta) > \{1/2-\mu(\theta)\} + \frac{1}{2 \sqrt{n}} z_{1-\alpha}  \right) \eqsp. 
\end{align*}

Puisque $1/2 < \mu(\truetheta)= 1- F(-\theta)$ pour $\truetheta > 0$, la loi des grands nombres montre que 
le test est \alert{consistant}: pour tout $\theta > 0$, \alert{
\[
\lim_{n \to \infty}  \puissance^{\mathrm{sign}}_{n,\alpha}(\theta) = 1 \eqsp.
\]
}
\end{frame}



\begin{frame}
\frametitle{Asymptotique du $z$-test}
\begin{itemize}
\item Théorème Central limite: $n^{-1/2} \sum_{i=1}^n (X_i - \theta) \dlim_{\PP_\theta} \gauss(0,\sigma^2_f)$.
\item Le test de région critique $\{ \bar{X}_n > n^{-1/2} \sigma_f z_{1-\alpha} \}$ est un test de niveau asymptotique $\alpha$:
\begin{align*}
\lim_{n \to \infty} \PP_0 \left( \bar{X}_n > n^{-1/2} \sigma_f z_{1-\alpha} \right)
&= \lim_{n \to \infty} \PP_0 \left( n^{1/2} \bar{X}_n / \sigma_f >  z_{1-\alpha} \right) \\
&= 1 - \Phi(z_{1-\alpha})= \alpha \eqsp.
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Puissance du $z$-test}
La puissance du test de niveau $\alpha$ est donnée par
\[
\puissance^{\mathrm{t-test}}_{n,\alpha}(\theta)= \PP^n_{\truetheta}(\sqrt{n} \bar{X}_n > \sigma_f z_{1-\alpha})
\]
Comme, par la loi des grands nombres
\begin{align*}
\frac{1}{n} \sum_{i=1}^n (X_i - \theta) &\plim[\PP^n_{\theta}] 0 \\
n^{-1/2} \sigma_f z_{1-\alpha} -   \truetheta &\plim[\PP^n_{\theta}] -\truetheta
\end{align*}
le $z$-test de niveau asymptotique $\alpha$ est \alert{consistant}: pour tout $\theta > 0$, \alert{
\[
\lim_{n \to \infty} \puissance^{\mathrm{z-test}}_{n,\alpha}(\theta) = 1 \eqsp.
\]
}
\end{frame}

\begin{frame}
\frametitle{Comparaison asymptotique des puissances}
\begin{itemize}
\item A l'instar de l'estimation ponctuelle, la simple notion de \alert{consistance} ne nous donne pas une méthode permettant de \alert{comparer} des tests: tous les tests \alert{raisonnables} sont consistants, comme tous les estimateurs \alert{raisonnables}  des paramètres.
\item Pour pouvoir comparer asymptotiquement les tests, il faut introduire une élementaire que la consistance.. 
\item Deux approches possibles:
\item \alert{Bahadur}: mesurer la vitesse à laquelle la vitesse du test tend vers $1$ (théorie des \alert{grandes déviations})
\item \alert{Pitman}:   
\begin{itemize}
\item rendre la discrimination entre l'hypothèse nulle $H_0$ et l'hypothèse alternative $H_1$ plus \alert{difficile} quand $n \to \infty$.
\item \alert{considérer un test $H_0: \theta = 0$ contre \alert{une suite d'hypothèses alternatives} $H^n_1: \theta = \theta_n$ avec $\theta_n > 0$ et $\lim_{n \to \infty} \theta_n = 0$.}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test du signe}
$$
\puissance^{\mathrm{sign}}_{n,\alpha}(\theta)=
\PP_\theta( \sqrt{n} \sigma^{-1}(\theta) (U_n - \mu(\theta)) > \sigma^{-1}(\theta) \{ \sigma(0) z_{1-\alpha} + \sqrt{n} \{\mu(0)-\mu(\theta)\}\})
$$
\begin{itemize}
\item  Nous cherchons à déterminer la puissance du test contre la suite de contre-alternatives $H_1^n: \theta_n > 0$, où $\theta_n \to 0$.
\item La puissance dépend de $\sqrt{n} (\mu(0) - \mu(\theta_n))$ où
\[
\mu(\theta) = 1 - F(-\theta) \eqsp.
\]
\item Comme $F$ est différentiable en $\theta = 0$, nous avons
\[
\sqrt{n}(\mu(\theta_n) - \mu(0))= - \sqrt{n} ( F(-\theta_n) - F(0)) =  \sqrt{n} \theta_n f(0) + o(\sqrt{n} \theta_n) \eqsp,
\]
car
\[
F(-\theta) = F(0) - \theta f(0) + o(\theta)
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test du signe}
\begin{itemize}
\item Si $\sqrt{n} \theta_n \to 0$, alors $\sqrt{n}(\mu(0)- \mu(\theta_n)) \to
  0$; donc, $\puissance_{n,\alpha}^{\mathrm{sign}}(\theta_n) \to \alpha$, on ne
  distingue pas l'hypothèse de base et l'alternative.
\item Si $\sqrt{n} \theta_n \to \infty$, alors $\sqrt{n}(\mu(0)- \mu(\theta_n)) \to -\infty$: $\puissance_{n,\alpha}^{\mathrm{sign}}(\theta_n) \to 1$, problème \alert{trop facile}, la puissance tend vers $1$.
\item Cas intéressant !
\alert{
\[
\lim_{n \to \infty} \sqrt{n} \theta_n= h > 0
\]
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficacité asymptotique des tests}
\begin{itemize}
\item Ceci conduit à une approche naturelle de comparaison des tests, qui
  consiste à comparer la \alert{puissance locale des tests} \alert{
\[
\puissance(h)= \lim_{n \to \infty} \puissance_n( h / \sqrt{n}) \eqsp.
\]
}
\item Pour les modèles réguliers, cette fonction de \alert{puissance locale asymptotique} est bien définie (preuve délicate, basée sur la notion de \alert{contiguité})
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficacité asymptotique locale des tests}
\begin{theo}
Soit $g$ une fonction telle $\PE_\theta[g^2(X_1)] < \infty$ pour tout $\theta \in \Theta$. On note $\PE_\theta[g(X_1)]= \mu(\theta)$
et $\PVar_\theta[g(X_1)]= \sigma^2(\theta)$. 
\begin{enumerate}
\item $\mu$ est différentiable en $0$
\item $\sigma$ continue en $0$.
\end{enumerate}
On pose $G_n= n^{-1/2} \sum_{k=1}^n \{ g(X_k) - \mu(0) \}$. 
\begin{enumerate} 
\item La suite de test $\{ G_n > \sigma(0) z_{1-\alpha}\}$  est de niveau asymptotique $\alpha$: $\lim_{n \to \infty} \PP_{0}( G_n > \sigma(0) z_{1-\alpha})= \alpha$.
\item La \alert{puissance locale asymptotique} de cette suite de tests est donnée par
\[
\puissance(h)= \lim_{n \to \infty} \puissance_n( h/\sqrt{n})= \Phi(h \mu'(0)/\sigma(0) - z_{1-\alpha}) \eqsp.
\]
\end{enumerate}
\end{theo}
\end{frame}

\begin{frame}
\frametitle{Elements de preuve}
\begin{itemize}
\item On pose 
\begin{align*}
G_n       &= n^{-1/2} \sum_{k=1}^n \{g(X_k) - \mu(0) \} \\
\Lambda_n &= \Lambda_n(X_1,\dots,X_n) = \prod_{k=1}^n \frac{f(hn^{-1/2},X_k)}{f(0,X_k)}
\end{align*}
\item On cherche à déterminer la loi de $G_n$ sous la suite de lois $\PP_{\theta_n}^n$.  On va chercher à calculer
\tco{$\lim_{n \to \infty} \PE_{\theta_n}\left[ \rme^{\rmi u G_n} \right]$}
\item \alert{Remarque fondamentale:} \tco{$\PE_{\theta_n}[ \rme^{\rmi u G_n}] = \PE_{0}[\Lambda_n \rme^{\rmi u G_n}]$}
\begin{align*}
&\PE_{\theta_n}[ \rme^{\rmi u G_n}] 
= \idotsint \rme^{\rmi u n^{-1/2} \sum_{k=1}^n \{g(x_k) - \mu(0)\}} \prod_{k=1}^n f(\theta_n,x_k) \rmd x_1 \dots \rmd x_n \\
&= \idotsint \rme^{\rmi u n^{-1/2} \sum_{k=1}^n \{g(x_k) - \mu(0)\}} \Lambda_n(x_1,\dots,x_n) \prod_{k=1}^n f(0,x_k) \rmd x_1 \dots \rmd x_n
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Elements de preuve 2}
\begin{itemize}
\item On est ramené à étudier, sous $\PP_{\theta_0}^n$ la limite du vecteur aléatoire
\[
\left[\begin{array}{c}
  \log \Lambda_n \\
  G_n 
\end{array}
\right]
=
\left[\begin{array}{c}
   \sum_{k=1}^n \log f(hn^{-1/2},X_k) - \log f(0,X_k)  \\
   n^{-1/2} \sum_{k=1}^n \{ g(X_k) - \mu(0)\} 
 \end{array}
\right]
\]
\item Pour un modèle régulier, en posant $\ell(\theta,x)= \log f(\theta,x)$ (avec la convention $\ell(x)= \ell(0,x)$),
nous avons 
\[
\ell(hn^{-1/2},x)= \ell(x) + h n^{-1/2} \dot\ell(x) + (h^2 n^{-1})/2 \ddot \ell(x) + o(n^{-1})
\]
ce qui implique
\begin{align*}
 \sum_{k=1}^n \ell (hn^{-1/2},X_k) - \ell(X_k) 
 &= \frac{h}{\sqrt{n}} \sum_{k=1}^n \dot\ell(X_k) + \frac{h^2}{2n} \sum_{k=1}^n \ddot \ell(X_k) + o_{\PP_0^n}(1) \\
 &= \frac{h}{\sqrt{n}} \sum_{k=1}^n \dot\ell(X_k) - \frac{h^2}{2} I(0)  + o_{\PP_0^n}(1)
\end{align*}
où $I(0)$ est l'information de Fisher.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Elements de preuve 3}
\begin{itemize}
\item Par conséquent, nous avons
\[
\left[\begin{array}{c}
  \log \Lambda_n \\
  G_n
\end{array}
\right]
=
n^{-1/2}  \sum_{k=1}^n \left[ \begin{array}{c} \dot \ell(X_k) \\ g(X_k) - \mu(0) \end{array} \right]
- 
\left[
\begin{array}{c}
(h^2/2) I(0) \\
0
\end{array}
\right]
+
o_{\PP_{0}^n}(1) \eqsp.
\]
\item En utilisant un TCL, nous avons donc
\begin{align*}
&\left[\begin{array}{c}
  \log \Lambda_n \\
  G_n
\end{array}
\right]
\dlim{\PP_{0}^n} 
\left[\begin{array}{c}
  L \\
  G
\end{array}
\right]
\\
&\sim
\gauss\left( \left[ \begin{array}{c} - (h^2/2) I(0) \\ 0 \end{array}\right], \left[ 
\begin{array}{cc}
\PE_0[ (\dot \ell(X_1))^2] & \PE_0[ g(X_1)  \dot \ell(X_1)] \\
\PE_0[ g(X_1)  \dot \ell(X_1)] & \Var_0(g(X_1)) 
\end{array}
\right] \right)
\end{align*}
\item  Nous avons:
\begin{enumerate}
\item $\PE_0[ (\dot \ell(X_1))^2]= I(0)$
\item $\PE_0[ (g(X_1)- \mu(0))^2]= \sigma(0)^2$.
\item $\PE_0[ (g(X_1) - \mu) \dot \ell(X_1)]= \PE_0[ g(X_1) \dot\ell(X_1)]= \int g(x) \dot f(x-\theta) \rmd x \vert_{\theta=0}= -\dot\mu(0)$
\end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preuve: on rassemble les bouts}
\begin{itemize}
\item En utilisant $\PE_{\theta_n}[ \rme^{\rmi u G_n}] = \PE_0[ \rme^{\log(\Lambda_n)} \rme^{\rmi u G_n}]$ et 
la convergence en loi de $[\log(\Lambda_n), G_n]$ implique (il y a encore des petits détails à régler)
\[
\lim_{n \to \infty} \PE_{\theta_n}[ \rme^{\rmi u G_n}]= \PE[ \rme^{L} \rme^{\rmi u G}]
\] 
où $[L,G]^T$ est un vecteur gaussien de moyenne $[-h^2 I(0)/2,0]^T$ et de covariance
\[
\left[
\begin{array}{cc}
I(0) & -\dot \mu(0) \\
-\dot \mu(0) & \sigma^2(0)
\end{array}
\right] 
\] 
\item Conclusion par un calcul élémentaire...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item Nous disposons maintenant d'une méthode simple pour comparer les tests de l'hypothèse $H_0 \theta = 0$;, contre $H_1: \theta > 0$, en nous basant sur la puissance asympotique locale...
\item Pour les tests asymptotiquement normaux, il suffit de comparer la \alert{pente} des tests, à savoir \alert{$\mu'(0)/\sigma(0)$}.
\item Plus la pente est grande, plus la puissance asymptotique locale $\pi(h)$ augmente rapidement avec $h$ !
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Application}
\begin{itemize}
\item Test du signe
\begin{itemize}
\item Statistique $U_n= n^{-1} \sum_{i=1}^n \indi{X_i > 0}$.
\item $\mu(\theta)= 1 - F(-\theta)$, $\dot \mu(\theta)=  f(\theta)$.
\item $\sigma^2(\theta)= (1 - F(-\theta)) F(-\theta)$
\item \alert{Pente:} $\dot\mu(0)/\sigma(0)= 2 f(0)$.
\end{itemize}
\item $z$-test
\begin{itemize}
\item Statistique $\bar{X}_n$.
\item $\mu(\theta)= \theta$ and $\sigma(\theta)= \sigma_f$ avec $\sigma^2_f = \int x^2 f(x) \rmd x$.
\item \alert{Pente:} $\dot\mu(0)/\sigma(0)= 1/\sigma_f$.
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Efficacité relative}
\begin{enumerate}
\item test du signe: $\mu'(0)/\sigma(0)=2 f(0)$,
\item $t$-test: $\mu'(0)/\sigma(0)=1/\sigma_f$.
\end{enumerate}

\begin{itemize}
\item Laplace: $2 f(0) s = \sqrt{2}=1.414$.
\item Logistique: $2 f(0) s = \pi/(2\sqrt{3})=0.907$.
\item Gauss: $2 f(0) s = \sqrt{2/\pi}=0.798$.
\item Uniforme: $2 f(0) s = 1/\sqrt{3}=0.577$.
\end{itemize}
\end{frame}

\section{Quelques tests asymptotiques}

\subsection{Test du rapport de vraisemblance}
\begin{frame}
\frametitle{Test du rapport de vraisemblance}
 \begin{itemize}
 \item Soit  $X^{(n)}= (X_{1},\ ,\ X_{n})$ un $n$-échantillon du modèle statistique $\PP_{\theta}^{n}\ll\mu_{n},\ \theta\in\Theta$, de densité  $f(\theta,x^{(n)})= \rmd \PP_{\theta}^{n}/ \rmd \mu_{n}$.
 \item Pour tester $H_{0}$ : $\theta\in\Theta_{0}$  contre $H_{1}$ : $\theta\in\Theta-\Theta_{0}$, le test du \alert{rapport de vraisemblance} rejette $H_{0}$ lorsque la valeur du \alert{rapport de vraisemblance généralisé}
$$
\Lambda_{n}=\frac{\sup_{\theta\in\Theta_{0}}f(\theta,X^{(n)})}{\sup_{\theta\in\Theta}f(\theta,X^{(n)})}
$$
est inférieure à un seuil.
\item   Lorsque les hypothèses $H_{0},\ H_{1}$ sont simples, ce test est U.P.P. .
\item Pour des hypothèses composites, il n'y a en général aucun résultat d'optimalité, sauf dans des cas simples...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple 1: t-test}
\begin{itemize}
\item  Soient $X^{(n)}= (X_{1}, \dots, X_{n})$ un $n$-échantillon de $\gauss(\mu,\ \sigma^{2})$.
\item On teste l'hypothèse $H_0: \mu =0$ contre $H_1: \mu \ne 0$.
\item En posant $\theta=(\mu,\ \sigma^{2})$,
\begin{align*}
\Lambda_{n}&=\frac{\sup_{\theta\in\Theta_{0}}(1/\sigma^{n})\exp(-\frac{1}{2\sigma^{2}}\sum_{i}(X_{i}-\mu)^{2})}{\sup_{\theta\in\Theta}(1/\sigma^{n})\exp(-\frac{1}{2\sigma^{2}}\sum_{i}(X_{i}-\mu)^{2})}\\
&=\left( \frac{\sum_{i}(X_{i}-\overline{X}_{n})^{2}}{\sum_{i}X_{i}^{2}} \right)^{n/2}
\end{align*}
\item Un calcul élémentaire montre que $\Lambda_{n}<c$ est équivalent à $t_{n}^{2}>k$ où
$$
t_{n}=\frac{\sqrt{n}\overline{X}_{n}}{\sqrt{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}}}
$$
est la $\mathrm{t}$-statistique. En d'autres termes, le $\mathrm{t}$-test est un test de rapport de vraisemblance généralisé.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Justification}
\begin{align*}
t_{n}^{2} &=\frac{n\overline{X}_{n}^{2}}{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}} \\
&=\frac{\sum_{i}X_{i}^{2}-\sum_{i}(X_{i}-\overline{X}_{n})^{2}}{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}} \\
&=\frac{(n-1)\sum_{i}X_{i}^{2}}{\sum_{i}(X_{i}-\overline{X}_{n})^{2}}-(n\ -1) \\
&=(n-1)\Lambda_{n}^{-2/n}-(n-1)
\end{align*}
ce qui montre que
$$
\Lambda_{n}=(\frac{n-1}{t_{n}^{2}+n-1})^{n/2}
$$
\end{frame}

\begin{frame}
\frametitle{Distribution asymptotique}
Comme
$$
\Lambda_{n}=(\frac{n-1}{t_{n}^{2}+n-1})^{n/2}
$$
nous avons
\begin{align*}
&\log\Lambda_{n}=\frac{n}{2}\log\frac{n-1}{t_{n}^{2}+n-1} \\
&\Rightarrow \ -2\log\Lambda_{n}=n\log(1+\frac{t_{n}^{2}}{n-1}) \\
&=n \left(\frac{t_{n}^{2}}{n-1}+o_{p}(\frac{t_{n}^{2}}{n-1}) \right) \dlim_{\PP_0^n} \chi_{1}^{2}
\end{align*}
car sous $H_{0}$,  $t_{n} \dlim_{\PP_0^n} \gauss(0,1)$.

\bigskip

\alert{résultat vrai en toute généralité !}
\end{frame}


\begin{frame}
\frametitle{Test d'égalité des proportions pour une variable multinomiale}
\begin{itemize}
\item Soit $(X_1,\dots,X_n)$ un $n$-échantillon d'une loi multinomiale à $d$-instances
\item \alert{Paramètre} $\mathbf{p}= (p_1,\dots,p_d) \in \mathcal{M}_d= \{ (p_1, \dots, p_d), p_i \geq 0, \sum_{i=1}^d p_i =1 \}$.
\item \alert{Rapport de vraisemblance généralisé}
\begin{align*}
\Lambda_n
&= \frac{(1/d)^n}{\max_{(p_1,\dots,p_d) \in \mathcal{M}_d} \prod_{i=1}^d p_i^{N_i} } \\
&= \prod_{i=1}^d \left( \frac{n}{d N_i} \right)^{N_i} = \prod_{i=1}^d (d \hat{p}_{n,i})^{-N_i}
\end{align*}
où $N_i= \sum_{j=1}^n \indi{X_j=i}$  et $\hat{p}_{n,i}= N_i/n$ les fréquences empiriques.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des fréquences empiriques}
\begin{itemize}
\item On suppose $(X_1,\dots,X_n)$ $n$-échantillon multinomial de proportion $(q_1,\dots,q_d)$.
\item \alert{Comparaison des fréquences empiriques}
$$\widehat p_{n,\ell}=\frac{1}{n}\sum_{i=1}^n \indi{X_i=\ell}\;\;\;\text{\alert{proche de}}\;\;q_\ell,\;\;\ell=1,\ldots, d\; \alert{?}$$
\item Loi des grands nombres :
$$\big(\widehat p_{n,1},\ldots, \widehat p_{n,d}\big) \stackrel{\PP_{{\boldsymbol p}}}{\longrightarrow} (p_1,\ldots, p_d)={\boldsymbol p}.$$
\item \alert{Théorème central-limite ?}
$${\boldsymbol{U}_n}(\boldsymbol{p})=\sqrt{n}\Big(\frac{\widehat p_{n,1}-p_1}{\sqrt{p_1}},\ldots, \frac{\widehat p_{n,d}-p_d}{\sqrt{p_d}}\Big) \stackrel{d}{\longrightarrow} ?$$
\item Composante par composante oui. \alert{Convergence globale plus délicate}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Statistique du Chi-deux}
\begin{prop}
Si les composantes de $\boldsymbol{p}$ sont toutes non-nulles
\begin{itemize}
\item On a la \alert{convergence en loi} sous $\PP_{\boldsymbol{p}}$
$${\boldsymbol{U}_n}(\boldsymbol{p})\stackrel{d}{\longrightarrow} {\mathcal N}\big(0,V(\boldsymbol{p})\big)$$
avec $V(\boldsymbol{p}) = \mathrm{Id}_d-\sqrt{\boldsymbol{p}}\big(\sqrt{\boldsymbol{p}}\big)^T$ et $\sqrt{\boldsymbol{p}} = (\sqrt{p_1},\ldots, \sqrt{p_d})^T$.
\item \alert{De plus}
$$\|{\boldsymbol{U}_n}(\boldsymbol{p})\|^2 = n\sum_{\ell=1}^d \frac{(\widehat p_{n,\ell}-p_\ell)^2}{p_\ell} \stackrel{d}{\longrightarrow} \chi^2(\alert{d-1}).$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve de la normalité asymptotique}
\begin{itemize}
\item Pour $i=1,\ldots, n$ et $1 \leq \ell \leq d$, on pose
$$Y_\ell^i=\frac{1}{\sqrt{p_\ell}}\big(\indi{X_i=\ell}-p_\ell\big).$$
\item Les vecteurs ${\boldsymbol Y}_i=(Y_1^i,\ldots, Y_d^i)$ sont \alert{indépendants et identiquement distribués} et
$${\boldsymbol U}_n(\boldsymbol{p}) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n {\boldsymbol Y}_i,$$
$\E\big[Y_\ell^i\big]=0$, $\E\big[(Y_\ell^i)^2\big]=1-p_\ell$, $\E\big[Y_\ell^iY_{\ell'}^i \big]=-(p_\ell p_{\ell'})^{1/2}$.
\item \alert{On applique le TCL vectoriel}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence de la norme au carré}
\begin{itemize}
\item On a donc ${\boldsymbol U}_n(\boldsymbol{p}) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0,V(\boldsymbol{p})\big)$.
\item On a aussi
\begin{align*}
\|{\boldsymbol U}_n(\boldsymbol{p}) \|^2 & \stackrel{d}{\longrightarrow} \| {\mathcal N}\big(0,V(\boldsymbol{p})\big)\|^2 \\
& \sim \chi^2\big(\mathrm{Rang}\big(V(\boldsymbol{p})\big)\big)
\end{align*}
par \alert{Cochran} :  $V(\boldsymbol{p}) = \mathrm{Id}_d-\sqrt{\boldsymbol{p}}\big(\sqrt{\boldsymbol{p}}\big)^T$ est la projection orthogonale sur $\mathrm{vect}\{\sqrt{\boldsymbol{p}}\}^\perp$ qui est de dimension $d-1$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Distribution limite de $-2 \log \Lambda_n$}
Nous avons
\begin{align*}
-2 \log \Lambda_n &= 2\sum_{i=1}^d N_i \log( \hat{p}_{N,i}/p_i) \\
                  &= 2 n \sum_{i=1}^d  (\hat{p}_{N,i} - p_i + p_i) \log\left( 1 + \frac{\hat{p}_{n,i}-p_i}{p_i} \right) \\
                  &= 2 n \sum_{i=1}^d \frac{(\hat{p}_{n,i}-p_i)^2}{p_i} + 2 n \sum_{i=1}^d p_i \left\{ \frac{(\hat{p}_{n,i}-p_i)}{p_i} - \frac{1}{2} \left(\frac{\hat{p}_{n,i}-p_i}{p_i} \right)^2 \right\} + o_{\PP}(1) \eqsp.
\end{align*}
car $\sum_{i=1}^d p_i (\hat{p}_{n,i} - p_i)/p_i= 0$... Par conséquent
\alert{
\[
-2 \log \Lambda_n \dlim_{\PP_{\mathbb{p}}} \chi^2(d-1)
\]
}
Trop beau pour qu'il n'y ait pas quelque chose de plus profond.. en PC
\end{frame}

\subsection{Tests de Wald}
\begin{frame}
\frametitle{Le test de Wald : hypothèse nulle simple}
\begin{itemize}
\item \underline{Situation} la suite d'expériences $\big(\Xset^n, \Xsigma^{\otimes n},\{\PP_\truetheta^n,\truetheta \in \Theta\}\big)$ est engendrée par l'observation $Z^n= (X_1,\dots,X_n)$, $\truetheta \in \Theta \subset \R$
\item \alert{Objectif} : Tester
$$H_0:\truetheta = \truetheta_0\;\;\;\text{contre}\;\;H_1 \;\truetheta\neq \truetheta_0.$$
\item \alert{Hypothèse} : on dispose d'un estimateur $\est$ \alert{asymptotiquement normal}
$$\boxed{\sqrt{n}(\est-\truetheta)\stackrel{d}{\rightarrow}{\mathcal N}\big(0,v(\truetheta)\big)}$$
en loi sous $\PP_{\truetheta}^n$, $\forall \truetheta \in \Theta$, où $\truetheta \leadsto v(\truetheta) >0$ est continue.
\item On a \alert{la convergence}
$$\sqrt{n}\frac{\est-\truetheta_0}{\sqrt{v(\est)}}\stackrel{d}{\longrightarrow}_{\PP_{\truetheta_0}^n} {\mathcal N}(0,1).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald (cont.)}
\begin{itemize}
\item \underline{Remarque} $\sqrt{v(\est)} \leftrightarrow \sqrt{v(\truetheta_0)}$ ou d'autres choix encore...
\item On a aussi
$$T_n = n\frac{(\est-\truetheta_0)^2}{v(\est)} \stackrel{d}{\longrightarrow}_{\PP_{\truetheta_0}^n} \chi^2(1).$$
\item Soit $q_{1-\alpha,1}^{\chi^2} >0$ tel que si $U \sim \chi^2(1)$, on a $\PP\big[U > q_{1-\alpha,1}^{\chi^2}\big]=\alpha$. On \alert{choisit la zone de rejet}
$${\mathcal R}_{n,\alpha} = \big\{T_n\geq q_{1-\alpha,1}^{\chi^2}\big\}.$$
\item Le test de zone de rejet ${\mathcal R}_{n,\alpha}$ s'appelle \alert{Test de Wald de l'hypothèse simple $\truetheta=\truetheta_0$ contre l'alternative $\truetheta \neq \truetheta_0$ basé sur $\est$.}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propriétés du test de Wald}
\begin{prop}
Le test de Wald de l'hypothèse simple $\truetheta=\truetheta_0$ contre l'alternative $\truetheta \neq \truetheta_0$ basé sur $\est$ est
\begin{itemize}
\item \alert{asymptotiquement} de niveau $\alpha$ :
$$\PP_{\truetheta_0}^n\big[ {\mathcal R}_{n,\alpha}\big]\rightarrow \alpha.$$
\item \alert{convergent ou (consistant)}. Pour tout point $\truetheta \neq \truetheta_0$
$$\PP_\truetheta^n\big[ {\mathcal R}_{n,\alpha}^c\big] \rightarrow 0.$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve}
\begin{itemize}
\item Test asymptotiquement de niveau $\alpha$ \alert{par construction}.
\item \underline{Contrôle de l'erreur de seconde espèce :}
Soit $\truetheta \neq \truetheta_0$. On a
\begin{align*}
T_n & = \Big(\sqrt{n}\frac{\est-\truetheta}{\sqrt{v(\est)}}+\sqrt{n}\frac{\truetheta-\truetheta_0}{\sqrt{v(\est)}}\Big)^2 \\
& =: T_{n,1}+T_{n,2}.
\end{align*}
On a $T_{n,1} \stackrel{d}{\longrightarrow} {\mathcal N}(0,1)$ sous $\PP_{\truetheta}^n$ et
$$T_{n,2} \stackrel{\PP_{\truetheta}^n}{\longrightarrow} \pm \infty\;\;\alert{\text{car}\;\;\truetheta \neq \truetheta_0}$$
Donc $T_{n}\stackrel{\PP_{\truetheta}^n}{\longrightarrow}+\infty$, d'où le résultat.
\item \alert{Remarque} : si $\truetheta \neq \truetheta_0$ mais $|\truetheta - \truetheta_0| \lesssim 1/\sqrt{n}$, le raisonnement ne s'applique pas. Résultat \alert{non uniforme en le paramètre}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald : cas vectoriel}
\begin{itemize}
\item \alert{ Même contexte:} $\Theta \subset \R^d$ et \alert{on dispose} d'un estimateur $\est$ asymptotiquement normal :
$$\sqrt{n}\big(\est-\truetheta\big)\stackrel{d}{\longrightarrow} {\mathcal N}\big(0, V(\truetheta)\big)$$
où la matrice $V(\truetheta)$ est \alert{définie positive} et continue en $\truetheta$.
\item On cherche à tester $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_1$.
\item Sous $\PP_\truetheta$, la convergence $n^{1/2} (\est - \truetheta) \dlim \gauss(0,V(\truetheta))$ implique que
$$
V^{-1/2}(\truetheta) n^{1/2} (\est - \theta) \dlim \gauss(0,\Id{d})
$$
et donc que
$$
n (\est - \truetheta)^{T} V^{-1}(\truetheta) (\est - \truetheta) \dlim \chi^2(d)  \eqsp.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple: loi exponentielle}
\begin{itemize}
\item \alert{Hypothèse}: $\{X_i\}_{i=1}^n$, \iid\ de loi exponentielle de paramètre $\theta \in \Theta= \rset_+^*$.
\item \alert{log-vraisemblance}
\[
\ell_n(\truetheta)= n^{-1} \sum_{i=1}^n \log f(\truetheta,X_i)= \log(\truetheta) - \theta \bar{X}_n
\]
où $\bar{X}_n= n^{-1} \sum_{i=1}^n X_i$ est la moyenne empirique.
\item Estimateur du MV: $\hat{\theta}_n = \bar{X}_n^{-1}$.
\item \alert{Modèle régulier}
\[
\sqrt{n} (\est - \truetheta) \dlim_{\PP_{\truetheta}} \gauss(0,I^{-1}(\truetheta))
\]
où $I(\truetheta)= \theta^{-2}$ est l'\alert{information de Fisher}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple: test loi exponentielle}
\begin{itemize}
\item \alert{Test de Wald} de l'hypothèse $H_0: \truetheta = \truetheta_0$ contre l'hypothèse $H_1: \truetheta \ne \truetheta_0$.
\begin{align*}
n (\est - \truetheta_0)^2 I(\est)= n (1 - \truetheta_0 / \est)^2 \dlim_{\PP_{\truetheta_0}} \geq q^{\chi2}_{1-\alpha,1}
\end{align*}
\item \alert{Application numérique} $n=100$, $\truetheta_0=0.5$,
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Test de Wald: cas vectoriel}
\begin{itemize}
\item Le test de Wald de l'hypothèse $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_0$
rejette $H_0$ si
\[
n (\est - \truetheta_0)^T V^{-1}(\est) (\est - \truetheta_0) > q^{\chi^2}_{1-\alpha,d} \eqsp
\]
\item On peut remplacer la matrice de covariance $V(\est)$ par $V(\truetheta_0)$ ou tout estimateur consistant de $V(\truetheta_0)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald : hypothèse nulle composite}
\begin{itemize}
\item \alert{ Même contexte:} $\Theta \subset \R^d$ et \alert{on dispose} d'un
  estimateur $\est$ asymptotiquement normal : pour tout $\truetheta \in \Theta$,
$$\sqrt{n}\big(\est-\truetheta\big)\stackrel{d}{\longrightarrow}_{\PP_{\truetheta}^n} {\mathcal N}\big(0, V(\truetheta)\big)$$
où la matrice $V(\truetheta)$ est \alert{définie positive} et continue en $\truetheta$.
\item \alert{But:} Tester $H_0: \truetheta \in \Theta_0$ contre $H_1:\truetheta \notin \Theta_0$, où
$$\boxed{\Theta_0 = \big\{\truetheta \in \Theta,\;\;g(\truetheta) = 0\big\}}$$
et
$$g:\R^d \rightarrow \R^m$$
($m \leq d$) est régulière.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Test de Wald cont.}
\begin{itemize}
\item \alert{Hypothèse : } la différentielle (de matrice $J_g(\truetheta)$) de $g$ est de rang maximal $m$ en tout point de (l'intérieur) de $\Theta_0$.
\end{itemize}
\begin{prop}
  En tout point $\truetheta$ de l'intérieur de $\Theta_0$ on a, en loi sous
  $\PP_\truetheta^n$ (i.e. \alert{sous l'hypothèse}) :
\begin{itemize}
\item $$\sqrt{n}g(\est) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0, J_g(\truetheta)V(\truetheta)J_g(\truetheta)^T\big),$$
\item $$\alert{T_n=ng(\est)^T\Sigma_g(\est)^{-1}g(\est)} \stackrel{d}{\longrightarrow} \chi^2(m)$$
%en loi sous $\PP_\truetheta^n$,
où $\Sigma_g(\truetheta) =J_g(\truetheta) V(\truetheta) J_g(\truetheta)^T$.
\end{itemize}
\end{prop}
\begin{itemize}
\item Preuve : méthode  delta  multidimensionnelle.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald (fin)}
\begin{prop}
Sous les hypothèses précédentes, le test de zone de rejet
$${\mathcal R}_{n,\alpha}  = \big\{T_n \geq q_{1-\alpha, m}^{\chi^2}\big\}$$
avec $\PP\big[U > q_{1-\alpha, m}^{\chi^2}\big]=\alpha$ si $U \sim \chi^2(m)$ est
\begin{itemize}
\item \alert{Asymptotiquement de niveau $\alpha$} en tout point $\truetheta$ de (l'intérieur) de $\Theta_0$ :
$$\PP_\truetheta^n\big[ {\mathcal R}_{n,\alpha}\big]\rightarrow \alpha.$$
\item \alert{Convergent} : pour tout $\truetheta \notin \Theta_0$ on a
$$\PP_\truetheta^n\big[{\mathcal R}_{n,\alpha}^c\big]\rightarrow 0.$$
\end{itemize}
\end{prop}
\begin{itemize}
\item C'est la  même preuve qu'en dimension 1.
\end{itemize}
\end{frame}



\subsection{Test de Rao}


\begin{frame}
\frametitle{Test du score (Rao)}
\begin{itemize}
\item Soit $(X_1, \cdots, X_n)$ un $n$-échantillon \iid\ associé à un modèle statistique $(\PP_\truetheta, \truetheta \in \Theta \subseteq \rset^d)$  \alert{régulier}
\item Pour $\truetheta \in \Theta$, le \alert{score de Fisher} est donné par
\[
\eta_{\truetheta}(x)= \nabla_{\truetheta} \log f(\truetheta,x)
\]
\item \alert{Propriétés}
\begin{itemize}
\item Le score de Fisher est centré sous $\PP_{\truetheta}$,
$$
\PE_{\truetheta}[\eta_{\truetheta}(X)]= 0 \eqsp, \quad \truetheta \in \Theta \eqsp.
$$
\item La matrice de covariance du score de Fisher est égale à la \alert{matrice d'Information de Fisher}
$$
I(\truetheta)= \PE_{\truetheta} \left[ \eta_{\truetheta}(X) \eta_{\truetheta}(X)^T \right]
$$
\end{itemize}
\item \alert{Conclusion} Pour tout $\truetheta \in \Theta$,
$$
Z_n(\truetheta) = n^{-1/2} \sum_{i=1}^n \eta_\truetheta(X_i) \dlim_{\PP_{\truetheta}^n} \gauss(0, I(\truetheta)) \eqsp.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Rao}
\begin{itemize}
\item Pour tester $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_0$, nous considérons la statistique de test
\[
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0)
\]
\item Sous l'hypothèse nulle,
$$
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0) \dlim_{\PP_{\truetheta_0}^n} \chi^2(d)
$$
et donc le test de Rao de rejet
$$
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0) \geq q_{1-\alpha,d}^{\chi^2}
$$
est asymptotiquement de niveau $\alpha$.
\end{itemize}
\end{frame}

\section{Tests d'adéquation}

\begin{frame}
\frametitle{Tests d'adéquation}
\begin{itemize}
\item \underline{Situation} On observe (pour simplifier) un $n$-échantillon de loi $F$ inconnue
$$X_1,\ldots, X_n\sim_{\text{i.i.d.}}F$$
\item \alert{Objectif} Tester
$$H_0:F=F_0\;\;\text{contre $H_1$:}\;\;F\neq F_0$$
où
$F_0$ distribution donnée. Par exemple : $F_0$ \alert{gaussienne centrée réduite}.
\item Il est \alert{très facile de construire un test asymptotiquement de niveau $\alpha$.}
Il suffit de trouver une statistique $\phi(X_1,\ldots, X_n)$ de loi connue sous l'hypothèse de base.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test d'adéquation : situation}
\begin{itemize}
\item \alert{Exemples : sous l'hypothèse}
  \begin{align*}
    &  \phi_1(X_1\ldots, X_n) = \sqrt{n}\overline{X}_n \sim {\mathcal N}(0,1) \\
    & \phi_2(X_1,\ldots, X_n) = \sqrt{n}\frac{\overline{X}_n}{S_n} \sim \text{Student}(n-1) \qquad \text{avec} \ S_n = (n-1)^{-1} \sum_{k=1}^n (X_k - \bar X_n)^2 \\
    & \phi_3(X_1,\ldots, X_n) = (n-1)s_n^2 \sim \chi^2(n-1).
  \end{align*}
%% $$\phi_1(X_1\ldots, X_n) = \sqrt{n}\overline{X}_n \sim {\mathcal N}(0,1)$$
%% $$\phi_2(X_1,\ldots, X_n) = \sqrt{n}\frac{\overline{X}_n}{S_n} \sim \text{Student}(n-1) \qquad \text{avec} \ S_n = (n-1)^{-1} \sum_{k=1}^n (X_k \bar X_n)^2$$
%% $$\phi_3(X_1,\ldots, X_n) = (n-1)s_n^2 \sim \chi^2(n-1).$$
\item Le problème est que ces tests \alert{ont une faible puissance} : ils ne sont pas consistants.
\item Pas exemple, si $F\neq$ gaussienne mais $\int_{\R}xdF(x)=0$, $\int_{\R}x^2dF(x)=1$, alors
$$\PP_{F}\big[\phi_1(X_1,\ldots,X_n) \leq x \big] \rightarrow \int_{-\infty}^x e^{-u^2/2}\frac{du}{\sqrt{2\pi}},\;\;x \in \R.$$
(résultats analogues pour $\phi_2$ et $\phi_3$).
\item La statistique de test $\phi_i$ \alert{ne caractérise pas} la loi $F_0$.
\end{itemize}
\end{frame}


\subsection{Tests de Kolmogorov-Smirnov}

\begin{frame}
\frametitle{Test de Kolmogorov-Smirnov}
\begin{itemize}
\item \underline{Rappel} Si la fonction de répartition $F$ est continue,
$$\sqrt{n}\sup_{x\in \R}\big|\widehat F_n(x)-F(x)\big| \stackrel{d}{\longrightarrow} \mathbb{B}$$
où la loi de $\mathbb{B}$ ne dépend pas de $F$.
\end{itemize}
\begin{prop}[Test de Kolmogorov-Smirnov]
Soit $q_{1-\alpha}^{\mathbb{B}}$ tel que $\PP\big[\mathbb{B}>q_{1-\alpha}^{\mathbb{B}}\big]=\alpha$. Le test défini par la zone de rejet
$${\mathcal R}_{n,\alpha} = \big\{\sqrt{n}\sup_{x\in \R}\big|\widehat
F_n(x)-F_0(x)\big| \geq q_{1-\alpha}^{\mathbb{B}} \big\}$$
est
\alert{asymptotiquement de niveau $\alpha$ :} $\PP_{F_0}\big[ {\mathcal
  R}_{n,\alpha}\big]\rightarrow \alpha$ et \alert{consistant} :
$$\forall F \neq F_0: \PP_{F}\big[{\mathcal R}_{n,\alpha}^c\big] \rightarrow 0.$$
\end{prop}
\end{frame}

\subsection{Tests du $\chi^2$}

\begin{frame}
\frametitle{Test du Chi-deux}
\begin{itemize}
\item $X$ variable \alert{qualitative} : $X \in \{1,\ldots, d\}$.
$$\PP\big[X=\ell\big]=p_\ell,\;\ell=1,\ldots d.$$
\item La loi de $X$ est caratérisée par ${\boldsymbol p} = (p_1,\ldots, p_d)^T$.
\item \underline{Notation}
$${\mathcal M}_d  = \big\{{\boldsymbol p}=(p_1,\ldots, p_d)^T,\;\;0 \leq p_\ell,\sum_{\ell=1}^dp_\ell=1\big\}.$$
\item \alert{Objectif} ${\boldsymbol q}\in {\mathcal M}_d$ donnée. A partir d'un $n$-échantillon
$$X_1,\ldots, X_n \sim_{\text{i.i.d.}}{\boldsymbol p},$$
tester
$H_0:{\boldsymbol p}={\boldsymbol q}$ \alert{contre} $H_1:{\boldsymbol p}\neq{\boldsymbol q}.$
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Test d'adéquation du $\chi^2$}
\begin{itemize}
\item  distance du $\chi^2$:
$$\chi^2(\boldsymbol{p},\boldsymbol{q})=\sum_{\ell=1}^d \frac{(p_\ell-q_\ell)^2}{q_\ell}.$$
\item Avec ces notations
$\|{\boldsymbol U}_n(\boldsymbol{p})\|^2=n\chi^2(\widehat {\boldsymbol p}_n,\boldsymbol{p}).$
\end{itemize}
\begin{prop}
Pour $\boldsymbol{q} \in {\mathcal M}_d$ le test simple défini par la zone de rejet
$${\mathcal R}_{n,\alpha} = \big\{n\chi^2(\widehat {\boldsymbol p}_n,\boldsymbol{q}) \geq q_{1-\alpha,d-1}^{\chi^2} \big\}$$
où
%$q_{1-\alpha,d-1}^{chi^2}>0$ est défini par
$\PP\big[U > q_{1-\alpha,d-1}^{\chi^2}\big]=\alpha$ si $U \sim \chi^2(d-1)$ est
\alert{asymptotiquement de niveau $\alpha$ et consistant} pour tester
$$H_0:\boldsymbol{p}=\boldsymbol{q}\;\;\;\text{contre}\;\;\;
H_1:\boldsymbol{p}\neq\boldsymbol{q}.$$
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Exemple de mise en oeuvre : expérience de Mendel}
\begin{itemize}
\item Soit $d=4$ et
$$\boldsymbol{q}=\Big(\frac{9}{16},\frac{3}{16},\frac{3}{16},\frac{1}{16}\Big).$$
\item \alert{Répartition observée} : $n=556$
$$\widehat {\boldsymbol p}_{556} = \frac{1}{556}(315,101,108,32).$$
\item \alert{Calcul de la statistique du $\chi^2$}
$$556 \times \chi^2(\widehat {\boldsymbol p}_{556}, \boldsymbol{q})=0,47.$$
\item On a $q_{95\%, 3}=0,7815$.
\item \alert{Conclusion :} Puisque $0,47 < 0,7815$, on accepte l'hypothèse $\boldsymbol{p}=\boldsymbol{q}$ au niveau $\alpha = 5\%$.
\end{itemize}
\end{frame}




\end{document}








