
\input{../../def/defslide}


\title{MAP 433 : Introduction aux méthodes statistiques. Cours 7}
%\author{Marc Hoffmann}
%\institute{Université Paris-Dauphine}
\begin{document}
\date{9 Octobre 2015}
\maketitle



\begin{frame}
\frametitle{Aujourd'hui}
\tableofcontents
\end{frame}






%\section{Construction d'un test : hypothèses générales}
%
%
%%\frametitle
%
%\begin{frame}
%\frametitle{Situation}
%\begin{itemize}
%\item \underline{Situation} : on part d'une expérience statistique $\big(\Xset, \Xsigma,\{\PP_\truetheta, \truetheta \in \Theta\}\big)$ engendrée par l'observation $Z$.
%\item On souhaite tester :
%$$H_0:\;\truetheta \in \Theta_0 \subset \Theta\;\;\;\text{\alert{contre}}\;\;\;H_1:\truetheta \in \Theta_1$$
%avec $\alert{\Theta_0 \cap \Theta_1 = \emptyset}$.
%\item Si $\Theta_0 = \{\truetheta_0\}$ et $\Theta_1 = \{\truetheta_1\}$, on a Neyman-Pearson. \alert{ Et sinon ?}
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Principe de construction}
%\begin{itemize}
%\item  Trouver  une \alert{statistique libre sous l'hypothèse} : toute quantité $\phi(Z)$ \alert{observable} dont on connait la loi sous l'hypothèse, c'est-à-dire la loi de $\phi(Z)$ sous $\PP_\truetheta$ avec $\truetheta \in \Theta_0$.
%\item On  regarde  si le comportement de $\phi(Z)$ est \alert{typique} d'un comportement sous l'hypothèse.
%\item Si oui, on \alert{accepte} $H_0$, si non on \alert{rejette} $H_0$.
%\item On quantifie  oui/non  par le niveau $\alpha$ du test.
%\end{itemize}
%\end{frame}
%
%\subsection{Retour sur un exemple}
%\begin{frame}
%\frametitle{Exemple : test sur la variance}
%\begin{itemize}
%\item On observe $Z=(Y_1,\ldots, Y_n)$,
%$$Y_1,\ldots, Y_n \sim_{\text{i.i.d.}}{\mathcal N}(\mu,\sigma^2)$$
%avec $\truetheta =(\mu,\sigma^2) \in \Theta = \R \times (0,+\infty)$.
%\item \alert{Premier cas} : on teste
%$$ \alert{ H_0:\sigma^2 = \sigma_0^2\;\;\;\text{ contre}\;\;\; H_1:\sigma^2 > \sigma_0^2.}$$
%\item Sous l'hypothèse (c'est-à-dire sous $\PP_\truetheta$ avec $\truetheta = (\mu,\sigma_0)$ et $\mu \in \R$ quelconque), on a
%$$\boxed{(n-1)\frac{s_n^2}{\sigma_0^2} \sim \chi^2(n-1)}$$
%avec $s_n^2:=\frac{1}{n-1}\sum_{i = 1}^n (Y_i-\overline{Y}_n)^2$.
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (cont.)}
%\begin{itemize}
%\item Donc, \alert{sous l'hypothèse}, le comportement  typique de
%$$\phi(Z) = (n-1)\frac{s_n^2}{\sigma_0^2} $$
%est celui d'une variable aléatoire de loi du $\chi^2$ à $n-1$ degrés de liberté.
%\item Soit $q_{1-\alpha,n-1}^{\chi^2}>0$ tel que si $U \sim {\chi^2}(n-1)$, alors
%$$\PP\big[U > q_{1-\alpha,n-1}^{\chi^2}\big] = \alpha.$$
%\item \alert{Sous l'hypothèse} $\phi(Z)\stackrel{d}{=} U$ et donc la probabilité pour que $\phi(Z)$ dépasse $q_{1-\alpha,n-1}^{\chi^2}$ est inférieure (égale) à $\alpha$ (comportement atypique si $\alpha$ petit).
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (cont.)}
%\begin{itemize}
%\item \alert{Règle de décision :} On accepte l'hypothèse si
%$$\phi(Z) \leq  q_{1-\alpha,n-1}^{\chi^2}.$$
%On la rejette sinon.
%\item Par construction, on a un \alert{test de niveau $\alpha$}.
%\item On ne \alert{sait rien dire sur l'erreur de seconde espèce}, mis à part qu'elle est minimale parmi les tests de zone de rejet de la forme de $\{\phi(Z) > c\}$, $c>0$...
%\end{itemize}
%\end{frame}
%
%\begin{frame}
%\frametitle{Test sur la variance (fin)}
%\begin{itemize}
%\item \alert{Deuxième cas :} On teste
%$$H_0: \alert{ \sigma^2 \leq \sigma_0^2}\;\;\;\text{ contre}\;\;\; H_1:\sigma^2 > \sigma_0^2.$$
%\item \alert{Pas de statistique libre évidente...} Mais, pour $\sigma^2 \leq \sigma_0^2$, on a
%\begin{align*}
%\PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma_0^2} > q_{1-\alpha,n-1}^{\chi^2}\big]
%= & \PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma^2} > \alert{\tfrac{\sigma_0^2}{\sigma^2} } q_{1-\alpha,n-1}^{\chi^2}\big] \\
%\leq &  \PP_{\sigma}\big[(n-1)\tfrac{s_n^2}{\sigma^2} > q_{1-\alpha,n-1}^{\chi^2}\big] \\
%= & \alpha.
%\end{align*}
%\item La même statistique de test convient pour contrôler l'erreur de première espèce que pour l'hypohèse nulle simple. On choisit \alert{ ici} la \alert{même} règle de décision.
%\end{itemize}
%\end{frame}
%
%\subsection{Principe de construction}
%
%\begin{frame}
%\frametitle{Conclusion provisoire}
%\begin{itemize}
%\item Pour contruire un test de l'hypothèse $H_0:\truetheta \in \Theta_0$ contre $H_1:\truetheta \in \Theta_1$, on cherche \alert{une statistique libre} sous l'hypothèse et on rejette pour un seuil qui dépend de la loi de la statistique sous $H_0$, de sorte de fournir une zone de rejet \alert{ maximale}.
%%étudie séparément l'erreur de seconde espèce
% \item Le plus souvent, la statistique est obtenue via un estimateur. Sauf exception (comme la cas gaussien) une telle statistique est difficile à trouver en général.
% \item \alert{Simplification} cadre asymptotique (où la gaussianité réapparaît le plus souvent...).
%\end{itemize}
%
%\end{frame}

\section{Tests asymptotiques}
\subsection{Elements de la théorie asymptotique des tests}
\begin{frame}
\frametitle{Quelques définitions}
\begin{itemize}
\item Soit $(\PP_\truetheta, \truetheta \in \Theta)$ une famille de probabilités sur $(\Xset,\Xsigma)$ admettant des densités $\{ f(\truetheta,x), \truetheta \in \Theta \}$ par rapport à une mesure de domination $\mu$.
\item Supposons que nous disposions d'un $n$-échantillon $(X_1,X_2,\dots, X_n)$ de ce modèle statistique.
\item Considérons le problème de tester l'hypothèse de base $H_0: \theta \in \Theta_0$ contre l'alternative $H_1: \theta \in \Theta_1$, où $\Theta_0 \cap \Theta_1= \emptyset$ et $\Theta_0 \cup \Theta_1= \Theta$.
\item Un \alert{test} pour un échantillon de taille $n$ est une  fonction mesurable
\[
\varphi_n : \Xset^n \to \ccint{0,1} \eqsp.
\]
\item Si le test est \alert{non randomisé} $\varphi_n \in \{0,1\}$, l'ensemble
\[
\{ (x_1,\dots, x_n) \in \Xset^n, \varphi_n(x_1,\dots,x_n)= 1\}
\]
est appelée la \alert{région critique du test}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Tests asymptotiques}
\begin{itemize}
\item  On dit qu'une suite de  tests $\{\varphi_n, n \in \nset\}$ est \alert{asymptotiquement de niveau $\alpha$} pour $\alpha \in \ccint{0,1}$ si
\alert{
\[
\lim_{n \to \infty} \PE^n_\truetheta[ \varphi_n(X_1,\dots,X_n)] \leq \alpha \eqsp, \text{pour tout $\truetheta \in \Theta_0$}
\]
}
\item La puissance de ce test est la fonction
\[
\truetheta \mapsto \puissance_n(\truetheta)= \PE^n_{\truetheta}[ \varphi_n(X_1,\dots,X_n)]
\]
\item On dit qu'une suite de tests $\{\varphi_n, n \in \nset\}$ est asymptotiquement \alert{consistante} si, pour tout $\theta \in \Theta_1$,
\alert{
\[
\lim_{n \to \infty} \puissance_n(\truetheta) = 1 \eqsp.
\]
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Modèle régulier}
\begin{df} La famille de densités $\{f(\truetheta,\cdot),\truetheta \in \Theta\}$,  par rapport à la mesure dominante $\mu$, $\Theta \subset \R$, est \alert{régulière} si
\begin{itemize}
\item $\Theta$ ouvert et $\{f(\truetheta, \cdot)>0\}=\{f(\truetheta', \cdot)>0\}$, $\forall \truetheta, \truetheta' \in \Theta$.
\item $\mu$-p.p. $\truetheta \leadsto f(\truetheta,\cdot)$, $\truetheta \leadsto \log f(\truetheta,\cdot)$ sont ${\mathcal C}^2$.
 \item Pour tout $\truetheta \in \Theta$, il existe un voisinage ${\mathcal V}_\truetheta \subset \Theta$ t.q. pour $\tilde{\truetheta} \in {\mathcal V}_\truetheta$
$$|\nabla_\truetheta^{2}\log f(\tilde{\truetheta},x)|+|\nabla_\truetheta \log f(\tilde{\truetheta},x)|+\big(\nabla_\truetheta\log f(\tilde{\truetheta},x)\big)^2\leq g(x)$$
où
$$\int_{\Xset}g(x)\sup_{\tilde{\truetheta} \in {\mathcal V}(\truetheta)}f(\tilde{\truetheta},x)\mu(dx)<+\infty.$$
\item L'information de Fisher est non-dégénérée : pour tout $\truetheta \in \Theta$,
$$\;\;\mathbb{I}(\truetheta) >0.$$
\end{itemize}
\end{df}
\end{frame}

\begin{frame}
\frametitle{Consistance du test de Neyman-Pearson}
\begin{itemize}
\item Supposons que $\Theta= \{\theta_0,\theta_1\}$ avec $\theta_0 \ne \theta_1$ et que l'on cherche à tester $H_0: \theta = \theta_0$ contre $H_1: \theta= \theta_1$.
\item Le lemme de Neyman-Pearson montre que le test qui rejette $H_0$ si
\[
\frac{\prod_{i=1}^n f(\truetheta_1,X_i)}{\prod_{i=1}^{n} f(\truetheta_0,X_i)} \geq c_{n,\alpha}
\]
est U.P.P.
\item De façon équivalente, en prenant le logarithme de chaque membre de l'identité, le test de N.P. rejette $H_0$ si
\[
\Lambda_n(\truetheta_0,\truetheta_1) = \sum_{i=1}^n \{ \ell(X_i; \truetheta_1) - \ell(X_i; \truetheta_0) \} \geq k_{n,\alpha}
\]
où $\ell(x;\truetheta)= \log f(\truetheta,x)$ et $k_{n,\alpha}$ est choisi de telle sorte que
\[
\PP_{\truetheta_0}^n [ \Lambda_n(\truetheta_0,\truetheta_1)  \geq  k_{n,\alpha}] = \alpha
\]
(on suppose qu'une telle valeur existe, autrement il faudrait randomiser)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul asymptotique du seuil critique}
\begin{itemize}
\item En pratique, il est souvent difficile de déterminer exactement le seuil critique $k_{n,\alpha}$... mais il est souvent facile de déterminer
une suite $\{k_{n,\alpha}, n \in \nset\}$ telle que
\[
\lim_{n \to \infty} \PP^n_{\theta_0} ( \Lambda_n(\truetheta_0,\truetheta_1) \geq  k_{n,\alpha}) = \alpha.
\]
\item En effet, le théorème central limite montre que, sous $H_0$,
\[
n^{-1/2} \sum_{k=1}^n \{ \ell(X_k; \truetheta_1) - \ell(X_k; \truetheta_0) + \KL{\truetheta_0}{\truetheta_1} \} \dlim_{\PP^n_{\truetheta_0}} \gauss(0,J(\truetheta_0,\truetheta_1))
\]
où $\KL{\truetheta_0}{\truetheta_1}$ est la \alert{divergence de Kullback-Leibler} définie par
\[
\KL{\truetheta_0}{\truetheta_1} = \PE_{\truetheta_0}\left[ \ell(X_1;\truetheta_0) - \ell(X_1;\truetheta_1) \right] > 0
\]
et
\[
J(\truetheta_0,\truetheta_1)= \PVar_{\truetheta_0}[ \ell(X_1;\truetheta_1) - \ell(X_1;\truetheta_0) ] \eqsp.
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Calcul asymptotique du seuil critique}
\begin{itemize}
\item Pour $\alpha \in \ooint{0,1}$, on note $z_{1-\alpha}$ le quantile $1-\alpha$ de la loi gaussienne standardisée.
\item  Nous avons donc:
\[
\lim_{n \to \infty} \PP_{\truetheta_0}^n \left( n^{-1/2}
  J^{-1/2}(\truetheta_0,\truetheta_1) \{ \Lambda_n(\truetheta_0,\truetheta_1) +
  n \KL{\truetheta_0}{\truetheta_1} \} \geq z_{1-\alpha} \right) = \alpha
\]
ce qui implique, en posant
\[
k_{n,\alpha}= -n \KL{\truetheta_0}{\truetheta_1} + n^{1/2} z_{1-\alpha} \sqrt{J(\truetheta_0,\truetheta_1)}
\]
que le test de région critique $\{ \Lambda_n(\truetheta_0,\truetheta_1) \geq
k_{n,\alpha} \}$ est asymptotiquement de niveau $\alpha$,
\[
\lim_{n \to \infty} \PP^n_{\truetheta_0} [ \Lambda_n(\truetheta_0,\truetheta_1)
\geq k_{n,\alpha} ] = \alpha \eqsp.
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Distribution du test sous l'hypothèse alternative}
\begin{itemize}
\item Sous $\PP^n_{\theta_1}$, nous avons
\[
\Delta_n= \frac{1}{\sqrt{n}} \sum_{i=1}^n \{ \ell(X_i;\truetheta_1) -
\ell(X_i;\truetheta_0) - \KL{\truetheta_1}{\truetheta_0} \}
\dlim_{\PP^n_{\truetheta_1}} \gauss( 0, J(\theta_1,\theta_0) )
\]
où
\begin{align*}
&\KL{\truetheta_1}{\truetheta_0}= \PE_{\truetheta_1}[ \ell(X_1;\truetheta_1) - \ell(X_1;\truetheta_0)]  \\
&J(\theta_1,\theta_0) = \PVar_{\truetheta_1} ( \ell(X_1;\truetheta_1) - \ell(X_1;\truetheta_0) )
\end{align*}
\pause \item Par conséquent
\[
\{ \Lambda_n(\truetheta_0,\truetheta_1) \geq k_{n,\alpha} \} = \left\{ \Delta_n
  \geq z_{1-\alpha} \sqrt{J(\truetheta_0,\truetheta_1)} - n^{1/2}
  I(\truetheta_0,\truetheta_1) \right\}
\]
où
$$
I(\truetheta_0,\truetheta_1)= \KL{\truetheta_0}{\truetheta_1} + \KL{\truetheta_1}{\truetheta_0} \eqsp.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Puissance du test de NP}
\begin{itemize}
\item \alert{Conclusion} Si $\KL{\truetheta_0}{\truetheta_1} \ne 0$ alors
\[
\lim_{n \to \infty} \puissance_n(\truetheta_1)= 1 \eqsp.
\]
\item Si le modèle est identifiable, alors il existe un test de niveau asymptotique $\alpha$ et donc la puissance tend vers 1.
\end{itemize}
\end{frame}

\subsection{Efficacité asymptotique relative}
\begin{frame}
\frametitle{Efficacité asymptotique... à travers un exemple}
\begin{itemize}
\item Supposons que $(X_1,\dots,X_n)$ est un $n$-échantillon indépendant de densité $f(\theta,x)= f(x-\truetheta)$ par rapport à la mesure de Lebesgue sur $\rset$.
\item \alert{Hypothèses}
\begin{itemize}
\item Variance finie: $\int |x|^2 f(x) \rmd x < \infty$
\item Parité: $f$ est une fonction paire (donc $\theta$ est la moyenne et la
  médiane de la loi)
\item $f$ est continue et $f(0) > 0$ : unicité de la médiane
\end{itemize}
\item On note $F$ la cdf associée à la densité $f$
\item On cherche à tester $\theta= 0$ contre $H_1: \theta > 0$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Un exemple}
\begin{itemize}
\item On considère deux statistiques de tests:
\begin{align*}
U_n&= \frac{1}{n} \sum_{i=1}^n \indi{X_i > 0} &&\text{test du signe} \\
T_n&= \frac{\bar{X}_n}{S_n}     &&\text{t-test}
\end{align*}
où \begin{itemize}
\item $\bar{X}_n= n^{-1} \sum_{i=1}^n X_i$ est la \alert{moyenne empirique}
\item $S_n^2= n^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$ est la \alert{variance empirique}.
\end{itemize}
\item \alert{Question:} quel test est le meilleur ?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Asymptotique du test du signe}
$$
U_n= n^{-1} \sum_{i=1}^n \indi{X_i > 0}
$$
\begin{itemize}
\item Par le théorème de la limite centrale
$$
n^{1/2} \sigma^{-1}(\theta) (U_n - \mu(\theta)) \dlim_{\PP^n_\theta} \gauss(0,1)
$$
où
\[
\mu(\theta) = 1 - F(-\theta) \quad \qquad \sigma^2(\theta) = (1-F(-\theta)) F(-\theta) \eqsp.
\]
\item Par conséquent, sous $H_0: \theta = 0$
\[
2 \sqrt{n} (U_n - 1/2) \dlim_{\PP^n_0} \gauss(0,1) \eqsp.
\]
\item Le test de région critique  $\{ 2 \sqrt{n} (U_n -1/2) > z_{ 1-\alpha} \}$ est un test de niveau asymptotique $\alpha$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Puissance du test de signe}
La puissance du test de signe de niveau asymptotique $\alpha$ est donnée pour tout $\theta > 0$, par
\begin{align*}
\puissance^{\mathrm{sign}}_{n,\alpha}(\theta)
&= \PP_\theta( \sqrt{n}(U_n - \mu(0)) > \sigma(0) z_{1-\alpha}) \\
&= \PP_\theta( \sqrt{n} \sigma^{-1}(\theta) (U_n - \mu(\theta)) > \sigma^{-1}(\theta) \{ \sigma(0) z_{1-\alpha} + n^{1/2} \{\mu(0)-\mu(\theta)\}\}). \end{align*}

Puisque $\mu(0) < \mu(\truetheta)$ pour $\truetheta > 0$, le test est
\alert{consistant}: pour tout $\theta > 0$, \alert{
\[
\lim_{n \to \infty}  \puissance^{\mathrm{sign}}_{n,\alpha}(\theta) = 1 \eqsp.
\]
}
\end{frame}



\begin{frame}
\frametitle{Asymptotique du $t$-test}
On considère la moyenne empirique \alert{studentisée}
\begin{equation*}
T_n= \frac{1}{n} \sum_{i=1}^n \frac{X_i}{S_n} \quad \text{t-test}
\end{equation*}
où $S^2_n= n^{-1} \sum_{i=1}^n (X_i - \bar{X}_n)^2$ est la \alert{variance empirique}
\begin{itemize}
\item Loi des grands nombres : $S^2_n \plim_{\PP_{\theta}} s^2 = \int x^2 f(x) \rmd x$.
\item Théorème Central limite: $n^{-1/2} \sum_{i=1}^n (X_i - \theta) \dlim_{\PP_\theta} \gauss(0,s^2)$.
\item Slutsky: $n^{-1/2} S_n^{-1} \left\{ \sum_{i=1}^n (X_i-\theta)  \right\} \dlim_{\PP_\theta} \gauss(0,1)$.
\item Le test de région critique $\{ T_n > n^{-1/2} z_{1-\alpha} \}$ est un test de niveau asymptotique $\alpha$:
\begin{align*}
\lim_{n \to \infty} \PP_0 \left( T_n > n^{-1/2} z_{1-\alpha} \right)
&= \lim_{n \to \infty} \PP_0 \left( n^{1/2} T_n > z_{1-\alpha} \right) \\
&= 1 - \Phi(z_{1-\alpha})= \alpha \eqsp.
\end{align*}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Puissance du $t$-test}
La puissance du test de niveau $\alpha$ est donnée par
\begin{align*}
\puissance^{\mathrm{t-test}}_{n,\alpha}(\theta)
&= \PP_{\truetheta}(\sqrt{n} T_n > z_{1-\alpha})\\
&=  \PP_{\truetheta}(n^{-1/2} S_n^{-1} \sum_{i=1}^n (X_i - \theta) > z_{1-\alpha} - \sqrt{n} S_n^{-1} \truetheta).
\end{align*}
Comme $\lim_{n \to \infty} \{ z_{1-\alpha} - \sqrt{n} S_n^{-1} \theta\} = -
\infty$, $\PP_\theta$-p.s. pour tout $\theta >0$, le $t$-test de niveau
asymptotique $\alpha$ est \alert{consistant}: pour tout $\theta > 0$, \alert{
\[
\lim_{n \to \infty} \puissance^{\mathrm{t-test}}_{n,\alpha}(\theta) = 1 \eqsp.
\]
}
\end{frame}

\begin{frame}
\frametitle{Comparaison asymptotique des puissances}
\begin{itemize}
\item Nous devons rendre la discrimination entre l'hypothèse nulle $H_0$ et l'hypothèse alternative $H_1$ plus \alert{difficile} quand $n \to \infty$.
\item \alert{Idée: }  considérer un test $H_0: \theta = 0$ contre \alert{une suite d'hypothèses alternatives} $H^n_1: \theta = \theta_n$ avec $\theta_n > 0$ et $\lim_{n \to \infty} \theta_n = 0$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test du signe}
$$
\puissance^{\mathrm{sign}}_{n,\alpha}(\theta)=
\PP_\theta( \sqrt{n} \sigma^{-1}(\theta) (U_n - \mu(\theta)) > \sigma^{-1}(\theta) \{ \sigma(0) z_{1-\alpha} + \sqrt{n} \{\mu(0)-\mu(\theta)\}\})
$$
\begin{itemize}
\item  la puissance du test contre la suite de contre-alternatives $H_1^n: \theta_n > 0$, dépend de $\sqrt{n} (\mu(0) - \mu(\theta_n))$ où
\[
\mu(\theta) = 1 - F(-\theta) \eqsp.
\]
\item Comme $F$ est différentiable en $\theta = 0$, nous avons
\[
\sqrt{n}(\mu(\theta_n) - \mu(0))= - \sqrt{n} ( F(-\theta_n) - F(0)) =  \sqrt{n} \theta_n f(0) + o(\sqrt{n} \theta_n) \eqsp.
\]
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test du signe}
\begin{itemize}
\item Si $\sqrt{n} \theta_n \to 0$, alors $\sqrt{n}(\mu(0)- \mu(\theta_n)) \to
  0$; donc, $\puissance_{n,\alpha}^{\mathrm{sign}}(\theta_n) \to \alpha$, on ne
  distingue pas l'hypothèse de base et l'alternative.
\item Si $\sqrt{n} \theta_n \to \infty$, alors $\sqrt{n}(\mu(0)- \mu(\theta_n)) \to -\infty$: $\puissance_{n,\alpha}^{\mathrm{sign}}(\theta_n) \to 1$, problème \alert{trop facile}, la puissance tend vers $1$.
\item Cas intéressant !
\alert{
\[
\lim_{n \to \infty} \sqrt{n} \theta_n= h > 0
\]
}
\item Dans ce cas, $\sqrt{n}(\mu(0)- \mu(\theta_n)) \to -h f(0)$ et $\lim_n \sigma(\theta_n) = \sigma(0)=1/2$, donc
\alert{
\[
\lim_{n \to \infty} \puissance_{n,\alpha}^{\mathrm{sign}}(\theta_n) = \Phi(2 h f(0) - z_{1-\alpha})
\]
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficacité asymptotique des tests}
\begin{itemize}
\item Ceci conduit à une approche naturelle de comparaison des tests, qui
  consiste à comparer la \alert{puissance locale des tests} \alert{
\[
\puissance(h)= \lim_{n \to \infty} \puissance_n( h / \sqrt{n}) \eqsp.
\]
}
\item Pour les modèles réguliers, cette fonction de \alert{puissance locale asymptotique} est bien définie (preuve délicate en toute généralité)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficacité asymptotique locale des tests}
\begin{theo}
Soit $\sequence{\theta}[n][\nset] \subset \rset^+_*$ telle que $\lim_{n \to \infty}\sqrt{n} \theta_n = h$.
Soit $\sequence{T}[n][\nset]$ une suite de statistiques vérifiant:
\begin{enumerate}
\item $\sqrt{n} \sigma(\theta_n)^{-1} (T_n - \mu(\theta_n)) \dlim_{\PP_{\theta_n}} \gauss(0,1)$
\item $\mu$ est différentiable en $0$
\item $\sigma$ continue en $0$.
\end{enumerate}
Soit  $\varphi_n$ une suite de tests simples de région critique $\{ T_n > t_{n,\alpha}\}$  de niveau asymptotique $\alpha$: $\lim_{n \to \infty} \PP_{0}( T_n > t_{n,\alpha})= \alpha$.

\bigskip

La \alert{puissance locale asymptotique} de cette suite de tests est donnée par
\[
\puissance(h)= \lim_{n \to \infty} \puissance_n( \sqrt{n} \theta_n)= \Phi(h
\mu'(0)/\sigma(0) - z_{1-\alpha}) \eqsp.
\]
\end{theo}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item Nous disposons maintenant d'une méthode simple de comparer les tests, en nous basant sur la puissance asympotique locale...
\item Pour les tests asymptotiquement normaux, il suffit de comparer la \alert{pente} des tests, à savoir \alert{$\mu'(0)/\sigma(0)$}.
\item Plus la pente est grande, plus $\pi(h)$ augmente rapidement avec $h$ !
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Application: test du signe}
\begin{itemize}
\item $U_n= n^{-1} \sum_{i=1}^n \indi{X_i > 0}$.
\item $\mu(\theta)= 1 - F(-\theta)$, $\mu'(\theta)=  f(\theta)$.
\item $\sigma^2(\theta)= (1 - F(-\theta)) F(-\theta)$
\item \alert{Pente:} $\mu'(0)/\sigma(0)= 2 f(0)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Application: t-test}
\begin{itemize}
\item $T_n = \bar{X}_n/S_n$.
\item $\mu(\theta)= \theta/s$ and $\sigma(\theta)= 1$. En effet
\begin{align*}
\sqrt{n} ( T_n - \theta_n/s)
&= \sqrt{n} ( \bar{X}_n / S_n - \theta_n / S_n) + \sqrt{n} \theta_n (S_n^{-1} - s^{-1}) \\
&= n^{-1/2} \sum_{i=1}^n (X_i - \theta_n)/S_n + + \sqrt{n} \theta_n (S_n^{-1} - s^{-1}) \dlim_{\PP_{\theta_n}} \gauss(0,1) \eqsp.
\end{align*}
\item \alert{Pente:} $\mu'(0)/\sigma(0)= 1/s$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Efficacité relative}
\begin{enumerate}
\item test du signe: $\mu'(0)/\sigma(0)=2 f(0)$,
\item $t$-test: $\mu'(0)/\sigma(0)=1/s$.
\end{enumerate}

\begin{itemize}
\item Laplace: $2 f(0) s = 2$.
\item Logistique: $2 f(0) s = \pi^2/12=0.822$.
\item Gauss: $2 f(0) s = 2/\pi=0.6366$.
\item Uniforme: $2 f(0) s = 1/3$.
\end{itemize}
\end{frame}

\section{Quelques tests asymptotiques}

\subsection{Test du rapport de vraisemblance}
\begin{frame}
\frametitle{Test du rapport de vraisemblance}
 \begin{itemize}
 \item Soit  $X^{(n)}= (X_{1},\ ,\ X_{n})$ un $n$-échantillon du modèle statistique $\PP_{\theta}^{n}\ll\mu_{n},\ \theta\in\Theta$, de densité  $f(\theta,x^{(n)})= \rmd \PP_{\theta}^{n}/ \rmd \mu_{n}$.
 \item Pour tester $H_{0}$ : $\theta\in\Theta_{0}$  contre $H_{1}$ : $\theta\in\Theta-\Theta_{0}$, le test du \alert{rapport de vraisemblance} rejette $H_{0}$ lorsque la valeur du \alert{rapport de vraisemblance généralisé}
$$
\Lambda_{n}=\frac{\sup_{\theta\in\Theta_{0}}f(\theta,X^{(n)})}{\sup_{\theta\in\Theta}f(\theta,X^{(n)})}
$$
est inférieure à un seuil.
\item   Lorsque les hypothèses $H_{0},\ H_{1}$ sont simples, ce test est U.P.P. .
\item Pour des hypothèses composites, il n'y a en général aucun résultat d'optimalité, sauf dans des cas simples...
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{t-test}
\begin{itemize}
\item  Soient $X^{(n)}= (X_{1}, \dots, X_{n})$ un $n$-échantillon de $\gauss(\mu,\ \sigma^{2})$.
\item On teste l'hypothèse $H_0: \mu =0$ contre $H_1: \mu \ne 0$.
\item En posant $\theta=(\mu,\ \sigma^{2})$,
\begin{align*}
\Lambda_{n}&=\frac{\sup_{\theta\in\Theta_{0}}(1/\sigma^{n})\exp(-\frac{1}{2\sigma^{2}}\sum_{i}(X_{i}-\mu)^{2})}{\sup_{\theta\in\Theta}(1/\sigma^{n})\exp(-\frac{1}{2\sigma^{2}}\sum_{i}(X_{i}-\mu)^{2})}\\
&=\left( \frac{\sum_{i}(X_{i}-\overline{X}_{n})^{2}}{\sum_{i}X_{i}^{2}} \right)^{n/2}
\end{align*}
\item Un calcul élémentaire montre que $\Lambda_{n}<c$ est équivalent à $t_{n}^{2}>k$ où
$$
t_{n}=\frac{\sqrt{n}\overline{X}_{n}}{\sqrt{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}}}
$$
est la $\mathrm{t}$-statistique. En d'autres termes, le $\mathrm{t}$-test est un test de rapport de vraisemblance généralisé.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Justification}
\begin{align*}
t_{n}^{2} &=\frac{n\overline{X}_{n}^{2}}{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}} \\
&=\frac{\sum_{i}X_{i}^{2}-\sum_{i}(X_{i}-\overline{X}_{n})^{2}}{\frac{1}{n-1}\sum_{i}(X_{i}-\overline{X}_{n})^{2}} \\
&=\frac{(n-1)\sum_{i}X_{i}^{2}}{\sum_{i}(X_{i}-\overline{X}_{n})^{2}}-(n\ -1) \\
&=(n-1)\Lambda_{n}^{-2/n}-(n-1)
\end{align*}
ce qui montre que
$$
\Lambda_{n}=(\frac{n-1}{t_{n}^{2}+n-1})^{n/2}
$$
\end{frame}

\begin{frame}
\frametitle{Distribution asymptotique}
Comme
$$
\Lambda_{n}=(\frac{n-1}{t_{n}^{2}+n-1})^{n/2}
$$
nous avons
\begin{align*}
&\log\Lambda_{n}=\frac{n}{2}\log\frac{n-1}{t_{n}^{2}+n-1} \\
&\Rightarrow \ -2\log\Lambda_{n}=n\log(1+\frac{t_{n}^{2}}{n-1}) \\
&=n \left(\frac{t_{n}^{2}}{n-1}+o_{p}(\frac{t_{n}^{2}}{n-1}) \right) \dlim_{\PP_0^n} \chi_{1}^{2}
\end{align*}
car sous $H_{0}$,  $t_{n} \dlim_{\PP_0^n} \gauss(0,1)$.

\bigskip

\alert{résultat vrai en toute généralité !}
\end{frame}


\begin{frame}
\frametitle{Test d'égalité des proportions pour une variable multinomiale}
\begin{itemize}
\item Soit $(X_1,\dots,X_n)$ un $n$-échantillon d'une loi multinomiale à $d$-instances
\item \alert{Paramètre} $\mathbf{p}= (p_1,\dots,p_d) \in \mathcal{M}_d= \{ (p_1, \dots, p_d), p_i \geq 0, \sum_{i=1}^d p_i =1 \}$.
\item \alert{Rapport de vraisemblance généralisé}
\begin{align*}
\Lambda_n
&= \frac{(1/d)^n}{\max_{(p_1,\dots,p_d) \in \mathcal{M}_d} \prod_{i=1}^d p_i^{N_i} } \\
&= \prod_{i=1}^d \left( \frac{n}{d N_i} \right)^{N_i} = \prod_{i=1}^d (d \hat{p}_{n,i})^{-N_i}
\end{align*}
où $N_i= \sum_{j=1}^n \indi{X_j=i}$  et $\hat{p}_{n,i}= N_i/n$ les fréquences empiriques.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Loi limite des fréquences empiriques}
\begin{itemize}
\item On suppose $(X_1,\dots,X_n)$ $n$-échantillon multinomial de proportion $(q_1,\dots,q_d)$.
\item \alert{Comparaison des fréquences empiriques}
$$\widehat p_{n,\ell}=\frac{1}{n}\sum_{i=1}^n \indi{X_i=\ell}\;\;\;\text{\alert{proche de}}\;\;q_\ell,\;\;\ell=1,\ldots, d\; \alert{?}$$
\item Loi des grands nombres :
$$\big(\widehat p_{n,1},\ldots, \widehat p_{n,d}\big) \stackrel{\PP_{{\boldsymbol p}}}{\longrightarrow} (p_1,\ldots, p_d)={\boldsymbol p}.$$
\item \alert{Théorème central-limite ?}
$${\boldsymbol{U}_n}(\boldsymbol{p})=\sqrt{n}\Big(\frac{\widehat p_{n,1}-p_1}{\sqrt{p_1}},\ldots, \frac{\widehat p_{n,d}-p_d}{\sqrt{p_d}}\Big) \stackrel{d}{\longrightarrow} ?$$
\item Composante par composante oui. \alert{Convergence globale plus délicate}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Statistique du Chi-deux}
\begin{prop}
Si les composantes de $\boldsymbol{p}$ sont toutes non-nulles
\begin{itemize}
\item On a la \alert{convergence en loi} sous $\PP_{\boldsymbol{p}}$
$${\boldsymbol{U}_n}(\boldsymbol{p})\stackrel{d}{\longrightarrow} {\mathcal N}\big(0,V(\boldsymbol{p})\big)$$
avec $V(\boldsymbol{p}) = \mathrm{Id}_d-\sqrt{\boldsymbol{p}}\big(\sqrt{\boldsymbol{p}}\big)^T$ et $\sqrt{\boldsymbol{p}} = (\sqrt{p_1},\ldots, \sqrt{p_d})^T$.
\item \alert{De plus}
$$\|{\boldsymbol{U}_n}(\boldsymbol{p})\|^2 = n\sum_{\ell=1}^d \frac{(\widehat p_{n,\ell}-p_\ell)^2}{p_\ell} \stackrel{d}{\longrightarrow} \chi^2(\alert{d-1}).$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve de la normalité asymptotique}
\begin{itemize}
\item Pour $i=1,\ldots, n$ et $1 \leq \ell \leq d$, on pose
$$Y_\ell^i=\frac{1}{\sqrt{p_\ell}}\big(\indi{X_i=\ell}-p_\ell\big).$$
\item Les vecteurs ${\boldsymbol Y}_i=(Y_1^i,\ldots, Y_d^i)$ sont \alert{indépendants et identiquement distribués} et
$${\boldsymbol U}_n(\boldsymbol{p}) = \frac{1}{\sqrt{n}}\sum_{i = 1}^n {\boldsymbol Y}_i,$$
$\E\big[Y_\ell^i\big]=0$, $\E\big[(Y_\ell^i)^2\big]=1-p_\ell$, $\E\big[Y_\ell^iY_{\ell'}^i \big]=-(p_\ell p_{\ell'})^{1/2}$.
\item \alert{On applique le TCL vectoriel}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Convergence de la norme au carré}
\begin{itemize}
\item On a donc ${\boldsymbol U}_n(\boldsymbol{p}) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0,V(\boldsymbol{p})\big)$.
\item On a aussi
\begin{align*}
\|{\boldsymbol U}_n(\boldsymbol{p}) \|^2 & \stackrel{d}{\longrightarrow} \| {\mathcal N}\big(0,V(\boldsymbol{p})\big)\|^2 \\
& \sim \chi^2\big(\mathrm{Rang}\big(V(\boldsymbol{p})\big)\big)
\end{align*}
par \alert{Cochran} :  $V(\boldsymbol{p}) = \mathrm{Id}_d-\sqrt{\boldsymbol{p}}\big(\sqrt{\boldsymbol{p}}\big)^T$ est la projection orthogonale sur $\mathrm{vect}\{\sqrt{\boldsymbol{p}}\}^\perp$ qui est de dimension $d-1$.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Distribution limite de $-2 \log \Lambda_n$}
Nous avons
\begin{align*}
-2 \log \Lambda_n &= 2\sum_{i=1}^d N_i \log( \hat{p}_{N,i}/p_i) \\
                  &= 2 n \sum_{i=1}^d  (\hat{p}_{N,i} - p_i + p_i) \log\left( 1 + \frac{\hat{p}_{n,i}-p_i}{p_i} \right) \\
                  &= 2 n \sum_{i=1}^d \frac{(\hat{p}_{n,i}-p_i)^2}{p_i} + 2 n \sum_{i=1}^d p_i \left\{ \frac{(\hat{p}_{n,i}-p_i)}{p_i} - \frac{1}{2} \left(\frac{\hat{p}_{n,i}-p_i}{p_i} \right)^2 \right\} + o_{\PP}(1) \eqsp.
\end{align*}
car $\sum_{i=1}^d p_i (\hat{p}_{n,i} - p_i)/p_i= 0$... Par conséquent
\alert{
\[
-2 \log \Lambda_n \dlim_{\PP_{\mathbb{p}}} \chi^2(d-1)
\]
}
Trop beau pour qu'il n'y ait pas quelque chose de plus profond.. en PC
\end{frame}

\subsection{Tests de Wald}
\begin{frame}
\frametitle{Le test de Wald : hypothèse nulle simple}
\begin{itemize}
\item \underline{Situation} la suite d'expériences $\big(\Xset^n, \Xsigma^{\otimes n},\{\PP_\truetheta^n,\truetheta \in \Theta\}\big)$ est engendrée par l'observation $Z^n= (X_1,\dots,X_n)$, $\truetheta \in \Theta \subset \R$
\item \alert{Objectif} : Tester
$$H_0:\truetheta = \truetheta_0\;\;\;\text{contre}\;\;H_1 \;\truetheta\neq \truetheta_0.$$
\item \alert{Hypothèse} : on dispose d'un estimateur $\est$ \alert{asymptotiquement normal}
$$\boxed{\sqrt{n}(\est-\truetheta)\stackrel{d}{\rightarrow}{\mathcal N}\big(0,v(\truetheta)\big)}$$
en loi sous $\PP_{\truetheta}^n$, $\forall \truetheta \in \Theta$, où $\truetheta \leadsto v(\truetheta) >0$ est continue.
\item On a \alert{la convergence}
$$\sqrt{n}\frac{\est-\truetheta_0}{\sqrt{v(\est)}}\stackrel{d}{\longrightarrow}_{\PP_{\truetheta_0}^n} {\mathcal N}(0,1).$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald (cont.)}
\begin{itemize}
\item \underline{Remarque} $\sqrt{v(\est)} \leftrightarrow \sqrt{v(\truetheta_0)}$ ou d'autres choix encore...
\item On a aussi
$$T_n = n\frac{(\est-\truetheta_0)^2}{v(\est)} \stackrel{d}{\longrightarrow}_{\PP_{\truetheta_0}^n} \chi^2(1).$$
\item Soit $q_{1-\alpha,1}^{\chi^2} >0$ tel que si $U \sim \chi^2(1)$, on a $\PP\big[U > q_{1-\alpha,1}^{\chi^2}\big]=\alpha$. On \alert{choisit la zone de rejet}
$${\mathcal R}_{n,\alpha} = \big\{T_n\geq q_{1-\alpha,1}^{\chi^2}\big\}.$$
\item Le test de zone de rejet ${\mathcal R}_{n,\alpha}$ s'appelle \alert{Test de Wald de l'hypothèse simple $\truetheta=\truetheta_0$ contre l'alternative $\truetheta \neq \truetheta_0$ basé sur $\est$.}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Propriétés du test de Wald}
\begin{prop}
Le test de Wald de l'hypothèse simple $\truetheta=\truetheta_0$ contre l'alternative $\truetheta \neq \truetheta_0$ basé sur $\est$ est
\begin{itemize}
\item \alert{asymptotiquement} de niveau $\alpha$ :
$$\PP_{\truetheta_0}^n\big[ {\mathcal R}_{n,\alpha}\big]\rightarrow \alpha.$$
\item \alert{convergent ou (consistant)}. Pour tout point $\truetheta \neq \truetheta_0$
$$\PP_\truetheta^n\big[ {\mathcal R}_{n,\alpha}^c\big] \rightarrow 0.$$
\end{itemize}
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Preuve}
\begin{itemize}
\item Test asymptotiquement de niveau $\alpha$ \alert{par construction}.
\item \underline{Contrôle de l'erreur de seconde espèce :}
Soit $\truetheta \neq \truetheta_0$. On a
\begin{align*}
T_n & = \Big(\sqrt{n}\frac{\est-\truetheta}{\sqrt{v(\est)}}+\sqrt{n}\frac{\truetheta-\truetheta_0}{\sqrt{v(\est)}}\Big)^2 \\
& =: T_{n,1}+T_{n,2}.
\end{align*}
On a $T_{n,1} \stackrel{d}{\longrightarrow} {\mathcal N}(0,1)$ sous $\PP_{\truetheta}^n$ et
$$T_{n,2} \stackrel{\PP_{\truetheta}^n}{\longrightarrow} \pm \infty\;\;\alert{\text{car}\;\;\truetheta \neq \truetheta_0}$$
Donc $T_{n}\stackrel{\PP_{\truetheta}^n}{\longrightarrow}+\infty$, d'où le résultat.
\item \alert{Remarque} : si $\truetheta \neq \truetheta_0$ mais $|\truetheta - \truetheta_0| \lesssim 1/\sqrt{n}$, le raisonnement ne s'applique pas. Résultat \alert{non uniforme en le paramètre}.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald : cas vectoriel}
\begin{itemize}
\item \alert{ Même contexte:} $\Theta \subset \R^d$ et \alert{on dispose} d'un estimateur $\est$ asymptotiquement normal :
$$\sqrt{n}\big(\est-\truetheta\big)\stackrel{d}{\longrightarrow} {\mathcal N}\big(0, V(\truetheta)\big)$$
où la matrice $V(\truetheta)$ est \alert{définie positive} et continue en $\truetheta$.
\item On cherche à tester $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_1$.
\item Sous $\PP_\truetheta$, la convergence $n^{1/2} (\est - \truetheta) \dlim \gauss(0,V(\truetheta))$ implique que
$$
V^{-1/2}(\truetheta) n^{1/2} (\est - \theta) \dlim \gauss(0,\Id{d})
$$
et donc que
$$
n (\est - \truetheta)^{T} V^{-1}(\truetheta) (\est - \truetheta) \dlim \chi^2(d)  \eqsp.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple: loi exponentielle}
\begin{itemize}
\item \alert{Hypothèse}: $\{X_i\}_{i=1}^n$, \iid\ de loi exponentielle de paramètre $\theta \in \Theta= \rset_+^*$.
\item \alert{log-vraisemblance}
\[
\ell_n(\truetheta)= n^{-1} \sum_{i=1}^n \log f(\truetheta,X_i)= \log(\truetheta) - \theta \bar{X}_n
\]
où $\bar{X}_n= n^{-1} \sum_{i=1}^n X_i$ est la moyenne empirique.
\item Estimateur du MV: $\hat{\theta}_n = \bar{X}_n^{-1}$.
\item \alert{Modèle régulier}
\[
\sqrt{n} (\est - \truetheta) \dlim_{\PP_{\truetheta}} \gauss(0,I^{-1}(\truetheta))
\]
où $I(\truetheta)= \theta^{-2}$ est l'\alert{information de Fisher}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exemple: test loi exponentielle}
\begin{itemize}
\item \alert{Test de Wald} de l'hypothèse $H_0: \truetheta = \truetheta_0$ contre l'hypothèse $H_1: \truetheta \ne \truetheta_0$.
\begin{align*}
n (\est - \truetheta_0)^2 I(\est)= n (1 - \truetheta_0 / \est)^2 \dlim_{\PP_{\truetheta_0}} \geq q^{\chi2}_{1-\alpha,1}
\end{align*}
\item \alert{Application numérique} $n=100$, $\truetheta_0=0.5$,
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Test de Wald: cas vectoriel}
\begin{itemize}
\item Le test de Wald de l'hypothèse $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_0$
rejette $H_0$ si
\[
n (\est - \truetheta_0)^T V^{-1}(\est) (\est - \truetheta_0) > q^{\chi^2}_{1-\alpha,d} \eqsp
\]
\item On peut remplacer la matrice de covariance $V(\est)$ par $V(\truetheta_0)$ ou tout estimateur consistant de $V(\truetheta_0)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald : hypothèse nulle composite}
\begin{itemize}
\item \alert{ Même contexte:} $\Theta \subset \R^d$ et \alert{on dispose} d'un
  estimateur $\est$ asymptotiquement normal : pour tout $\truetheta \in \Theta$,
$$\sqrt{n}\big(\est-\truetheta\big)\stackrel{d}{\longrightarrow}_{\PP_{\truetheta}^n} {\mathcal N}\big(0, V(\truetheta)\big)$$
où la matrice $V(\truetheta)$ est \alert{définie positive} et continue en $\truetheta$.
\item \alert{But:} Tester $H_0: \truetheta \in \Theta_0$ contre $H_1:\truetheta \notin \Theta_0$, où
$$\boxed{\Theta_0 = \big\{\truetheta \in \Theta,\;\;g(\truetheta) = 0\big\}}$$
et
$$g:\R^d \rightarrow \R^m$$
($m \leq d$) est régulière.
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Test de Wald cont.}
\begin{itemize}
\item \alert{Hypothèse : } la différentielle (de matrice $J_g(\truetheta)$) de $g$ est de rang maximal $m$ en tout point de (l'intérieur) de $\Theta_0$.
\end{itemize}
\begin{prop}
  En tout point $\truetheta$ de l'intérieur de $\Theta_0$ on a, en loi sous
  $\PP_\truetheta^n$ (i.e. \alert{sous l'hypothèse}) :
\begin{itemize}
\item $$\sqrt{n}g(\est) \stackrel{d}{\longrightarrow} {\mathcal N}\big(0, J_g(\truetheta)V(\truetheta)J_g(\truetheta)^T\big),$$
\item $$\alert{T_n=ng(\est)^T\Sigma_g(\est)^{-1}g(\est)} \stackrel{d}{\longrightarrow} \chi^2(m)$$
%en loi sous $\PP_\truetheta^n$,
où $\Sigma_g(\truetheta) =J_g(\truetheta) V(\truetheta) J_g(\truetheta)^T$.
\end{itemize}
\end{prop}
\begin{itemize}
\item Preuve : méthode  delta  multidimensionnelle.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Wald (fin)}
\begin{prop}
Sous les hypothèses précédentes, le test de zone de rejet
$${\mathcal R}_{n,\alpha}  = \big\{T_n \geq q_{1-\alpha, m}^{\chi^2}\big\}$$
avec $\PP\big[U > q_{1-\alpha, m}^{\chi^2}\big]=\alpha$ si $U \sim \chi^2(m)$ est
\begin{itemize}
\item \alert{Asymptotiquement de niveau $\alpha$} en tout point $\truetheta$ de (l'intérieur) de $\Theta_0$ :
$$\PP_\truetheta^n\big[ {\mathcal R}_{n,\alpha}\big]\rightarrow \alpha.$$
\item \alert{Convergent} : pour tout $\truetheta \notin \Theta_0$ on a
$$\PP_\truetheta^n\big[{\mathcal R}_{n,\alpha}^c\big]\rightarrow 0.$$
\end{itemize}
\end{prop}
\begin{itemize}
\item C'est la  même preuve qu'en dimension 1.
\end{itemize}
\end{frame}



\subsection{Test de Rao}


\begin{frame}
\frametitle{Test du score (Rao)}
\begin{itemize}
\item Soit $(X_1, \cdots, X_n)$ un $n$-échantillon \iid\ associé à un modèle statistique $(\PP_\truetheta, \truetheta \in \Theta \subseteq \rset^d)$  \alert{régulier}
\item Pour $\truetheta \in \Theta$, le \alert{score de Fisher} est donné par
\[
\eta_{\truetheta}(x)= \nabla_{\truetheta} \log f(\truetheta,x)
\]
\item \alert{Propriétés}
\begin{itemize}
\item Le score de Fisher est centré sous $\PP_{\truetheta}$,
$$
\PE_{\truetheta}[\eta_{\truetheta}(X)]= 0 \eqsp, \quad \truetheta \in \Theta \eqsp.
$$
\item La matrice de covariance du score de Fisher est égale à la \alert{matrice d'Information de Fisher}
$$
I(\truetheta)= \PE_{\truetheta} \left[ \eta_{\truetheta}(X) \eta_{\truetheta}(X)^T \right]
$$
\end{itemize}
\item \alert{Conclusion} Pour tout $\truetheta \in \Theta$,
$$
Z_n(\truetheta) = n^{-1/2} \sum_{i=1}^n \eta_\truetheta(X_i) \dlim_{\PP_{\truetheta}^n} \gauss(0, I(\truetheta)) \eqsp.
$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test de Rao}
\begin{itemize}
\item Pour tester $H_0: \truetheta= \truetheta_0$ contre $H_1: \truetheta \ne \truetheta_0$, nous considérons la statistique de test
\[
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0)
\]
\item Sous l'hypothèse nulle,
$$
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0) \dlim_{\PP_{\truetheta_0}^n} \chi^2(d)
$$
et donc le test de Rao de rejet
$$
Z_n(\truetheta_0)^T I^{-1} (\truetheta_0) Z_n(\truetheta_0) \geq q_{1-\alpha,d}^{\chi^2}
$$
est asymptotiquement de niveau $\alpha$.
\end{itemize}
\end{frame}

\section{Tests d'adéquation}

\begin{frame}
\frametitle{Tests d'adéquation}
\begin{itemize}
\item \underline{Situation} On observe (pour simplifier) un $n$-échantillon de loi $F$ inconnue
$$X_1,\ldots, X_n\sim_{\text{i.i.d.}}F$$
\item \alert{Objectif} Tester
$$H_0:F=F_0\;\;\text{contre $H_1$:}\;\;F\neq F_0$$
où
$F_0$ distribution donnée. Par exemple : $F_0$ \alert{gaussienne centrée réduite}.
\item Il est \alert{très facile de construire un test asymptotiquement de niveau $\alpha$.}
Il suffit de trouver une statistique $\phi(X_1,\ldots, X_n)$ de loi connue sous l'hypothèse de base.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Test d'adéquation : situation}
\begin{itemize}
\item \alert{Exemples : sous l'hypothèse}
  \begin{align*}
    &  \phi_1(X_1\ldots, X_n) = \sqrt{n}\overline{X}_n \sim {\mathcal N}(0,1) \\
    & \phi_2(X_1,\ldots, X_n) = \sqrt{n}\frac{\overline{X}_n}{S_n} \sim \text{Student}(n-1) \qquad \text{avec} \ S_n = (n-1)^{-1} \sum_{k=1}^n (X_k - \bar X_n)^2 \\
    & \phi_3(X_1,\ldots, X_n) = (n-1)s_n^2 \sim \chi^2(n-1).
  \end{align*}
%% $$\phi_1(X_1\ldots, X_n) = \sqrt{n}\overline{X}_n \sim {\mathcal N}(0,1)$$
%% $$\phi_2(X_1,\ldots, X_n) = \sqrt{n}\frac{\overline{X}_n}{S_n} \sim \text{Student}(n-1) \qquad \text{avec} \ S_n = (n-1)^{-1} \sum_{k=1}^n (X_k \bar X_n)^2$$
%% $$\phi_3(X_1,\ldots, X_n) = (n-1)s_n^2 \sim \chi^2(n-1).$$
\item Le problème est que ces tests \alert{ont une faible puissance} : ils ne sont pas consistants.
\item Pas exemple, si $F\neq$ gaussienne mais $\int_{\R}xdF(x)=0$, $\int_{\R}x^2dF(x)=1$, alors
$$\PP_{F}\big[\phi_1(X_1,\ldots,X_n) \leq x \big] \rightarrow \int_{-\infty}^x e^{-u^2/2}\frac{du}{\sqrt{2\pi}},\;\;x \in \R.$$
(résultats analogues pour $\phi_2$ et $\phi_3$).
\item La statistique de test $\phi_i$ \alert{ne caractérise pas} la loi $F_0$.
\end{itemize}
\end{frame}


\subsection{Tests de Kolmogorov-Smirnov}

\begin{frame}
\frametitle{Test de Kolmogorov-Smirnov}
\begin{itemize}
\item \underline{Rappel} Si la fonction de répartition $F$ est continue,
$$\sqrt{n}\sup_{x\in \R}\big|\widehat F_n(x)-F(x)\big| \stackrel{d}{\longrightarrow} \mathbb{B}$$
où la loi de $\mathbb{B}$ ne dépend pas de $F$.
\end{itemize}
\begin{prop}[Test de Kolmogorov-Smirnov]
Soit $q_{1-\alpha}^{\mathbb{B}}$ tel que $\PP\big[\mathbb{B}>q_{1-\alpha}^{\mathbb{B}}\big]=\alpha$. Le test défini par la zone de rejet
$${\mathcal R}_{n,\alpha} = \big\{\sqrt{n}\sup_{x\in \R}\big|\widehat
F_n(x)-F_0(x)\big| \geq q_{1-\alpha}^{\mathbb{B}} \big\}$$
est
\alert{asymptotiquement de niveau $\alpha$ :} $\PP_{F_0}\big[ {\mathcal
  R}_{n,\alpha}\big]\rightarrow \alpha$ et \alert{consistant} :
$$\forall F \neq F_0: \PP_{F}\big[{\mathcal R}_{n,\alpha}^c\big] \rightarrow 0.$$
\end{prop}
\end{frame}

\subsection{Tests du $\chi^2$}

\begin{frame}
\frametitle{Test du Chi-deux}
\begin{itemize}
\item $X$ variable \alert{qualitative} : $X \in \{1,\ldots, d\}$.
$$\PP\big[X=\ell\big]=p_\ell,\;\ell=1,\ldots d.$$
\item La loi de $X$ est caratérisée par ${\boldsymbol p} = (p_1,\ldots, p_d)^T$.
\item \underline{Notation}
$${\mathcal M}_d  = \big\{{\boldsymbol p}=(p_1,\ldots, p_d)^T,\;\;0 \leq p_\ell,\sum_{\ell=1}^dp_\ell=1\big\}.$$
\item \alert{Objectif} ${\boldsymbol q}\in {\mathcal M}_d$ donnée. A partir d'un $n$-échantillon
$$X_1,\ldots, X_n \sim_{\text{i.i.d.}}{\boldsymbol p},$$
tester
$H_0:{\boldsymbol p}={\boldsymbol q}$ \alert{contre} $H_1:{\boldsymbol p}\neq{\boldsymbol q}.$
\end{itemize}
\end{frame}




\begin{frame}
\frametitle{Test d'adéquation du $\chi^2$}
\begin{itemize}
\item  distance du $\chi^2$:
$$\chi^2(\boldsymbol{p},\boldsymbol{q})=\sum_{\ell=1}^d \frac{(p_\ell-q_\ell)^2}{q_\ell}.$$
\item Avec ces notations
$\|{\boldsymbol U}_n(\boldsymbol{p})\|^2=n\chi^2(\widehat {\boldsymbol p}_n,\boldsymbol{p}).$
\end{itemize}
\begin{prop}
Pour $\boldsymbol{q} \in {\mathcal M}_d$ le test simple défini par la zone de rejet
$${\mathcal R}_{n,\alpha} = \big\{n\chi^2(\widehat {\boldsymbol p}_n,\boldsymbol{q}) \geq q_{1-\alpha,d-1}^{\chi^2} \big\}$$
où
%$q_{1-\alpha,d-1}^{chi^2}>0$ est défini par
$\PP\big[U > q_{1-\alpha,d-1}^{\chi^2}\big]=\alpha$ si $U \sim \chi^2(d-1)$ est
\alert{asymptotiquement de niveau $\alpha$ et consistant} pour tester
$$H_0:\boldsymbol{p}=\boldsymbol{q}\;\;\;\text{contre}\;\;\;
H_1:\boldsymbol{p}\neq\boldsymbol{q}.$$
\end{prop}
\end{frame}

\begin{frame}
\frametitle{Exemple de mise en oeuvre : expérience de Mendel}
\begin{itemize}
\item Soit $d=4$ et
$$\boldsymbol{q}=\Big(\frac{9}{16},\frac{3}{16},\frac{3}{16},\frac{1}{16}\Big).$$
\item \alert{Répartition observée} : $n=556$
$$\widehat {\boldsymbol p}_{556} = \frac{1}{556}(315,101,108,32).$$
\item \alert{Calcul de la statistique du $\chi^2$}
$$556 \times \chi^2(\widehat {\boldsymbol p}_{556}, \boldsymbol{q})=0,47.$$
\item On a $q_{95\%, 3}=0,7815$.
\item \alert{Conclusion :} Puisque $0,47 < 0,7815$, on accepte l'hypothèse $\boldsymbol{p}=\boldsymbol{q}$ au niveau $\alpha = 5\%$.
\end{itemize}
\end{frame}




\end{document}








