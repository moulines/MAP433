\documentclass[12pt]{article}

\usepackage{amsmath, amsthm, amsfonts, amssymb,enumerate}
\usepackage{dsfont} % utilisation : {\mathds N} (au lieu de mathbb)
\usepackage{mathrsfs,bbm} % utilisation : {\mathscr B} (plus joli que mathcal qu'il remplace)

\usepackage{epsfig,graphics}

%\usepackage{caption}
%\captionsetup[figure]{labelformat=empty}

\usepackage{ccaption}

\usepackage{verbatim} % utile notamment pour l'environnement "comment"

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}

\usepackage{xcolor}
\usepackage{fancybox}
\def\loigauss{\mathcal{N}}
\def\loibeta{\operatorname{Beta}}
\def\loigamma{\operatorname{Gamma}}
\def\loiunif{\operatorname{Unif}}
\def\iid{i.i.d.}
\def\rset{\R}
\def\Rset{\mathbb{R}}
\def\rme{\mathrm{e}}
\def\eqdef{\triangleq}
\def\1{\mathbbm{1}}
\definecolor{mongris}{gray}{0.9}

%%%%%%%%%%%%%%%% mise en page
\setlength{\topmargin}{-1.7cm} \setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm} \setlength{\textwidth}{15.5truecm}
\setlength{\textheight}{25truecm}


\def\ds{\displaystyle}

\newcommand{\C}{{\mathds C}}
\newcommand{\E}{{\mathds E}}
\newcommand{\PE}{{\mathds E}}
\newcommand{\N}{{\mathds N}}
\renewcommand{\P}{{\mathds P}}
\newcommand{\R}{{\mathds R}}
\newcommand{\Z}{{\mathds Z}}
\newcommand{\PP}{{\mathbb P}}
\newcommand{\eps}{\varepsilon}
\newcommand{\un}{\mathsf{1}}

\newcommand{\ii}{{\textup{i}}}
\newcommand{\ee}{{\textup{e}}}

\newcommand{\boF}{{\mathscr F}}
\newcommand{\boG}{{\mathscr G}}
\newcommand{\boB}{{\mathscr B}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\pslim}{\ensuremath{\stackrel{\text{p.s.}}{\longrightarrow}}}


%\def\N{\mathscr N}

\DeclareMathOperator{\Pto}{\stackrel{\P}{\longrightarrow}}
\DeclareMathOperator{\Lto}{\stackrel{\scriptscriptstyle{\textup{loi}}}{\longrightarrow}}
\DeclareMathOperator{\PSto}{\stackrel{p.s.}{\longrightarrow}}

\DeclareMathOperator{\ind}{{\mathds 1}}
\DeclareMathOperator{\Var}{\mathop{\rm Var}\nolimits}

%partie réelle d'un complexe
\renewcommand{\Re}{\operatorname{Re}}

%partie imaginaire d'un complexe
\renewcommand{\Im}{\operatorname{Im}}

\theoremstyle{definition}
\newtheorem{exercice}{Exercice}
\def\cl{\stackrel{\text{(loi)}}{\to}}
\def\eqsp{\,}

 % barre pour separer les exos
\newcommand{\barre}{
\begin{center}
\rule{.3\linewidth}{1pt}
\end{center}
}

\parindent=0pt

%-- Si cette ligne est lue, le corrig√© ne s'affiche pas :
%%\newcommand{\Corrige}[1]{}
%-- Si la ligne suivante est lue, le corrig√© s'affiche :
\newcommand{\Corrige}[1]{\noindent {\small {\bf Corrig√© :}\\ #1} }


\begin{document}

\textsc{\'Ecole Polytechnique}\hskip 3.2cm {\bf MAP 433} \hskip 4.5cm
2015/2016 \par \hrule

\par\vspace{0.2cm}

\begin{center} {\bf \Large PC 2 (Mod√®le statistique)}\end{center}

\barre

\section{Mod\`ele exponentiel}
Une grande partie des mod\`eles utilis\'es dans les exemples √©l√©mentaires sont des
mod\`eles exponentiels (mod\`ele gaussien, log-normal, exponentiel,
gamma, Bernouilli, Poisson, etc). Nous allons \'etudier quelques
propri\'et\'es de ces mod\`eles. On appelle mod\`ele exponentiel une
famille de lois $\{\PP_{\theta},\ \theta\in\Theta\}$ ayant une
densit\'e par rapport \`a une mesure $\mu$ $\sigma$-finie sur $\R$
ou $\N$ de la forme
$$p_{\theta}(x)=c(\theta)\exp\pa{m(\theta)f(x)+h(x)}.$$
On supposera que $\Theta$ est un intervalle ouvert de $\R$, $m(\theta)=\theta$, $c$ de classe $C^2$, $c(\theta)>0$ pour tout $\theta\in \Theta$. On
notera $X$ une variable al\'eatoire de loi $\PP_{\theta}$ et on
admettra que
$${\partial^i\over \partial \theta^i}\int\exp(\theta f(x)+h(x))\, \mu(dx)=\int f(x)^i\exp(\theta f(x)+h(x))\, \mu(dx)<+\infty,\quad \textrm{pour }i=1,2.$$
 \begin{enumerate}
\item Montrez que
$\varphi(\theta):=\E_{\theta}\pa{f(X)}=-{d\over
d\theta}\log(c(\theta))$.
\item Montrez que ${\rm Var_{\theta}}(f(X))=\varphi'(\theta)=-{d^2\over d\theta^2}\log( c\pa{\theta}).$
\item On dispose d'un $n$-\'echantillon $X_{1},\ldots,X_{n}$ de loi $\p_{\theta}$. On note
$\hat \theta_{n}$ l'estimateur obtenu en r\'esolvant $\varphi(\hat
\theta_{n})={1\over n}\sum_{i=1}^n f(X_{i})$.
En supposant ${\rm Var_{\theta}}(f(X))>0$, montrez que
$$\sqrt{n}\pa{\hat\theta_{n}-\theta}\stackrel{\textrm{loi}}{\to}\mathcal N\pa{0,{1\over {\rm Var}_{\theta}\pa{f(X)}}}.$$
\end{enumerate}
\Corrige{ Observer que puisque $p_\theta$ est une densit√©, on a 
\[
\frac{1}{c(\theta)} = \int \exp(\theta f(x) + h(x)) \ \mu(dx),
\]
et que le r√©sultat admis dit que $\theta \mapsto 1/c(\theta)$ est de classe $C^2$.
  \begin{enumerate}
  \item D'apr√®s le r√©sultat admis,
\[
\int f(x) p_\theta(x) \mu(dx) = c(\theta) \frac{d}{d\theta} \left(
  \frac{1}{c(\theta)} \right)
\]
ce qui donne le r√©sultat demand√©.
\item De m√™me, on a 
\[
\int f^2(x) p_\theta(x) \mu(dx) = c(\theta) \frac{d^2}{d\theta^2}
\frac{1}{c(\theta)}
\]
ce qui donne 
\[
\int f^2(x) p_\theta(x) \mu(dx) = -\frac{c''(\theta)}{c(\theta)} + 2 \left(
  \frac{c'(\theta)}{c(\theta)} \right)^2
\]
En combinant ce calcul avec la question pr√©c√©dente, on obtient
\[
\mathrm{Var}_\theta(f(X)) = \left(\frac{c'(\theta)}{c(\theta)} \right)^2 -
\frac{c''(\theta)}{c(\theta)}
\]
ce qui donne le r√©sultat demand√©.
\item Par le TCL pour des v.a. i.i.d. 
\[
\sqrt{n} \left( \phi(\hat{\theta}_n) - \PE_\theta[f(X)] \right) \dlim 
\mathcal{N}(0, {\rm Var}_{\theta}\pa{f(X)})
\]
puis on conclut par la m√©thode delta appliqu√©e avec la fonction $\phi^{-1}$.
  \end{enumerate}
}

\section{Estimation par la m\'ethode plug-in} \label{Plugin}
Soit $X_1,\ldots,X_n$ des variables al\'eatoires r\'eelles i.i.d. de fonction de r\'epartition $F$, soit $a<b$ deux r√©els et soit $\theta=F(b)-F(a)$.
\begin{enumerate}
\item D\'eterminer l'estimateur plug-in $\widehat{\theta}$ de $\theta$.
\item D\'eterminer l'estimateur plug-in de la variance de $\widehat{\theta}$ et en d\'eduire un intervalle de confiance asymptotique pour $\theta$ de niveau $1-\alpha$.
\end{enumerate}
\Corrige{
  \begin{enumerate}
  \item L'estimateur de substitution est donn√© par
\[
\hat{\theta}_n = \widehat{F}_n(b) - \widehat{F}_n(a) = \frac{1}{n} \sum_{k=1}^n \un_{a < X_k \leq b}.
\]
\item Notons $\widehat{\sigma}^2_n$ l'estimateur plug-in de la variance de $\widehat{\theta}_n$. On a 
\[
\widehat{\sigma}^2_n = \frac{\widehat{\theta}_n \left(1 - \widehat{\theta}_n \right)}{n}.
\]
Le TCL pour des v.a. i.i.d. et le lemme de Slutsky (noter que $n
\widehat{\sigma}^2_n \pslim \theta (1-\theta)$) donnent 
\[
\frac{1}{\widehat{\sigma}_n} \left( \widehat{\theta}_n - \theta \right) \dlim  \mathcal{N}(0, 1).
\]
Notons $z_{1-\alpha/2}$ le quantile d'ordre $1-\alpha/2$ d'une loi
$\mathcal{N}(0,1)$. On d√©duit de cette convergence en loi un intervalle de
confiance asymptotique pour $\theta$ au niveau $(1-\alpha)$
\[
\left[\widehat{\theta}_n - z_{1-\alpha/2} \widehat{\sigma}_n;
  \widehat{\theta}_n + z_{1-\alpha/2} \widehat{\sigma}_n\right]
\]
  \end{enumerate}
}

\section{Stabilisation de la variance}
 On dispose d'un
\'echantillon $X_{1},\ldots,X_{n}$ i.i.d. de loi de Bernoulli de
param\`etre $0<\theta<1$.
\begin{enumerate}
\item On note $\bar X_{n}$ la moyenne empirique des $X_{i}$. Que
disent la loi des grands nombres et le TCL?
\item Cherchez une fonction $g$ telle que $\sqrt{n}(g(\bar X_{n})-g(\theta))$
converge en loi vers $Z$ de loi $\mathcal{N}(0,1)$.
\item On note $z_{\alpha}$ le quantile d'ordre $1-\alpha/2$ de la loi normale
standard. En d\'eduire un intervalle $\hat I_{n, \alpha}$ fonction
de $z_{\alpha}, n, \bar X_{n}$ tel que $\lim_{n\to
\infty}\p(\theta\in \hat I_{n,\alpha})=1-\alpha$.
\end{enumerate}
\Corrige{
  \begin{enumerate}
  \item On a 
\[
\overline{X}_n \pslim \PE_\theta[X]= \theta \qquad \sqrt{n}\left(
  \overline{X}_n - \PE_\theta[X] \right) \dlim \mathcal{N}(0, \theta
(1-\theta)).
\]
\item Par la m√©thode delta, on cherche une fonction $g$ d√©rivable en $\theta$,
  pour tout $\theta \in ]0,1[$, telle que $\left(g'(\theta)\right)^2 \theta
  (1-\theta) = 1 $.  Plusieurs solutions possibles
\[
\theta \mapsto \arccos(2\theta-1) \qquad \qquad \theta \mapsto 2
\arccos{\sqrt{\theta}}
\]
et puis les analogues avec $\arcsin$ puisque $\arcsin(u) + \arccos(u) = \pi/2$
pour tout $u \in [-1,1]$.
\item On en d√©duit l'intervalle de confiance asympotique de niveau $1-\alpha$
\[
I_{n,\alpha} = \left[ \frac{1}{2} \left( 1+ \cos\left( g(\overline{X}_n) + \frac{z_\alpha}{\sqrt{n}} \right)\right);  \frac{1}{2} \left( 1+ \cos\left( g(\overline{X}_n) - \frac{z_\alpha}{\sqrt{n}} \right)\right)  \right]
\]
avec $g(x) = \arccos(2 x-1)$.
  \end{enumerate}
}

\section{Mod\`ele d'autor\'egression}
On consid\`ere l'observation $Z=(X_1,\dots,X_n)$, o\`u les $X_i$
sont issus du processus d'autor\'egression:
$$
X_i=\theta X_{i-1} + \xi_i, \quad i=1,\dots,n, \quad\quad X_0=0,
$$
avec les $\xi_i$ i.i.d. de loi normale ${\mathcal N}(0,\sigma^2)$ et
$\theta\in \R$. \'Ecrire le mod\`ele statistique engendr\'e
par l'observation $Z$.

\Corrige{\begin{itemize}
\item espace probabilisable: $\Rset^n$ muni de la tribu bor√©lienne.
\item une famille de lois $\PP_\psi$ sur $\Rset^n$ donn√©es par
\[
\PP_\psi(A) = \int_A \frac{1}{(\sqrt{2 \pi} \sigma)^n}
\frac{1}{\sqrt{\mathrm{det}(\Gamma)}} \ \exp\left(- \frac{1}{2\sigma^2}
  \underline{x}^T \Gamma^{-1} \underline{x} \right) d  \underline{x}
\]
o√π $\psi = (\theta, \sigma) \in \Rset \times ]0, \infty[$ et 
\[
\Gamma^{-1} = 
\left[\begin{matrix}
  1+\theta^2 & -\theta & 0 & \cdots & \cdots \\
-\theta & 1+\theta^2 & -\theta & 0 & \cdots \\
0 & -\theta & 1+\theta^2 & -\theta & \cdots \\
\cdots & \cdots & \cdots & \cdots & \cdots  \\
\cdots & \cdots & -\theta & 1+\theta^2 & -\theta \\
\cdots & \cdots &0 &-\theta&1
\end{matrix} \right]
\]
Noter que $\Gamma = A A^T$ avec
\[
A = \left[
  \begin{matrix}
    1 & 0 &  \cdots & 0 & 0 \\
\theta & 1 & \cdots & 0 & 0 \\
\theta^2 & \theta & 1 & 0 & \cdots \\
\cdots &  \cdots & \cdots & \cdots & \cdots \\
\theta^{n-1} & \theta^{n-2} & \cdots & \theta & 1
  \end{matrix} \right]
\]
\end{itemize} 
Pour obtenir l'expression de la loi $\PP_\psi$, on peut remarquer que $X_k$ est
une combinaison lin√©aire des v.a. $\xi_1, \cdots, \xi_k$; et √©crire le vecteur
$Z$ comme une transformation affine du vecteur gaussien $(\xi_1, \cdots,
\xi_n)$. 
\\
{\bf Autre expression} L'expression de $\PP_\psi(A)$ est √©quivalente √† 
\[
\PP_\psi(A) = \int_A \frac{1}{(\sqrt{2 \pi} \sigma)^n} \exp \left( -
  \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \theta x_{i-1})^2 \right) d
\underline{x}
\]
que l'on obtient par exemple de la fa√ßon suivante. Soit $f$ la
densit√© de probabilit√© du vecteur $(\xi_1, \xi_1, \ldots, \xi_n)$:
$$
f(z_1, z_1, \ldots, z_n ; \sigma^2) = (2\pi\sigma^2)^{-n/2} \exp\left(-
  \frac{1}{2\sigma^2}\sum_{i=1}^n z_i^2 \right)
$$
Soit $\phi$ l'application de $\R^n$ dans $\R^n$ d√©finie par
$$\phi(x_1, x_2, \ldots, x_n) = (z_1, z_1, \ldots, z_n)$$
Le Jacobien de $\phi$
est la matrice
$$ J_\phi = \left(
  \begin{matrix}
    1 & -\theta &  0 & 0 & 0 & \cdots & 0  \\
    0 & 1 & -\theta & 0 & 0 & \cdots & 0  \\
    0 & 0 & 1 & -\theta &  0 & \cdots &  0 \\
 \cdots &  \cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
   0&0&0&0&0&1 & -\theta  \\
   0&0&0&0&0&0&1   \\
  \end{matrix} \right)
$$
On v√©rifie facilement que son d√©terminant vaut 1. Le d√©terminant du Jacobien de $\phi^{-1}$ vaut donc √©galement 1.
On en d√©duit l'expression de la densit√© $g$ de $(X_1, \ldots , X_n)$ :
\begin{align*}
  g(x_1, x_2, \ldots,x_n ; \theta,\sigma_2) &= f(\phi(x_1, x_2, \ldots,x_n ; \sigma^2)) \\
  & = (2\pi\sigma^2)^{-n/2} \exp\left(- \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i -
    \theta x_{i-1})^2 \right)
\end{align*}
}

\section{Survie}
On √©tudie un syst\`eme qui fonctionne si deux machines de types
diff\'erents fonctionnent. Les dur\'ees de vie $X_1$ et $X_2$ des deux machines
suivent des lois exponentielles de param\`etres $\lambda_1$ et
$\lambda_2$ : $\P(X_i > x)=e^{-\lambda_i x}$.
Les variables al\'eatoires $X_1$ et $X_2$ sont
suppos\'ees ind\'ependantes.
\begin{enumerate}
\item Calculer la probabilit\'e pour que le syst\`eme ne tombe pas en panne
avant la date $t$. En d\'eduire la loi de la dur\'ee de vie $Z$ du
syst\`eme. Calculer la probabilit\'e pour que la panne du syst\`eme
soit due \`a une d\'efaillance de la machine $1$.
\item Soit $I = 1$ si la
panne du syst\`eme est due \`a une d\'efaillance de la machine $1$,
$I = 0$ sinon. Calculer $\P(Z > t; I = \delta)$, pour tout $t\geq 0$
et $\delta\in\{0,1\}$. En d\'eduire que $Z$ et $I$ sont
ind\'ependantes.
\item On dispose de $n$ syst\`emes identiques et fonctionnant
ind\'ependamment les uns des autres dont on observe les dur\'ees de
vie
$Z_1,\ldots,Z_n$.\\
(a) \'Ecrire le mod\`ele statistique correspondant. Les param\`etres
$\lambda_1$ et $\lambda_2$ sont-ils identifiables?\\
(b) Supposons maintenant que l'on observe \`a la fois les dur\'ees de vie des syst\`emes $Z_1,\ldots,Z_n$ et
les causes de la d\'efaillance correspondantes $I_1,\dots,I_n$, $I_i\in\{0,1\}$.  \'Ecrire le mod\`ele statistique dans ce cas. Les param\`etres
$\lambda_1$ et $\lambda_2$ sont-ils identifiables?
\end{enumerate}

\Corrige{
  \begin{enumerate}
  \item La probabilit√© que le syst√®me ne tombe pas en panne avant la date $t$
    est 
\[
\PP(Z>t) = \exp(-(\lambda_1 +\lambda_2) t).
\]
La probabilit√© que la panne soit d√ªe √† la d√©faillance de la machine 1 est
\[
 \PP(Z=X_1) = \PP(X_2 \geq X_1) =  \frac{\lambda_1}{\lambda_1+\lambda_2}.
\]
\item  Pour tout $t\geq 0$,
\[
\PP(Z >t, I=1) = \PP( X_2 \geq X_1, X_1 > t) = \frac{\lambda_1}{\lambda_1 +
  \lambda_2} \exp(-(\lambda_1+ \lambda_2) t) = \PP(Z=X_1) \ \PP(Z>t).
\]
R√©sultat analogue pour $\PP(Z>t, I=0)$.
\item 
  \begin{enumerate}[(a)]
  \item Mod√®le statistique
\begin{itemize}
\item Espace probabilisable: $]0, +\infty[^n$ muni de la tribu bor√©lienne.
\item Famille de fonctions de r√©partition $F_\theta$ sur $]0, \infty[$ index√©es par  $\theta = (\lambda_1,
  \lambda_2) \in \Theta = ]0, +\infty[^2$ et d√©finies par
\[
F_\theta(x) = \left( 1 - \exp(-(\lambda_1+\lambda_2) x) \right) \un_{\Rset^+}(x) 
\]
\end{itemize} 
Mod√®le non identifiable.
\item Mod√®le statistique
\begin{itemize}
\item Espace probabilisable: $(]0, +\infty[ \times \{0,1\})^n$ muni de la tribu
  engendr√©e par les ensembles $A\times\{0\}, A \times \{1 \}$ o√π $A$ est un bor√©lien.
\item Famille de fonctions de r√©partition $F_\theta$ sur $]0, \infty[ \times
  \{0,1\}$ index√©es par $\theta = (\lambda_1, \lambda_2) \in \Theta = ]0,
  +\infty[^2$ et d√©finies par
  \begin{align*}
    F_\theta(x,1) & = \left( 1 - \exp(-(\lambda_1+\lambda_2) x) \right)
    \un_{\Rset^+}(x) \ \frac{\lambda_1}{ \lambda_1+\lambda_2} \\
    F_\theta(x,0) & = \left( 1 - \exp(-(\lambda_1+\lambda_2) x) \right)
    \un_{\Rset^+}(x) \ \frac{\lambda_2}{ \lambda_1+\lambda_2}
  \end{align*}
\end{itemize}
Mod√®le identifiable.  \end{enumerate}
  \end{enumerate}}

\hrule

\vspace{4mm}

\begin{center}
{\bf Exercices bonus}
\end{center}

{\small

\begin{exercice}\label{exo:LimiteBeta}
Pour tout $\alpha > 0$, on appelle loi $\loigamma(\alpha)$ la loi sur $\rset^+$ de densit√©
$$
g_\alpha(x) = \frac{1}{\Gamma(\alpha)} x^{\alpha-1} \rme^{-x} \;,
\quad\text{o√π}\quad \Gamma(\alpha) \eqdef \int_0^\infty x^{\alpha-1}
\rme^{-x} dx\;.
$$
Pour $a,b > 0$, on appelle loi $\loibeta(a,b)$ la loi sur $[0,1]$ de densit√©
$$
h_{a,b}(x)= \frac{\Gamma(a+b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1} \eqsp.
$$
\begin{enumerate}
\item \label{qu:bonus1} Soit $s$ et $t > 0$ et soit $X$ et $Y$ deux variables ind√©pendantes de loi $\loigamma(s)$ et $\loigamma(t)$, respectivement.
On pose
\begin{align*}
&U= X+Y \\
& V= X / (X+Y)
\end{align*}
Montrer que $U$ et $V$ sont ind√©pendantes et que $U$ est distribu√©e suivant une loi $\loigamma(s+t)$ et
$V$ suivant une loi $\loibeta(s,t)$. [\emph{Indication~:} on pourra consid√©rer la densit√© jointe de $(U,V)$ sans se
pr√©occuper des constantes de normalisation.]
\item Soit $\{Z_n\}_{n \geq 0}$ une suite de variables al√©atoires telles que, pour tout $n\geq0$, $Z_n$ est de loi
  $\loigamma(n)$. Montrer que
\[
\sqrt{n}\left(\frac{Z_n}{n}-1\right) \cl \loigauss(0,1) \;.
\]
\item \label{bonus:qu3:exo1} oient $p \in (0,1)$ et $\{ k_n \}$ une suite
  monotone croissante d'entiers v√©rifiant
\begin{equation}
\label{eq:condition-suite}
\sqrt{n} \left( \frac{k_n}{n} - p \right) \rightarrow 0 \;.
\end{equation}
Soient $\{X_n\}_{n \geq 0}$ et $\{Y_n\}_{n \geq 0}$ deux suites ind√©pendantes telles que $X_n \sim \loigamma(k_n)$ et $Y_n \sim \loigamma(n-k_n)$.
On pose
\[
V_n = \frac{X_n}{X_n+Y_n} \;.
\]
Montrer que
$$
\sqrt{n} \left( V_n - p \right) \cl\loigauss(0,p(1-p)) \;.
$$
[\emph{Indication~:} on pourra, dans un premier temps, consid√©rer le comportement asymptotique du couple $\frac1n(X_n,Y_n)-(p,1-p)$.]
\item Conclure.
\end{enumerate}
\end{exercice}


\Corrige{
  \begin{enumerate}
  \item Les v.a. $X,Y$ √©tant ind√©pendantes, la loi jointe est donn√©e par
\[
f_{(X,Y)}(x,y) = f_X(x) \, f_Y(y) \propto x^{s-1} \exp(-x) y^{t-1} \exp(-y) \un_{\Rset^+}(x) \un_{\Rset^+}(y).
\]
Par le changement de variable 
\[
\phi: \left[\begin{matrix} x \\ y \end{matrix} \right] \to \left[\begin{matrix} u \\ v \end{matrix} \right] = \left[ 
  \begin{matrix}
  x+y \\ \frac{x}{x+y}
  \end{matrix}
\right] 
\]
on obtient pour toute fonction $h$ mesurable positive
\[
\PE\left[h(U,V) \right] \propto \int_{\Rset^+_\star \times ]0,1[} h(u,v)
u^{s+t-1} \exp(-u) \ v^{s-1} (1-v)^{t-1} \ du \, dv
\]
dont on d√©duit que $(U,V)$ sont deux variables ind√©pendantes; $U$ est une loi
Gamma de param√®tre $s+t$ et $V$ est une loi Beta de param√®tres $(s,t)$.
\item Par la question pr√©c√©dente, $Z_n$ a m√™me loi que $\sum_{k=1}^n W_k$ o√π
  $\{W_k, k \geq 1 \}$ sont i.i.d. de loi Gamma de param√®tre $1$. Donc la limite en loi de $\sqrt{n}(Z_n/n -1)$ est la limite en loi de 
\[
\sqrt{n} \left(\frac{1}{n} \sum_{k=1}^n W_k -1 \right)
\] 
Puisque $\PE[W_1] = \mathrm{Var}(W_1) = 1$, on obtient le r√©sultat demand√© en
appliquant le TCL pour des v.a. i.i.d.
\item {\bf $\blacktriangleright$ Solution 1 (plus rapide)} D'apr√®s la
  question~\ref{qu:bonus1}, $V_n$ suit une loi Beta de param√®tres $(k_n, n -
  k_n)$. On en d√©duit le r√©sultat demand√© en √©tudiant la limite simple de sa
  densit√© (quand $n \to \infty$) et en appliquant le lemme de Scheff√©.
  \\
  {\bf $\blacktriangleright$ Solution 2} \textcolor{red}{On propose une preuve
    bas√©e sur l'indication suivante: √©tablir la loi de $X_n/Y_n$ puis observer
    que $V_n = g(X_n/Y_n)$ avec $g(x)
    = x/(1+x)$} \\
  $\bullet$ D'apr√®s la question~\ref{qu:bonus1}, en observant que $X/Y =
  V/(1-V)$, on obtient par la m√©thode d'identification
\[
\PE\left[h\left( \frac{X}{Y}\right) \right] = \frac{\Gamma(s+t)}{\Gamma(s)
  \Gamma(t)} \int_0^\infty h(z) \frac{z^{s-1}}{(z+1)^{s+t}} dz
\]
(on reconna√Æt une Loi Beta de seconde esp√®ce). Soit $R_n = X_n/Y_n$; sa densit√©
est proportionnelle √† $ z^{k_n-1} (1+z)^{-n} \un_{\Rset^+}(z)$.  \\
$\bullet$ Montrons que~\footnote{intuiter le terme de recentrage et la variance
  limite pour que l'application de la m√©thode delta permette d'aboutir au
  r√©sultat demand√©.}
\begin{equation}
  \label{eq:bonus:qu3}
  \sqrt{n} \left(R_n - \frac{p}{1-p}\right) \dlim \mathcal{N} \left(0,\frac{p}{(1-p)^3} \right).
\end{equation}
Soit $h$ une fonction continue born√©e.  Posons $c = p/(1-p)$.
\begin{align*}
  & \PE\left[h\left( \sqrt{n}(R_n - c)\right) \right] =
  \frac{\Gamma(n)}{\Gamma(k_n) \Gamma(n-k_n)} \int_0^\infty
  h\left( \sqrt{n} (z - c) \right) z^{k_n-1} (1+z)^{-n}  dz \\
  & = \frac{1}{\sqrt{n}} \frac{\Gamma(n)}{\Gamma(k_n) \Gamma(n-k_n)} \int_{-c
    \sqrt{n}}^{+\infty} h(v) \left( \frac{v}{\sqrt{n}}+c\right)^{k_n-1} \ 
  \left( 1 + c
    + \frac{v}{\sqrt{n}}\right)^{-n}  \ dv  \\
  & = \frac{1}{\sqrt{n}} \frac{c^{k_n-1}}{(1-c)^n}\frac{\Gamma(n)}{\Gamma(k_n)
    \Gamma(n-k_n)}\int_{-c \sqrt{n}}^{+\infty} h(v) \left( 1+ \frac{v}{c
      \sqrt{n}}\right)^{k_n-1} \ \left( 1 + \frac{v}{(1+c)\sqrt{n}}\right)^{-n}
  \ dv
\end{align*}
On √©crit $\ln(1+x) = x - x^2/2 + o(x^2)$ au voisinage de zero, puis on applique
le lemme de Scheff√©.  Pour identifier la densit√© limite, observer que
\begin{multline*}
  \left(k_n-1 \right) \ln \left( 1+ \frac{v}{c \sqrt{n}}\right) - n \ln\left(
    1 + \frac{v}{(1+c)\sqrt{n}}\right) \\
  = \frac{\sqrt{n} v}{c} \left(\frac{k_n-1}{n} - \frac{c}{1+c} \right) -
  \frac{v^2}{2c^2} \left(\frac{k_n-1}{n} - \frac{c^2}{(1+c)^2}\right) \left(1 +
    o(1)\right).
\end{multline*}
Puisque $c/(1+c) = p$, en utilisant (\ref{eq:condition-suite}) le terme de
droite converge vers (√† $v$ fix√©, quand $n \to \infty$)
\[
- \frac{v^2}{2 c^2 } p (1-p) = - \frac{v^2}{2 } \frac{(1-p)^3}{p}.
\]
On v√©rifie ensuite que le terme constant converge vers la constante de
normalisation de $v \mapsto \exp(-v^2 (1-p)^3/(2p))$. \\
$\bullet$ On a $V_n = R_n/(1+R_n) = g(R_n)$ en ayant pos√© $g(x) = x/(x+1)$.
Notons que $g$ est $C^1$ sur $\Rset^+$ et que $g'(x) = 1/(1+x)^2$.  En
observant que
\[
g\left( \frac{p}{1-p}\right) = p \qquad g'\left( \frac{p}{1-p}\right) = (1-p)^{2}
\]
on obtient le r√©sultat demand√© en appliquant la m√©thode delta √† la convergence
(\ref{eq:bonus:qu3}).
  \end{enumerate}
}


\begin{exercice}\label{exo:medianeCLT}
Soit $f$ une densit√© de probabilit√© port√©e par un intervalle  (non n√©cessairement born√©)
$(a,b) \subset \rset$. On suppose que $f$ est continue et ne s'annule pas sur $(a,b)$.
On note $F(x)= \int_{-\infty}^x f(u) du$ la fonction de r√©partition associ√©e.
Cette fonction de r√©partition est alors strictement monotone sur $x\in[a,b]$
et d√©finit une bijection de $[a,b] \to [0,1]$. On note $F^{-1}$ la fonction r√©ciproque de $F$ de  $[0,1]\to[a,b]$. De plus,
par continuit√© de $f$, $F$ est continuement d√©rivable sur $(a,b)$ de d√©riv√©e $f$ et il s'en suit que  $F^{-1}$ est d√©rivable sur $(0,1)$.

\begin{enumerate}
\item Soit $U$ une variable uniforme sur $[0,1]$, $U \sim \loiunif([0,1])$. Montrer que la variable $X$ d√©finie par $X= F^{-1}(U)$
a pour densit√© $f$. R√©ciproquement, montrer que si $X$ est une loi de densit√© $f$, alors $U= F(X)$ est une loi uniforme sur $[0,1]$.
\item Soient $g$ une densit√© et $Y_1, \dots, Y_n$, $n$ v.a. \iid\ de densit√© $g$. On note $(Y_{(1)}, \dots, Y_{(n)})$ la statistique d'ordre
de l'√©chantillon, $Y_{(1)} < Y_{(2)} < \dots < Y_{(n)}$. Montrer que $Y_{(k)}$ a pour densit√©
\[
g_{Y_{(k)}}(y) = \frac{n!}{(k-1)!(n-k)!} G(y)^{k-1}[1 - G(y)]^{n-k} g(y) \;,
\]
o√π $G$ est la fonction de r√©partition associ√©e √† $g$.
[\emph{Indication~:} on pourra montrer successivement
$g_{Y_{(k)}}(y) = n!\PP(Y_1<\dots<Y_{k-1} <y <Y_{k+1}<\dots<Y_n) g(y)$ puis
$\PP(\max(Y_1,\dots,Y_{k-1}) <y)=(k-1)!\;\PP(Y_1<\dots<Y_{k-1} <y)$ et $\PP(y<\min(Y_{k+1},\dots,Y_n))=(n-k)!\;\PP(y <Y_{k+1}<\dots<Y_n)$.]
% que la densit√© jointe de $(Y_{(1)} , Y_{(2)} ,\dots , Y_{(n)})$ est donn√©e par $n!\prod_{k=1}^ng(y_k)\1( y_1<y_2<\dots<y_n)$.]
%en remarquant
%que, pour toute fonction de $h:\rset^n\to\rset$,
%$h(Y_{(1)} , Y_{(2)} ,\dots , Y_{(n)})=\sum_{\sigma\in\calP_n} h(Y_{\sigma(1)},\dots,Y_{\sigma(n)})\1( Y_{\sigma(1)}<\dots<Y_{\sigma(n)}),
%$$
%o√π $\calP_n$ est l'ensemble des permutations de $\{1,\dots,n\}$,
%puis en prenant l'esp√©rance de cette expression.]
\item Quelle est la loi de  $Y_{(k)}$ si $g= \1_{[0,1]}$ est la densit√© de la loi uniforme sur $[0,1]$ ?
\item \label{bonus:qu4:exo2} Soit $p \in (0,1)$. On note $x_p$ le quantile d'ordre $p$, \emph{i.e.} $x_p = F^{-1}(p)$. Montrer que
\[
\sqrt{n} (X_{(k_n)} - x_p) \cl\loigauss\left(0,\frac{p(1-p)}{f^2(x_p)}\right) \;.
\]
\item Soit $X_1, \dots, X_n$ une suite de v.a. \iid\ normales de moyenne $\mu$ et de variance $\sigma^2$. Montrer que la m√©diane est un estimateur consistant et asymptotiquement normal de la moyenne.
D√©terminer la variance asymptotique de cet estimateur. Cet estimateur doit-il √™tre pr√©f√©r√© √† la moyenne empirique ?
\item Reprendre la question pr√©c√©dente avec $X_1, \dots, X_n$ suite de v.a. \iid\ distribu√©es suivant une loi de Laplace de
densit√© $f_\mu(x)= \frac{1}{2} \rme^{-|x-\mu|}$. Commenter.
\end{enumerate}
\end{exercice}
\Corrige{
  \begin{enumerate}
  \item Pour tout $x \in [a,b]$, $\{ F^{-1}(U) \leq x \} = \{U \leq F(x) \}$ et
    pour tout $t \in [0,1]$, on a $\{F(X) \leq t \} = \{X \leq F^{-1}(t) \}$.
    Ce qui donne le r√©sultat.
  \item \textcolor{red}{La correction ne suit pas l'indication: on calcule
      d'abord la fonction de r√©partition puis on obtient la densit√© par
      d√©rivation}  On a 
\[
\{ Y_{(k)} \leq t\} = \bigcup_{j=k}^n \{ \text{il existe exactement $j$
  variables dans $]-\infty,t]$ et $(n-j)$ dans $]t, \infty[$} \};
\]
Il s'agit d'une union disjointe et chaque √©v√©nement d√©crit le r√©sultat d'une
Bin√¥miale de param√®tres $(n,G(t))$. On en d√©duit que
\[
\PP(Y_{(k)} \leq t) = \sum_{j=k}^n \binom{n}{j} G(t)^{j} (1-G(t))^{n-j}
\]
puis par d√©rivation, on obtient pour densit√©
\[
t \mapsto n \binom{n-1}{k-1} \left( G(t) \right)^{k-1} \ \left(1-G(t)
\right)^{n-k} g(t)
\]
\item Le cas uniforme correspond √† $g(t) = \un_{[0,1]}(t)$ et $G(t)=t$ pour
  tout $t \in [0,1]$.
\item \textcolor{red}{Ici, $\{k_n, n \geq 1\}$ est une suite d'entiers telle
    que $\lim_n \sqrt{n}(k_n/n - p) = 0$}. On √©tablit la propri√©t√© dans le cas
  o√π les v.a. $\{X_k, k \geq 1 \}$ sont i.i.d. uniformes sur $[0,1]$ (ce qui
  entraine $x_p = p$). Le cas g√©n√©ral est une cons√©quence de la m√©thode delta
  appliqu√©e avec la fonction $t \mapsto F^{-1}(t)$ dont la d√©riv√©e est
  $1/f(F^{-1}(t))$. \\
  D'apr√®s la question pr√©c√©dente, $X_{(k_n)}$ suit une loi Beta de param√®tres
  $(k_n, n-k_n)$. En utilisant l'exercice pr√©c√©dent, on a
%% En utilisant la question pr√©c√©dente, pour toute fonction $h$ mesurable positive
%% \begin{align*}
%%   & \PE\left[h\left( \sqrt{n} \left(X_{(k_n)} - p \right) \right) \right]  \\
%% & 
%%   \sqrt{n} \binom{n-1}{k_n-1} p^{k_n-1} (1-p)^{n-k_n} \int_{- p
%%     \sqrt{n}}^{\sqrt{n}(1-p)} h(y) \left(1+ \frac{y}{p \sqrt{n}}
%%   \right)^{k_n-1} \ \left(1 - \frac{y}{(1-p)\sqrt{n}} \right)^{n-k_n} dy
%% \end{align*}
%% En proc√©dant comme dans la question~\ref{bonus:qu3:exo1} de
%% l'exercice~\ref{exo:LimiteBeta}, on obtient par le lemme se Scheff√© (voir PC1
%% exercice 2) que
%% \[
%% \lim_n \PE\left[h\left( \sqrt{n} \left(X_{(k_n)} - p \right) \right) \right]
%% \propto \int h(y) \ \exp(- \frac{y^2}{2p(1-p)}) dy.
%% \]
%% On en d√©duit que 
\[
\sqrt{n}\left( X_{(k_n)} -p \right) \dlim \mathcal{N}(0, p(1-p)).
\]
\item Par la moyenne empirique, un intervalle de confiance (non asymptotique)
  au noveau $(1-\alpha)$ est
\[
\left[ n^{-1} \sum_{k=1}^n X_n \pm z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}
\right]
\]
o√π $z_{1-\alpha/2}$ est le quantile d'ordre $1-\alpha/2$ d'une loi
$\mathcal{N}(0,1)$. \\
Par l'estimateur de la m√©diane, en utilisant la question pr√©c√©dente, on obtient
un intervalle de confiance (asymptotique) de niveau $1-\alpha$
\[
\left[ X_{(k_n)} \pm z_{1-\alpha/2} \sqrt{\frac{\pi}{2}}\frac{\sigma}{\sqrt{n}}
\right]
\]
\item  D'apr√®s la question~\ref{bonus:qu4:exo2}, il vient 
\[
\sqrt{n} \left( X_{\lfloor n/2\rfloor} - \mu \right) \dlim \mathcal{N}(0,1).
\] 
Par le TCL, puisque $\mathrm{Var}(X_1) = 2$,
\[
\sqrt{n} \left( n^{-1} \sum_{k=1}^n X_k - \mu \right) \dlim \mathcal{N}(0, 2).
\]
  \end{enumerate}
}
}




\end{document}

%%%%%%%%%%%%%%% VIEUX EXOS

\section{Mod\`ele probit}
\label{probit}
Nous disposons d'une information relative au comportement de remboursement ou
de non-remboursement d'emprunteurs :
\begin{equation*}
Y_{i}=\left\{
    \begin{array}{cc}
      1 & \text{si l'emprunteur }i\text{ rembourse}, \\
      0 & \text{si l'emprunteur }i\text{ est d\'efaillant}.%
    \end{array}%
  \right.
\end{equation*}%
Afin de mod\'eliser ce ph\'enom\`ene, on suppose l'existence d'une variable al\'eatoire
$Y_{i}^{\ast }$ normale, d'esp\'erance $m$ et de variance $\sigma ^{2}$, que l'on
appellera \emph{capacit\'e de remboursement} de l'individu $i$, telle que :
\begin{equation*}
  Y_{i}=\left\{
    \begin{array}{cc}
      1 & \text{si }Y_{i}^{\ast }>0, \\
      0 & \text{si }Y_{i}^{\ast }\leq 0.%
    \end{array}%
  \right.
\end{equation*}
On note $\Phi$ la fonction de r\'epartition de la loi normale $\mathcal{N}%
\left( 0,1\right) $.
\begin{enumerate}
\item Exprimer la loi de $Y_{i}$ en fonction de $\Phi$.
\item Les param\`etres $m$ et $\sigma^{2}$ sont-ils identifiables~?
\end{enumerate}

